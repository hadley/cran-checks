
R Under development (unstable) (2017-08-14 r73093) -- "Unsuffered Consequences"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "fastcluster"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('fastcluster')

Attaching package: ‘fastcluster’

The following object is masked from ‘package:stats’:

    hclust

> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("fastcluster")
> ### * fastcluster
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fastcluster
> ### Title: Fast hierarchical, agglomerative clustering routines for R and
> ###   Python
> ### Aliases: fastcluster fastcluster-package
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> # Taken and modified from stats::hclust
> #
> # hclust(...)        # new method
> # hclust.vector(...) # new method
> # stats::hclust(...) # old method
> 
> require(fastcluster)
> require(graphics)
> 
> hc <- hclust(dist(USArrests), "ave")
> plot(hc)
> plot(hc, hang = -1)
> 
> ## Do the same with centroid clustering and squared Euclidean distance,
> ## cut the tree into ten clusters and reconstruct the upper part of the
> ## tree from the cluster centers.
> hc <- hclust.vector(USArrests, "cen")
> # squared Euclidean distances
> hc$height <- hc$height^2
> memb <- cutree(hc, k = 10)
> cent <- NULL
> for(k in 1:10){
+   cent <- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
+ }
> hc1 <- hclust.vector(cent, method = "cen", members = table(memb))
> # squared Euclidean distances
> hc1$height <- hc1$height^2
> opar <- par(mfrow = c(1, 2))
> plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
> plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
> par(opar)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("hclust")
> ### * hclust
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hclust
> ### Title: Fast hierarchical, agglomerative clustering of dissimilarity
> ###   data
> ### Aliases: hclust
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> # Taken and modified from stats::hclust
> #
> # hclust(...)        # new method
> # stats::hclust(...) # old method
> 
> require(fastcluster)
> require(graphics)
> 
> hc <- hclust(dist(USArrests), "ave")
> plot(hc)
> plot(hc, hang = -1)
> 
> ## Do the same with centroid clustering and squared Euclidean distance,
> ## cut the tree into ten clusters and reconstruct the upper part of the
> ## tree from the cluster centers.
> hc <- hclust(dist(USArrests)^2, "cen")
> memb <- cutree(hc, k = 10)
> cent <- NULL
> for(k in 1:10){
+   cent <- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
+ }
> hc1 <- hclust(dist(cent)^2, method = "cen", members = table(memb))
> opar <- par(mfrow = c(1, 2))
> plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
> plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
> par(opar)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("hclust.vector")
> ### * hclust.vector
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: hclust.vector
> ### Title: Fast hierarchical, agglomerative clustering of vector data
> ### Aliases: hclust.vector
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> # Taken and modified from stats::hclust
> ## Perform centroid clustering with squared Euclidean distances,
> ## cut the tree into ten clusters and reconstruct the upper part of the
> ## tree from the cluster centers.
> hc <- hclust.vector(USArrests, "cen")
> # squared Euclidean distances
> hc$height <- hc$height^2
> memb <- cutree(hc, k = 10)
> cent <- NULL
> for(k in 1:10){
+   cent <- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
+ }
> hc1 <- hclust.vector(cent, method = "cen", members = table(memb))
> # squared Euclidean distances
> hc1$height <- hc1$height^2
> opar <- par(mfrow = c(1, 2))
> plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
> plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
> par(opar)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  0.532 0.012 0.555 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
