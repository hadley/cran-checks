
R Under development (unstable) (2017-08-15 r73096) -- "Unsuffered Consequences"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "lava"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('lava')
lava version 1.5
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("By")
> ### * By
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: By
> ### Title: Apply a Function to a Data Frame Split by Factors
> ### Aliases: By
> 
> ### ** Examples
> 
> By(datasets::CO2,~Treatment+Type,colMeans,~conc)
            Type
Treatment    Quebec Mississippi
  nonchilled    435         435
  chilled       435         435
> By(datasets::CO2,~Treatment+Type,colMeans,~conc+uptake)
Treatment: nonchilled
Type: Quebec
     conc    uptake 
435.00000  35.33333 
------------------------------------------------------------ 
Treatment: chilled
Type: Quebec
     conc    uptake 
435.00000  31.75238 
------------------------------------------------------------ 
Treatment: nonchilled
Type: Mississippi
     conc    uptake 
435.00000  25.95238 
------------------------------------------------------------ 
Treatment: chilled
Type: Mississippi
     conc    uptake 
435.00000  15.81429 
> 
> 
> 
> cleanEx()
> nameEx("Col")
> ### * Col
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Col
> ### Title: Generate a transparent RGB color
> ### Aliases: Col
> ### Keywords: color
> 
> ### ** Examples
> 
> plot(runif(1000),cex=runif(1000,0,4),col=Col(c("darkblue","orange"),0.5),pch=16)
> 
> 
> 
> cleanEx()
> nameEx("Combine")
> ### * Combine
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Combine
> ### Title: Report estimates across different models
> ### Aliases: Combine
> 
> ### ** Examples
> 
> data(serotonin)
> m1 <- lm(cau ~ age*gene1 + age*gene2,data=serotonin)
> m2 <- lm(cau ~ age + gene1,data=serotonin)
> m3 <- lm(cau ~ age*gene2,data=serotonin)
> 
> Combine(list(A=m1,B=m2,C=m3),fun=function(x)
+      c("_____"="",R2=" "%++%format(summary(x)$r.squared,digits=2)))
            A                            B                          
(Intercept)  0.88  [0.85;0.91]   p<0.001  0.89  [0.86;0.91]  p<0.001
age          0.02  [-0.01;0.05]  p=0.108 -0.01  [-0.02;0.01] p=0.414
gene1       -0.02  [-0.06;0.01]  p=0.228 -0.02  [-0.06;0.01] p=0.219
gene2        0     [-0.03;0.04]  p=0.93                             
age:gene1   -0.04  [-0.07;-0.01] p=0.019                            
age:gene2   -0.02  [-0.05;0.01]  p=0.218                            
_____                                                               
R2           0.039                        0.0083                    
            C                          
(Intercept)  0.88  [0.85;0.9]   p<0.001
age          0.01  [-0.02;0.03] p=0.549
gene1                                  
gene2        0     [-0.03;0.04] p=0.99 
age:gene1                              
age:gene2   -0.03  [-0.06;0.01] p=0.126
_____                                  
R2           0.012                     
> 
> 
> 
> cleanEx()
> nameEx("Expand")
> ### * Expand
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Expand
> ### Title: Create a Data Frame from All Combinations of Factors
> ### Aliases: Expand
> 
> ### ** Examples
> 
> dd <- Expand(iris, Sepal.Length=2:8, Species=c("virginica","setosa"))
> summary(dd)
  Sepal.Length        Species 
 Min.   :2.00   setosa    :7  
 1st Qu.:3.25   versicolor:0  
 Median :5.00   virginica :7  
 Mean   :5.00                 
 3rd Qu.:6.75                 
 Max.   :8.00                 
> 
> T <- with(warpbreaks, table(wool, tension))
> Expand(T)
    wool tension
1      A       L
1.1    A       L
1.2    A       L
1.3    A       L
1.4    A       L
1.5    A       L
1.6    A       L
1.7    A       L
1.8    A       L
2      B       L
2.1    B       L
2.2    B       L
2.3    B       L
2.4    B       L
2.5    B       L
2.6    B       L
2.7    B       L
2.8    B       L
3      A       M
3.1    A       M
3.2    A       M
3.3    A       M
3.4    A       M
3.5    A       M
3.6    A       M
3.7    A       M
3.8    A       M
4      B       M
4.1    B       M
4.2    B       M
4.3    B       M
4.4    B       M
4.5    B       M
4.6    B       M
4.7    B       M
4.8    B       M
5      A       H
5.1    A       H
5.2    A       H
5.3    A       H
5.4    A       H
5.5    A       H
5.6    A       H
5.7    A       H
5.8    A       H
6      B       H
6.1    B       H
6.2    B       H
6.3    B       H
6.4    B       H
6.5    B       H
6.6    B       H
6.7    B       H
6.8    B       H
> 
> 
> 
> cleanEx()
> nameEx("Graph")
> ### * Graph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Graph
> ### Title: Extract graph
> ### Aliases: Graph Graph<-
> ### Keywords: graphs models
> 
> ### ** Examples
> 
> 
> m <- lvm(y~x)
> Graph(m)
NULL
> 
> 
> 
> 
> cleanEx()
> nameEx("Missing")
> ### * Missing
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Missing
> ### Title: Missing value generator
> ### Aliases: Missing Missing, Missing<-
> 
> ### ** Examples
> 
> library(lava)
> set.seed(17)
> m <- lvm(y0~x01+x02+x03)
> m <- Missing(m,formula=x1~x01,Rformula=R1~0.3*x02+-0.7*x01,p=0.4)
> sim(m,10)
           y0         x01        x02         x03 R1        x1
1  -0.3307614  1.18078924  0.6810276 -1.17756957  0        NA
2  -1.0786445  0.64319207 -0.6820334 -0.96016651  0        NA
3  -0.5398741  1.29532187 -0.7232567 -0.87895224  0        NA
4  -2.5119604  0.18791807  1.6735260 -3.55613648  1 0.1879181
5   0.3507905  1.59120510 -0.5957556 -1.41674984  0        NA
6   0.4902836 -0.05517906  1.1598438 -0.44876927  0        NA
7   1.1528003  0.83847112  0.1174224 -0.77596771  1 0.8384711
8   1.3032974  0.15937013  0.2592214 -0.83182805  0        NA
9   1.3153836  0.62595440  0.3823621  0.05183012  1 0.6259544
10 -0.3278672  0.63358473 -0.7114817 -0.61655131  0        NA
> 
> 
> m <- lvm(y~1)
> m <- Missing(m,"y","r")
> ## same as
> ## m <- Missing(m,y~1,r~1)
> sim(m,10)
             y r          y0
1   0.07419352 1  0.07419352
2   1.75169617 0          NA
3  -0.23148744 0          NA
4   0.54345248 0          NA
5  -0.98900140 0          NA
6   0.31553146 1  0.31553146
7   2.44232746 1  2.44232746
8   0.54969286 1  0.54969286
9  -0.02924337 1 -0.02924337
10 -0.83078338 0          NA
> 
> ## same as
> m <- lvm(y~1)
> Missing(m,"y") <- r~x
> sim(m,10)
             y r          y0          x
1   0.03054575 1  0.03054575  0.5602348
2  -0.78551741 0          NA -1.7924178
3   0.32544056 0          NA -1.5654169
4  -0.88084355 0          NA -3.3203189
5   0.20932594 0          NA  0.1547216
6   0.15103295 1  0.15103295 -0.3646267
7  -0.34347879 0          NA -2.4336839
8   0.90587760 0          NA  0.3364643
9   0.91895485 0          NA -0.6404528
10 -0.55598749 1 -0.55598749  1.8211204
> 
> m <- lvm(y~1)
> m <- Missing(m,"y","r",suffix=".")
> ## same as
> ## m <- Missing(m,"y","r",missing.name="y.")
> ## same as
> ## m <- Missing(m,y.~y,"r")
> sim(m,10)
             y r          y.
1   0.46345064 1  0.46345064
2   0.88066627 1  0.88066627
3   0.13942195 0          NA
4  -0.37505483 0          NA
5   0.04253903 0          NA
6   0.63388029 1  0.63388029
7  -1.70748846 1 -1.70748846
8   0.01968655 1  0.01968655
9  -0.29879637 0          NA
10 -0.44176283 1 -0.44176283
> 
> 
> 
> 
> cleanEx()
> nameEx("Model")
> ### * Model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Model
> ### Title: Extract model
> ### Aliases: Model Model<-
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> m <- lvm(y~x)
> e <- estimate(m, sim(m,100))
> Model(e)
Latent Variable Model
                  
  y ~ x   gaussian

Exogenous variables:                   
  x        gaussian

> 
> 
> 
> 
> cleanEx()
> nameEx("backdoor")
> ### * backdoor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: backdoor
> ### Title: Backdoor criterion
> ### Aliases: backdoor
> 
> ### ** Examples
> 
> m <- lvm(y~c2,c2~c1,x~c1,m1~x,y~m1, v1~c3, x~c3,v1~y,
+          x~z1, z2~z1, z2~z3, y~z3+z2+g1+g2+g3)
> ll <- backdoor(m, y~x)
> backdoor(m, y~x|c1+z1+g1)
[1] TRUE
> 
> 
> 
> cleanEx()
> nameEx("blockdiag")
> ### * blockdiag
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: blockdiag
> ### Title: Combine matrices to block diagonal structure
> ### Aliases: blockdiag
> 
> ### ** Examples
> 
> A <- diag(3)+1
> blockdiag(A,A,A,pad=NA)
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
 [1,]    2    1    1   NA   NA   NA   NA   NA   NA
 [2,]    1    2    1   NA   NA   NA   NA   NA   NA
 [3,]    1    1    2   NA   NA   NA   NA   NA   NA
 [4,]   NA   NA   NA    2    1    1   NA   NA   NA
 [5,]   NA   NA   NA    1    2    1   NA   NA   NA
 [6,]   NA   NA   NA    1    1    2   NA   NA   NA
 [7,]   NA   NA   NA   NA   NA   NA    2    1    1
 [8,]   NA   NA   NA   NA   NA   NA    1    2    1
 [9,]   NA   NA   NA   NA   NA   NA    1    1    2
> 
> 
> 
> cleanEx()
> nameEx("bootstrap.lvm")
> ### * bootstrap.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bootstrap.lvm
> ### Title: Calculate bootstrap estimates of a lvm object
> ### Aliases: bootstrap.lvm bootstrap.lvmfit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> m <- lvm(y~x)
> d <- sim(m,100)
> e <- estimate(y~x, d)
> 
> 
> 
> cleanEx()
> nameEx("click")
> ### * click
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: click
> ### Title: Identify points on plot
> ### Aliases: click idplot click.default
> ### Keywords: iplot
> 
> ### ** Examples
> 
> if (interactive()) {
+     n <- 10; x <- seq(n); y <- runif(n)
+     plot(y ~ x); click(x,y)
+ 
+     data(iris)
+     l <- lm(Sepal.Length ~ Sepal.Width*Species,iris)
+     res <- plotConf(l,var2="Species")## ylim=c(6,8), xlim=c(2.5,3.3))
+     with(res, click(x,y))
+ 
+     with(iris, idplot(Sepal.Length,Petal.Length))
+ }
> 
> 
> 
> cleanEx()
> nameEx("closed.testing")
> ### * closed.testing
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: closed.testing
> ### Title: Closed testing procedure
> ### Aliases: closed.testing p.correct
> 
> ### ** Examples
> 
> m <- lvm()
> regression(m, c(y1,y2,y3,y4,y5,y6,y7)~x) <- c(0,0.25,0,0.25,0.25,0,0)
> regression(m, to=endogenous(m), from="u") <- 1
> variance(m,endogenous(m)) <- 1
> set.seed(2)
> d <- sim(m,200)
> l1 <- lm(y1~x,d)
> l2 <- lm(y2~x,d)
> l3 <- lm(y3~x,d)
> l4 <- lm(y4~x,d)
> l5 <- lm(y5~x,d)
> l6 <- lm(y6~x,d)
> l7 <- lm(y7~x,d)
> 
> (a <- merge(l1,l2,l3,l4,l5,l6,l7,subset=2))
    Estimate Std.Err     2.5% 97.5%  P-value
x    -0.0220  0.0993 -0.21668 0.173 8.25e-01
x.1   0.3723  0.1157  0.14564 0.599 1.29e-03
x.2   0.1198  0.1110 -0.09780 0.337 2.81e-01
x.3   0.4223  0.0926  0.24076 0.604 5.14e-06
x.4   0.2934  0.1214  0.05558 0.531 1.56e-02
x.5   0.2057  0.1062 -0.00246 0.414 5.28e-02
x.6   0.0524  0.1182 -0.17922 0.284 6.57e-01
> if (requireNamespace("mets",quietly=TRUE)) {
+    p.correct(a)
+ }
[1] 0.009453147
> as.vector(closed.testing(a))
[1] 0.8246281477 0.0207936759 0.6158722827 0.0001020005 0.0821962107
[6] 0.1907676602 0.7835113360
> 
> 
> 
> 
> cleanEx()
> nameEx("colorbar")
> ### * colorbar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: colorbar
> ### Title: Add color-bar to plot
> ### Aliases: colorbar
> 
> ### ** Examples
> 
> ## Not run: 
> ##D plotNeuro(x,roi=R,mm=-18,range=5)
> ##D colorbar(clut=Col(rev(rainbow(11,start=0,end=0.69)),0.5),
> ##D          x=c(-40,40),y.range=c(84,90),values=c(-5:5))
> ##D 
> ##D colorbar(clut=Col(rev(rainbow(11,start=0,end=0.69)),0.5),
> ##D          x=c(-10,10),y.range=c(-100,50),values=c(-5:5),
> ##D          direction="vertical",border=1)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("compare")
> ### * compare
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: compare
> ### Title: Statistical tests
> ### Aliases: compare
> ### Keywords: htest
> 
> ### ** Examples
> 
> m <- lvm();
> regression(m) <- c(y1,y2,y3) ~ eta; latent(m) <- ~eta
> regression(m) <- eta ~ x
> m2 <- regression(m, c(y3,eta) ~ x)
> set.seed(1)
> d <- sim(m,1000)
> e <- estimate(m,d)
> e2 <- estimate(m2,d)
> 
> compare(e)

	- Likelihood ratio test -

data:  
chisq = 2.7373, df = 2, p-value = 0.2544
sample estimates:
          log likelihood (model) log likelihood (saturated model) 
                       -5045.863                        -5044.494 

> 
> compare(e,e2) ## LRT, H0: y3<-x=0

	- Likelihood ratio test -

data:  
chisq = 1.6297, df = 1, p-value = 0.2017
sample estimates:
log likelihood (model 1) log likelihood (model 2) 
               -5045.863                -5045.048 

> compare(e,scoretest=y3~x) ## Score-test, H0: y3~x=0

	- Score test -

data:  y3 ~ x
chisq = 1.6059, df = 1, p-value = 0.2051

> compare(e2,par=c("y3~x")) ## Wald-test, H0: y3~x=0

	- Wald test -

	Null Hypothesis:
	[y3~x] = 0

data:  
chisq = 1.5752, df = 1, p-value = 0.2095
sample estimates:
          Estimate    Std.Err     2.5%      97.5%
[y3~x] -0.08157255 0.06499477 -0.20896 0.04581487

> 
> B <- diag(2); colnames(B) <- c("y2~eta","y3~eta")
> compare(e2,contrast=B,null=c(1,1))

	- Wald test -

	Null Hypothesis:
	[y2~eta] = 1
	[y3~eta] = 1

data:  
chisq = 0.40264, df = 2, p-value = 0.8177
sample estimates:
         Estimate    Std.Err      2.5%    97.5%
[y2~eta] 1.019845 0.03770718 0.9459398 1.093749
[y3~eta] 1.028685 0.05598807 0.9189509 1.138420

> 
> B <- rep(0,length(coef(e2))); B[1:3] <- 1
> compare(e2,contrast=B)

	- Wald test -

	Null Hypothesis:
	[y2] + [y3] + [eta] = 0

data:  
chisq = 0.15653, df = 1, p-value = 0.6924
sample estimates:
                      Estimate    Std.Err       2.5%     97.5%
[y2] + [y3] + [eta] 0.02605068 0.06584406 -0.1030013 0.1551027

> 
> compare(e,scoretest=list(y3~x,y2~x))

	- Score test -

data:  y3 ~ xy2 ~ x
chisq = 2.7607, df = 2, p-value = 0.2515

> 
> 
> 
> cleanEx()
> nameEx("confband")
> ### * confband
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confband
> ### Title: Add Confidence limits bar to plot
> ### Aliases: confband forestplot
> ### Keywords: iplot
> 
> ### ** Examples
> 
> plot(0,0,type="n",xlab="",ylab="")
> confband(0.5,-0.5,0.5,0,col="darkblue")
> confband(0.8,-0.5,0.5,0,col="darkred",vert=FALSE,pch=1,cex=1.5)
> 
> set.seed(1)
> K <- 20
> est <- rnorm(K)
> se <- runif(K,0.2,0.4)
> x <- cbind(est,est-2*se,est+2*se,runif(K,0.5,2))
> x[c(3:4,10:12),] <- NA
> rownames(x) <- unlist(lapply(letters[seq(K)],function(x) paste(rep(x,4),collapse="")))
> rownames(x)[which(is.na(est))] <- ""
> signif <- sign(x[,2])==sign(x[,3])
> forestplot(x,text.right=FALSE)
> forestplot(x[,-4],sep=c(2,15),col=signif+1,box1=TRUE,delta=0.2,pch=16,cex=1.5)
> forestplot(x,vert=TRUE,text=FALSE)
> forestplot(x,vert=TRUE,text=FALSE,pch=NA)
> ##forestplot(x,vert=TRUE,text.vert=FALSE)
> ##forestplot(val,vert=TRUE,add=TRUE)
> 
> z <- seq(10)
> zu <- c(z[-1],10)
> plot(z,type="n")
> confband(z,zu,rep(0,length(z)),col=Col("darkblue"),polygon=TRUE,step=TRUE)
> confband(z,zu,zu-2,col=Col("darkred"),polygon=TRUE,step=TRUE)
> 
> z <- seq(0,1,length.out=100)
> plot(z,z,type="n")
> confband(z,z,z^2,polygon="TRUE",col=Col("darkblue"))
> 
> set.seed(1)
> k <- 10
> x <- seq(k)
> est <- rnorm(k)
> sd <- runif(k)
> val <- cbind(x,est,est-sd,est+sd)
> par(mfrow=c(1,2))
> plot(0,type="n",xlim=c(0,k+1),ylim=range(val[,-1]),axes=FALSE,xlab="",ylab="")
> axis(2)
> confband(val[,1],val[,3],val[,4],val[,2],pch=16,cex=2)
> plot(0,type="n",ylim=c(0,k+1),xlim=range(val[,-1]),axes=FALSE,xlab="",ylab="")
> axis(1)
> confband(val[,1],val[,3],val[,4],val[,2],pch=16,cex=2,vert=FALSE)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("confint.lvmfit")
> ### * confint.lvmfit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: confint.lvmfit
> ### Title: Calculate confidence limits for parameters
> ### Aliases: confint.lvmfit confint.multigroupfit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm(y~x)
> d <- sim(m,100)
> e <- estimate(y~x, d)
> confint(e,3,profile=TRUE)
         2.5 %   97.5 %
y~~y 0.6127897 1.068052
> confint(e,3)
         2.5 %   97.5 %
y~~y 0.5773112 1.020076
> 
> 
> 
> cleanEx()
> nameEx("constrain-set")
> ### * constrain-set
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: constrain<-
> ### Title: Add non-linear constraints to latent variable model
> ### Aliases: constrain<- constrain constrain.default constrain<-.multigroup
> ###   constrain<-.default constraints parameter<-
> ### Keywords: models regression
> 
> ### ** Examples
> 
> ##############################
> ### Non-linear parameter constraints 1
> ##############################
> m <- lvm(y ~ f(x1,gamma)+f(x2,beta))
> covariance(m) <- y ~ f(v)
> d <- sim(m,100)
> m1 <- m; constrain(m1,beta ~ v) <- function(x) x^2
> ## Define slope of x2 to be the square of the residual variance of y
> ## Estimate both restricted and unrestricted model
> e <- estimate(m,d,control=list(method="NR"))
Warning in control0$backtrack * rbind(D) %*% Delta :
  Recycling array of length 1 in vector-array arithmetic is deprecated.
  Use c() or as.vector() instead.

Warning in control0$backtrack * rbind(D) %*% Delta :
  Recycling array of length 1 in vector-array arithmetic is deprecated.
  Use c() or as.vector() instead.

Warning in control0$backtrack * rbind(D) %*% Delta :
  Recycling array of length 1 in vector-array arithmetic is deprecated.
  Use c() or as.vector() instead.

> e1 <- estimate(m1,d)
> p1 <- coef(e1)
> p1 <- c(p1[1:2],p1[3]^2,p1[3])
> ## Likelihood of unrestricted model evaluated in MLE of restricted model
> logLik(e,p1)
'log Lik.' -131.7795 (df=4)
> ## Likelihood of restricted model (MLE)
> logLik(e1)
'log Lik.' -131.7795 (df=3)
> 
> ##############################
> ### Non-linear regression
> ##############################
> 
> ## Simulate data
> m <- lvm(c(y1,y2)~f(x,0)+f(eta,1))
> latent(m) <- ~eta
> covariance(m,~y1+y2) <- "v"
> intercept(m,~y1+y2) <- "mu"
> covariance(m,~eta) <- "zeta"
> intercept(m,~eta) <- 0
> set.seed(1)
> d <- sim(m,100,p=c(v=0.01,zeta=0.01))[,manifest(m)]
> d <- transform(d,
+                y1=y1+2*pnorm(2*x),
+                y2=y2+2*pnorm(2*x))
> 
> ## Specify model and estimate parameters
> constrain(m, mu ~ x + alpha + nu + gamma) <- function(x) x[4]*pnorm(x[3]+x[1]*x[2])
> 
> ##############################
> ### Multigroup model
> ##############################
> ### Define two models
> m1 <- lvm(y ~ f(x,beta)+f(z,beta2))
> m2 <- lvm(y ~ f(x,psi) + z)
> ### And simulate data from them
> d1 <- sim(m1,500)
> d2 <- sim(m2,500)
> ### Add 'non'-linear parameter constraint
> constrain(m2,psi ~ beta2) <- function(x) x
> ## Add parameter beta2 to model 2, now beta2 exists in both models
> parameter(m2) <- ~ beta2
> ee <- estimate(list(m1,m2),list(d1,d2),control=list(method="NR"))
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
> summary(ee)
||score||^2= 5.575583e-11 
Latent variables: 
____________________________________________________
Group 1 (n=500)
                    Estimate Std. Error  Z value Pr(>|z|)
Regressions:                                             
   y~x               0.93621    0.04767 19.64130   <1e-12
   y~z               1.03489    0.03217 32.17115   <1e-12
Intercepts:                                              
   y                -0.05578    0.04814 -1.15869   0.2466
Residual Variances:                                      
   y                 1.15827    0.07326 15.81139         
____________________________________________________
Group 2 (n=500)
                       Estimate Std. Error  Z value Pr(>|z|)
Regressions:                                                
   y~z                  0.96338    0.04278 22.51997   <1e-12
Intercepts:                                                 
   y                   -0.01359    0.04657 -0.29191   0.7704
Additional Parameters:                                      
   beta2                1.03489    0.03217 32.17115   <1e-12
Residual Variances:                                         
   y                    1.07306    0.06787 15.81139         

______________________________________________________________________
Non-linear constraints:
    Estimate Std. Error  Z value      Pr(>|z|)      2.5%    97.5%
psi 1.034886 0.03216812 32.17115 4.470272e-227 0.9718373 1.097934
______________________________________________________________________
Estimator: gaussian 
______________________________________________________________________

 Number of observations = 1000 
 BIC = 2994.955 
 AIC = 2960.6 
 log-Likelihood of model = -1473.3 

 log-Likelihood of saturated model = -1473.128 
 Chi-squared statistic: q = 0.3442204 , df = 1 
  P(Q>q) = 0.5574032 

 RMSEA (90% CI): 0 (0;0.0698)
  P(RMSEA<0.05)=0.85508

rank(Information) = 7 (p=7)
condition(Information) = 0.1917382
mean(score^2) = 7.965119e-12 
______________________________________________________________________
> 
> m3 <- lvm(y ~ f(x,beta)+f(z,beta2))
> m4 <- lvm(y ~ f(x,beta2) + z)
> e2 <- estimate(list(m3,m4),list(d1,d2),control=list(method="NR"))
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
> e2
____________________________________________________
Group 1 (n=500)
                    Estimate Std. Error  Z value Pr(>|z|)
Regressions:                                             
   y~x               0.93621    0.04767 19.64130   <1e-12
   y~z               1.03489    0.03217 32.17115   <1e-12
Intercepts:                                              
   y                -0.05578    0.04814 -1.15869   0.2466
Residual Variances:                                      
   y                 1.15827    0.07326 15.81139         
____________________________________________________
Group 2 (n=500)
                    Estimate Std. Error  Z value Pr(>|z|)
Regressions:                                             
   y~x               1.03489    0.03217 32.17115   <1e-12
   y~z               0.96338    0.04278 22.51997   <1e-12
Intercepts:                                              
   y                -0.01359    0.04657 -0.29191   0.7704
Residual Variances:                                      
   y                 1.07306    0.06787 15.81139         

> 
> 
> 
> cleanEx()
> nameEx("contr")
> ### * contr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: contr
> ### Title: Create contrast matrix
> ### Aliases: contr parsedesign
> 
> ### ** Examples
> 
> contr(2,n=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    0    0    0
> contr(as.list(2:4),n=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1    0    0    0
[2,]    0    0    1    0    0
[3,]    0    0    0    1    0
> contr(list(1,2,4),n=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]    0    1    0    0    0
[3,]    0    0    0    1    0
> contr(c(2,3,4),n=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    0    1   -1    0    0
[2,]    0    1    0   -1    0
> contr(list(c(1,3),c(2,4)),n=5)
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0   -1    0    0
[2,]    0    1    0   -1    0
> contr(list(c(1,3),c(2,4),5))
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0   -1    0    0
[2,]    0    1    0   -1    0
[3,]    0    0    0    0    1
> 
> parsedesign(c("aa","b","c"),"?","?",diff=c(FALSE,TRUE))
     [,1] [,2] [,3]
[1,]    0    1    0
[2,]    0    0    1
[3,]    0    1   -1
> 
> 
> 
> cleanEx()
> nameEx("covariance")
> ### * covariance
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: covariance
> ### Title: Add covariance structure to Latent Variable Model
> ### Aliases: covariance covariance<- covariance.lvm covariance<-.lvm
> ###   covfix<- covfix covfix<-.lvm covfix.lvm variance variance<-
> ###   variance.lvm variance<-.lvm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm()
> ### Define covariance between residuals terms of y1 and y2
> covariance(m) <- y1~y2
> covariance(m) <- c(y1,y2)~f(v) ## Same marginal variance
> covariance(m) ## Examine covariance structure
Covariance parameters:
      y1 y2
   y1 v  * 
   y2 *  v 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("density.sim")
> ### * density.sim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: density.sim
> ### Title: Plot sim object
> ### Aliases: density.sim plot.sim
> 
> ### ** Examples
> 
> n <- 1000
> val <- cbind(est1=rnorm(n,sd=1),est2=rnorm(n,sd=0.2),est3=rnorm(n,1,sd=0.5),
+              sd1=runif(n,0.8,1.2),sd2=runif(n,0.1,0.3),sd3=runif(n,0.25,0.75))
> 
> plot.sim(val,estimate=c(1,2),true=c(0,0),se=c(4,5),equal=TRUE)
> plot.sim(val,estimate=c(1,3),true=c(0,1),se=c(4,6),density.xlim=c(-3,3),ylim=c(-3,3))
Warning in graphics::rug(y, col = Col(col[1], rug.alpha[1])) :
  some values will be clipped
> plot.sim(val,estimate=c(1,2),true=c(0,0),se=c(4,5),equal=TRUE,plot.type="single")
> plot.sim(val,estimate=c(1),se=c(4,5,6),plot.type="single")
> plot.sim(val,estimate=c(1,2,3),equal=TRUE)
> plot.sim(val,estimate=c(1,2,3),equal=TRUE,byrow=TRUE)
> plot.sim(val,estimate=c(1,2,3),plot.type="single")
> plot.sim(val,estimate=1,se=c(3,4,5),plot.type="single")
> 
> density.sim(val,estimate=c(1,2,3),polygon.density=c(0,10,10),polygon.angle=c(0,45,-45))
> 
> 
> 
> cleanEx()
> nameEx("diagtest")
> ### * diagtest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diagtest
> ### Title: Calculate diagnostic tests for 2x2 table
> ### Aliases: diagtest odds riskcomp OR Ratio Diff
> 
> ### ** Examples
> 
> M <- as.table(matrix(c(42,12,
+                        35,28),ncol=2,byrow=TRUE,
+                      dimnames=list(rater=c("no","yes"),gold=c("no","yes"))))
> diagtest(M,exact=TRUE)
                        Estimate Std.Err  2.5% 97.5% P-value
Prevalence                 0.342      NA 0.257 0.435      NA
Test                       0.538      NA 0.444 0.631      NA
Sensitivity                0.700      NA 0.535 0.834      NA
Specificity                0.545      NA 0.428 0.659      NA
PositivePredictiveValue    0.444      NA 0.319 0.575      NA
NegativePredictiveValue    0.778      NA 0.644 0.880      NA
Accuracy                   0.598      NA 0.504 0.688      NA
Homogeneity                0.745      NA 0.597 0.861 0.00109
> 
> 
> 
> cleanEx()
> nameEx("dsep.lvm")
> ### * dsep.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dsep.lvm
> ### Title: Check d-separation criterion
> ### Aliases: dsep.lvm dsep
> 
> ### ** Examples
> 
> m <- lvm(x5 ~ x4+x3, x4~x3+x1, x3~x2, x2~x1)
> if (interactive()) {
+ plot(m,layoutType='neato')
+ }
> dsep(m,x5~x1|x2+x4)
[1] FALSE
> dsep(m,x5~x1|x3+x4)
[1] TRUE
> dsep(m,~x1+x2+x3|x4)
[1] FALSE
> 
> 
> 
> 
> cleanEx()
> nameEx("estimate.default")
> ### * estimate.default
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: estimate.default
> ### Title: Estimation of functional of parameters
> ### Aliases: estimate.default estimate estimate.estimate merge.estimate
> 
> ### ** Examples
> 
> 
> ## Simulation from logistic regression model
> m <- lvm(y~x+z);
> distribution(m,y~x) <- binomial.lvm("logit")
> d <- sim(m,1000)
> g <- glm(y~z+x,data=d,family=binomial())
> g0 <- glm(y~1,data=d,family=binomial())
> 
> ## LRT
> estimate(g,g0)

	- Likelihood ratio test -

data:  
chisq = 213.03, df = 2, p-value < 2.2e-16
sample estimates:
log likelihood (model 1) log likelihood (model 2) 
               -563.5555                -670.0711 

> 
> ## Plain estimates (robust standard errors)
> estimate(g)
            Estimate Std.Err   2.5% 97.5%  P-value
(Intercept)   0.0782  0.0971 -0.112 0.269 4.20e-01
z             0.9360  0.0809  0.777 1.094 5.56e-31
x             0.9955  0.1462  0.709 1.282 9.85e-12
> 
> ## Testing contrasts
> estimate(g,null=0)
              Estimate Std.Err   2.5% 97.5%  P-value
[(Intercept)]   0.0782  0.0971 -0.112 0.269 4.20e-01
[z]             0.9360  0.0809  0.777 1.094 5.56e-31
[x]             0.9955  0.1462  0.709 1.282 9.85e-12

 Null Hypothesis: 
  [(Intercept)] = 0
  [z] = 0
  [x] = 0 
 
chisq = 191.1158, df = 3, p-value <2e-16
> estimate(g,rbind(c(1,1,0),c(1,0,2)))
                     Estimate Std.Err  2.5% 97.5%  P-value
[(Intercept)] + [z]      1.01   0.133 0.754  1.27 1.96e-14
[(Intercept)] + 2[x]     2.07   0.241 1.597  2.54 9.22e-18

 Null Hypothesis: 
  [(Intercept)] + [z] = 0
  [(Intercept)] + 2[x] = 0 
 
chisq = 164.5653, df = 2, p-value <2e-16
> estimate(g,rbind(c(1,1,0),c(1,0,2)),null=c(1,2))
                     Estimate Std.Err  2.5% 97.5% P-value
[(Intercept)] + [z]      1.01   0.133 0.754  1.27   0.914
[(Intercept)] + 2[x]     2.07   0.241 1.597  2.54   0.774

 Null Hypothesis: 
  [(Intercept)] + [z] = 1
  [(Intercept)] + 2[x] = 2 
 
chisq = 0.1103, df = 2, p-value = 0.946
> estimate(g,2:3) ## same as cbind(0,1,-1)
          Estimate Std.Err   2.5% 97.5% P-value
[z] - [x]  -0.0595   0.161 -0.376 0.257   0.712

 Null Hypothesis: 
  [z] - [x] = 0 
> estimate(g,as.list(2:3)) ## same as rbind(c(0,1,0),c(0,0,1))
    Estimate Std.Err  2.5% 97.5%  P-value
[z]    0.936  0.0809 0.777  1.09 5.56e-31
[x]    0.995  0.1462 0.709  1.28 9.85e-12

 Null Hypothesis: 
  [z] = 0
  [x] = 0 
 
chisq = 168.8459, df = 2, p-value <2e-16
> ## Alternative syntax
> estimate(g,"z","z"-"x",2*"z"-3*"x")
            Estimate Std.Err   2.5%  97.5%  P-value
[z]           0.9360  0.0809  0.777  1.094 5.56e-31
[z] - [x]    -0.0595  0.1613 -0.376  0.257 7.12e-01
2[z] - 3[x]  -1.1144  0.4552 -2.007 -0.222 1.44e-02

 Null Hypothesis: 
  [z] = 0
  [z] - [x] = 0
  2[z] - 3[x] = 0 
 
chisq = 168.8459, df = 2, p-value <2e-16
> estimate(g,z,z-x,2*z-3*x)
            Estimate Std.Err   2.5%  97.5%  P-value
[z]           0.9360  0.0809  0.777  1.094 5.56e-31
[z] - [x]    -0.0595  0.1613 -0.376  0.257 7.12e-01
2[z] - 3[x]  -1.1144  0.4552 -2.007 -0.222 1.44e-02

 Null Hypothesis: 
  [z] = 0
  [z] - [x] = 0
  2[z] - 3[x] = 0 
 
chisq = 168.8459, df = 2, p-value <2e-16
> estimate(g,"?")  ## Wilcards
          Estimate Std.Err   2.5% 97.5% P-value
[z] - [x]  -0.0595   0.161 -0.376 0.257   0.712

 Null Hypothesis: 
  [z] - [x] = 0 
> estimate(g,"*Int*","z")
              Estimate Std.Err   2.5% 97.5%  P-value
[(Intercept)]   0.0782  0.0971 -0.112 0.269 4.20e-01
[z]             0.9360  0.0809  0.777 1.094 5.56e-31

 Null Hypothesis: 
  [(Intercept)] = 0
  [z] = 0 
 
chisq = 134.105, df = 2, p-value <2e-16
> estimate(g,"1","2"-"3",null=c(0,1))
              Estimate Std.Err   2.5% 97.5%  P-value
[(Intercept)]   0.0782  0.0971 -0.112 0.269 4.20e-01
[z] - [x]      -0.0595  0.1613 -0.376 0.257 5.15e-11

 Null Hypothesis: 
  [(Intercept)] = 0
  [z] - [x] = 1 
 
chisq = 85.3097, df = 2, p-value <2e-16
> estimate(g,2,3)
    Estimate Std.Err  2.5% 97.5%  P-value
[z]    0.936  0.0809 0.777  1.09 5.56e-31
[x]    0.995  0.1462 0.709  1.28 9.85e-12

 Null Hypothesis: 
  [z] = 0
  [x] = 0 
 
chisq = 168.8459, df = 2, p-value <2e-16
> 
> ## Usual (non-robust) confidence intervals
> estimate(g,robust=FALSE)
            Estimate Std.Err   2.5% 97.5%  P-value
(Intercept)   0.0782  0.0961 -0.110 0.266 4.15e-01
z             0.9360  0.0822  0.775 1.097 4.55e-30
x             0.9955  0.1471  0.707 1.284 1.32e-11
> 
> ## Transformations
> estimate(g,function(p) p[1]+p[2])
            Estimate Std.Err  2.5% 97.5%  P-value
(Intercept)     1.01   0.133 0.754  1.27 1.96e-14
> 
> ## Multiple parameters
> e <- estimate(g,function(p) c(p[1]+p[2],p[1]*p[2]))
> e
              Estimate Std.Err   2.5% 97.5%  P-value
(Intercept)     1.0142  0.1325  0.754 1.274 1.96e-14
(Intercept).1   0.0732  0.0917 -0.107 0.253 4.25e-01
> vcov(e)
            (Intercept) (Intercept)
(Intercept)  0.01756149 0.010143200
(Intercept)  0.01014320 0.008414001
> 
> ## Label new parameters
> estimate(g,function(p) list("a1"=p[1]+p[2],"b1"=p[1]*p[2]))
   Estimate Std.Err   2.5% 97.5%  P-value
a1   1.0142  0.1325  0.754 1.274 1.96e-14
b1   0.0732  0.0917 -0.107 0.253 4.25e-01
> ##'
> ## Multiple group
> m <- lvm(y~x)
> m <- baptize(m)
> d2 <- d1 <- sim(m,50)
> e <- estimate(list(m,m),list(d1,d2))
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
> estimate(e) ## Wrong
       Estimate Std.Err   2.5% 97.5%  P-value
1@y     -0.0884   0.112 -0.308 0.132 4.31e-01
1@y~x    0.8623   0.105  0.656 1.069 2.90e-16
1@y~~y   1.2543   0.142  0.976 1.532 9.33e-19
> estimate(e,id=rep(seq(nrow(d1)),2))
       Estimate Std.Err   2.5% 97.5%  P-value
1@y     -0.0884   0.159 -0.400 0.223 5.78e-01
1@y~x    0.8623   0.149  0.570 1.155 7.36e-09
1@y~~y   1.2543   0.201  0.861 1.647 4.03e-10
> estimate(lm(y~x,d1))
            Estimate Std.Err  2.5% 97.5%  P-value
(Intercept)  -0.0884   0.159 -0.40 0.223 5.78e-01
x             0.8623   0.149  0.57 1.155 7.36e-09
> 
> ## Marginalize
> f <- function(p,data)
+   list(p0=lava:::expit(p["(Intercept)"] + p["z"]*data[,"z"]),
+        p1=lava:::expit(p["(Intercept)"] + p["x"] + p["z"]*data[,"z"]))
> e <- estimate(g, f, average=TRUE)
> e
   Estimate Std.Err  2.5% 97.5%   P-value
p0    0.514  0.0211 0.473 0.556 4.10e-131
p1    0.710  0.0200 0.670 0.749 4.45e-275
> estimate(e,diff)
   Estimate Std.Err  2.5% 97.5% P-value
p1    0.195  0.0279 0.141  0.25 2.4e-12
> estimate(e,cbind(1,1))
            Estimate Std.Err 2.5% 97.5% P-value
[p0] + [p1]     1.22  0.0303 1.16  1.28       0

 Null Hypothesis: 
  [p0] + [p1] = 0 
> 
> ## Clusters and subset (conditional marginal effects)
> d$id <- rep(seq(nrow(d)/4),each=4)
> estimate(g,function(p,data)
+          list(p0=lava:::expit(p[1] + p["z"]*data[,"z"])),
+          subset=d$z>0, id=d$id, average=TRUE)
   Estimate Std.Err  2.5% 97.5%  P-value
p0     0.69   0.025 0.641 0.739 2.2e-168
> 
> ## More examples with clusters:
> m <- lvm(c(y1,y2,y3)~u+x)
> d <- sim(m,10)
> l1 <- glm(y1~x,data=d)
> l2 <- glm(y2~x,data=d)
> l3 <- glm(y3~x,data=d)
> 
> ## Some random id-numbers
> id1 <- c(1,1,4,1,3,1,2,3,4,5)
> id2 <- c(1,2,3,4,5,6,7,8,1,1)
> id3 <- seq(10)
> 
> ## Un-stacked and stacked i.i.d. decomposition
> iid(estimate(l1,id=id1,stack=FALSE))
  (Intercept)            x
1 -0.02846736  0.012951967
1 -0.06508757  0.073677746
4 -0.06429045 -0.120971471
1  0.13421361  0.018130506
3  0.21214235  0.077169067
1  0.10901243 -0.080488677
2 -0.01003826 -0.008297596
3 -0.04091701 -0.012076765
4 -0.02472170  0.015202153
5 -0.22184605  0.024703072
attr(,"bread")
             (Intercept)            x
(Intercept)  0.171567789 -0.007445287
x           -0.007445287  0.109047118
> iid(estimate(l1,id=id1))
  (Intercept)            x
1  0.14967112  0.024271541
2 -0.01003826 -0.008297596
3  0.17122535  0.065092302
4 -0.08901215 -0.105769318
5 -0.22184605  0.024703072
attr(,"bread")
             (Intercept)            x
(Intercept)  0.171567789 -0.007445287
x           -0.007445287  0.109047118
attr(,"N")
[1] 10
> 
> ## Combined i.i.d. decomposition
> e1 <- estimate(l1,id=id1)
> e2 <- estimate(l2,id=id2)
> e3 <- estimate(l3,id=id3)
> (a2 <- merge(e1,e2,e3))
              Estimate Std.Err   2.5%   97.5%  P-value
(Intercept)     -0.623  0.3301 -1.270  0.0241 5.92e-02
x                1.994  0.1292  1.741  2.2470 1.00e-53
(Intercept).1   -0.614  0.1948 -0.995 -0.2318 1.64e-03
x.1              1.352  0.0819  1.192  1.5129 2.74e-61
(Intercept).2   -0.407  0.4663 -1.321  0.5067 3.82e-01
x.2              1.845  0.3227  1.212  2.4773 1.08e-08
> 
> ## If all models were estimated on the same data we could use the
> ## syntax:
> ## Reduce(merge,estimate(list(l1,l2,l3)))
> 
> ## Same:
> iid(a1 <- merge(l1,l2,l3,id=list(id1,id2,id3)))
   (Intercept)            x (Intercept).1          x.1 (Intercept).2
1   0.14967112  0.024271541  -0.149892233  0.048723952   0.004219021
2  -0.01003826 -0.008297596  -0.009369173  0.010605705  -0.151889856
3   0.17122535  0.065092302  -0.010244382 -0.019276237  -0.029623861
4  -0.08901215 -0.105769318   0.065316432  0.008823397   0.030651651
5  -0.22184605  0.024703072   0.072763879  0.026468645   0.304166941
6   0.00000000  0.000000000   0.070437765 -0.052007301   0.142228318
7   0.00000000  0.000000000  -0.022248025 -0.018390143  -0.248640618
8   0.00000000  0.000000000  -0.016764262 -0.004948017   0.033053670
9   0.00000000  0.000000000   0.000000000  0.000000000  -0.123816275
10  0.00000000  0.000000000   0.000000000  0.000000000   0.039651010
            x.2
1  -0.001919554
2   0.171936094
3  -0.055741436
4   0.004140638
5   0.110644001
6  -0.105013426
7  -0.205525501
8   0.009755879
9   0.076138536
10 -0.004415232
> 
> iid(merge(l1,l2,l3,id=TRUE)) # one-to-one (same clusters)
   (Intercept)            x (Intercept).1          x.1 (Intercept).2
1  -0.02846736  0.012951967   0.004473809 -0.002035476   0.004219021
2  -0.06508757  0.073677746  -0.009369173  0.010605705  -0.151889856
3  -0.06429045 -0.120971471  -0.010244382 -0.019276237  -0.029623861
4   0.13421361  0.018130506   0.065316432  0.008823397   0.030651651
5   0.21214235  0.077169067   0.072763879  0.026468645   0.304166941
6   0.10901243 -0.080488677   0.070437765 -0.052007301   0.142228318
7  -0.01003826 -0.008297596  -0.022248025 -0.018390143  -0.248640618
8  -0.04091701 -0.012076765  -0.016764262 -0.004948017   0.033053670
9  -0.02472170  0.015202153  -0.066663611  0.040993559  -0.123816275
10 -0.22184605  0.024703072  -0.087702431  0.009765869   0.039651010
            x.2
1  -0.001919554
2   0.171936094
3  -0.055741436
4   0.004140638
5   0.110644001
6  -0.105013426
7  -0.205525501
8   0.009755879
9   0.076138536
10 -0.004415232
> iid(merge(l1,l2,l3,id=FALSE)) # independence
   (Intercept)            x (Intercept).1          x.1 (Intercept).2
1  -0.02846736  0.012951967   0.000000000  0.000000000   0.000000000
2  -0.06508757  0.073677746   0.000000000  0.000000000   0.000000000
3  -0.06429045 -0.120971471   0.000000000  0.000000000   0.000000000
4   0.13421361  0.018130506   0.000000000  0.000000000   0.000000000
5   0.21214235  0.077169067   0.000000000  0.000000000   0.000000000
6   0.10901243 -0.080488677   0.000000000  0.000000000   0.000000000
7  -0.01003826 -0.008297596   0.000000000  0.000000000   0.000000000
8  -0.04091701 -0.012076765   0.000000000  0.000000000   0.000000000
9  -0.02472170  0.015202153   0.000000000  0.000000000   0.000000000
10 -0.22184605  0.024703072   0.000000000  0.000000000   0.000000000
11  0.00000000  0.000000000   0.004473809 -0.002035476   0.000000000
12  0.00000000  0.000000000  -0.009369173  0.010605705   0.000000000
13  0.00000000  0.000000000  -0.010244382 -0.019276237   0.000000000
14  0.00000000  0.000000000   0.065316432  0.008823397   0.000000000
15  0.00000000  0.000000000   0.072763879  0.026468645   0.000000000
16  0.00000000  0.000000000   0.070437765 -0.052007301   0.000000000
17  0.00000000  0.000000000  -0.022248025 -0.018390143   0.000000000
18  0.00000000  0.000000000  -0.016764262 -0.004948017   0.000000000
19  0.00000000  0.000000000  -0.066663611  0.040993559   0.000000000
20  0.00000000  0.000000000  -0.087702431  0.009765869   0.000000000
21  0.00000000  0.000000000   0.000000000  0.000000000   0.004219021
22  0.00000000  0.000000000   0.000000000  0.000000000  -0.151889856
23  0.00000000  0.000000000   0.000000000  0.000000000  -0.029623861
24  0.00000000  0.000000000   0.000000000  0.000000000   0.030651651
25  0.00000000  0.000000000   0.000000000  0.000000000   0.304166941
26  0.00000000  0.000000000   0.000000000  0.000000000   0.142228318
27  0.00000000  0.000000000   0.000000000  0.000000000  -0.248640618
28  0.00000000  0.000000000   0.000000000  0.000000000   0.033053670
29  0.00000000  0.000000000   0.000000000  0.000000000  -0.123816275
30  0.00000000  0.000000000   0.000000000  0.000000000   0.039651010
            x.2
1   0.000000000
2   0.000000000
3   0.000000000
4   0.000000000
5   0.000000000
6   0.000000000
7   0.000000000
8   0.000000000
9   0.000000000
10  0.000000000
11  0.000000000
12  0.000000000
13  0.000000000
14  0.000000000
15  0.000000000
16  0.000000000
17  0.000000000
18  0.000000000
19  0.000000000
20  0.000000000
21 -0.001919554
22  0.171936094
23 -0.055741436
24  0.004140638
25  0.110644001
26 -0.105013426
27 -0.205525501
28  0.009755879
29  0.076138536
30 -0.004415232
> 
> 
> ## Monte Carlo approach, simple trend test example
> 
> m <- categorical(lvm(),~x,K=5)
> regression(m,additive=TRUE) <- y~x
> d <- simulate(m,100,seed=1,'y~x'=0.1)
> l <- lm(y~-1+factor(x),data=d)
> 
> f <- function(x) coef(lm(x~seq_along(x)))[2]
> null <- rep(mean(coef(l)),length(coef(l))) ## just need to make sure we simulate under H0: slope=0
> estimate(l,f,R=1e2,null.sim=null)
100 replications

         seq_along(x)
Mean       -0.0043673
SD          0.0547624
                     
2.5%       -0.1215400
97.5%       0.0978007
                     
Estimate    0.0809492
P-value     0.1600000

> 
> estimate(l,f)
             Estimate Std.Err    2.5% 97.5% P-value
seq_along(x)   0.0809  0.0613 -0.0393 0.201   0.187
> 
> 
> 
> cleanEx()
> nameEx("estimate.lvm")
> ### * estimate.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: estimate.lvm
> ### Title: Estimation of parameters in a Latent Variable Model (lvm)
> ### Aliases: estimate.lvm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> dd <- read.table(header=TRUE,
+ text="x1 x2 x3
+  0.0 -0.5 -2.5
+ -0.5 -2.0  0.0
+  1.0  1.5  1.0
+  0.0  0.5  0.0
+ -2.5 -1.5 -1.0")
> e <- estimate(lvm(c(x1,x2,x3)~u),dd)
> 
> ## Simulation example
> m <- lvm(list(y~v1+v2+v3+v4,c(v1,v2,v3,v4)~x))
> covariance(m) <- v1~v2+v3+v4
> dd <- sim(m,10000) ## Simulate 10000 observations from model
> e <- estimate(m, dd) ## Estimate parameters
> e
                     Estimate Std. Error   Z-value  P-value
Regressions:                                               
   y~v1               1.01646    0.01817  55.95107   <1e-12
   y~v2               0.99124    0.01112  89.17978   <1e-12
   y~v3               0.99614    0.01106  90.10392   <1e-12
   y~v4               0.97903    0.01098  89.18408   <1e-12
    v1~x              0.98424    0.00999  98.55298   <1e-12
   v2~x               0.99048    0.01012  97.89029   <1e-12
    v3~x              0.99910    0.00999 100.01601   <1e-12
   v4~x               0.98743    0.01017  97.11212   <1e-12
Intercepts:                                                
   y                 -0.00671    0.01012  -0.66279   0.5075
   v1                -0.00421    0.00994  -0.42364   0.6718
   v2                 0.00632    0.01007   0.62692   0.5307
   v3                -0.00670    0.00995  -0.67355   0.5006
   v4                -0.00475    0.01012  -0.46952   0.6387
Residual Variances:                                        
   y                  1.02422    0.01448  70.71068         
   v1                 0.98859    0.01108  89.19954         
   v1~~v2             0.50674    0.00876  57.83388   <1e-12
   v1~~v3             0.48934    0.00852  57.40580   <1e-12
   v1~~v4             0.49901    0.00869  57.44175   <1e-12
   v2                 1.01476    0.01435  70.71068         
   v3                 0.98908    0.01399  70.71068         
   v4                 1.02476    0.01449  70.71068         
> 
> ## Using just sufficient statistics
> n <- nrow(dd)
> e0 <- estimate(m,data=list(S=cov(dd)*(n-1)/n,mu=colMeans(dd),n=n))
> rm(dd)
> 
> ## Multiple group analysis
> m <- lvm()
> regression(m) <- c(y1,y2,y3)~u
> regression(m) <- u~x
> d1 <- sim(m,100,p=c("u,u"=1,"u~x"=1))
> d2 <- sim(m,100,p=c("u,u"=2,"u~x"=-1))
> 
> mm <- baptize(m)
> regression(mm,u~x) <- NA
> covariance(mm,~u) <- NA
> intercept(mm,~u) <- NA
> ee <- estimate(list(mm,mm),list(d1,d2))
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
Warning in is.na(ex) :
  is.na() applied to non-(list or vector) of type 'NULL'
> 
> ## Missing data
> d0 <- makemissing(d1,cols=1:2)
> e0 <- estimate(m,d0,missing=TRUE)
> e0
                    Estimate Std. Error  Z value Pr(>|z|)
Regressions:                                             
   y1~u              0.89205    0.07865 11.34280   <1e-12
    y2~u             0.90587    0.08429 10.74638   <1e-12
   y3~u              1.02505    0.07996 12.82021   <1e-12
    u~x              0.93755    0.10003  9.37310   <1e-12
Intercepts:                                              
   y1               -0.13116    0.10843 -1.20967   0.2264
   y2                0.06083    0.11674  0.52109   0.6023
   y3               -0.02050    0.11096 -0.18478   0.8534
   u                -0.00683    0.10125 -0.06741   0.9463
Residual Variances:                                      
   y1                0.96401    0.15057  6.40247         
   y2                1.00543    0.16531  6.08216         
   y3                1.23112    0.17412  7.07049         
   u                 1.02512    0.14499  7.07038         
> 
> 
> 
> cleanEx()
> nameEx("eventTime")
> ### * eventTime
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: eventTime
> ### Title: Add an observed event time outcome to a latent variable model.
> ### Aliases: eventTime eventTime<-
> ### Keywords: models regression survival
> 
> ### ** Examples
> 
> 
> # Right censored survival data without covariates
> m0 <- lvm()
> distribution(m0,"eventtime") <- coxWeibull.lvm(scale=1/100,shape=2)
> distribution(m0,"censtime") <- coxExponential.lvm(rate=10)
> m0 <- eventTime(m0,time~min(eventtime=1,censtime=0),"status")
> sim(m0,10)
    eventtime   censtime       time status
1  12.5698152 13.2610781 12.5698152      1
2  13.1685721  9.8852842  9.8852842      0
3   6.1268894  5.5712551  5.5712551      0
4   9.7818335  0.9628208  0.9628208      0
5   5.1143986 16.0106342  5.1143986      1
6   8.3531989  1.0715136  1.0715136      0
7   5.7603574  0.5691404  0.5691404      0
8   0.9014898  4.1430740  0.9014898      1
9   9.8361143  4.6344273  4.6344273      0
10  5.0173907 27.8407410  5.0173907      1
> 
> # Alternative specification of the right censored survival outcome
> ## eventTime(m,"Status") <- ~min(eventtime=1,censtime=0)
> 
> # Cox regression:
> # lava implements two different parametrizations of the same
> # Weibull regression model. The first specifies
> # the effects of covariates as proportional hazard ratios
> # and works as follows:
> m <- lvm()
> distribution(m,"eventtime") <- coxWeibull.lvm(scale=1/100,shape=2)
> distribution(m,"censtime") <- coxWeibull.lvm(scale=1/100,shape=2)
> m <- eventTime(m,time~min(eventtime=1,censtime=0),"status")
> distribution(m,"sex") <- binomial.lvm(p=0.4)
> distribution(m,"sbp") <- normal.lvm(mean=120,sd=20)
> regression(m,from="sex",to="eventtime") <- 0.4
> regression(m,from="sbp",to="eventtime") <- -0.01
> sim(m,6)
  eventtime censtime     time status sex      sbp
1 16.895019 5.686503 5.686503      0   0 150.2356
2  9.214196 9.425999 9.214196      1   0 127.7969
3 27.177778 4.441819 4.441819      0   1 107.5752
4 10.282034 6.597848 6.597848      0   1  75.7060
5  9.318246 4.946802 4.946802      0   1 142.4986
6 10.990690 7.696308 7.696308      0   0 119.1013
> # The parameters can be recovered using a Cox regression
> # routine or a Weibull regression model. E.g.,
> ## Not run: 
> ##D     set.seed(18)
> ##D     d <- sim(m,1000)
> ##D     library(survival)
> ##D     coxph(Surv(time,status)~sex+sbp,data=d)
> ##D 
> ##D     sr <- survreg(Surv(time,status)~sex+sbp,data=d)
> ##D     library(SurvRegCensCov)
> ##D     ConvertWeibull(sr)
> ##D 
> ## End(Not run)
> 
> # The second parametrization is an accelerated failure time
> # regression model and uses the function weibull.lvm instead
> # of coxWeibull.lvm to specify the event time distributions.
> # Here is an example:
> 
> ma <- lvm()
> distribution(ma,"eventtime") <- weibull.lvm(scale=3,shape=0.7)
> distribution(ma,"censtime") <- weibull.lvm(scale=2,shape=0.7)
> ma <- eventTime(ma,time~min(eventtime=1,censtime=0),"status")
> distribution(ma,"sex") <- binomial.lvm(p=0.4)
> distribution(ma,"sbp") <- normal.lvm(mean=120,sd=20)
> regression(ma,from="sex",to="eventtime") <- 0.7
> regression(ma,from="sbp",to="eventtime") <- -0.008
> set.seed(17)
> sim(ma,6)
  eventtime  censtime      time status sex       sbp
1 0.5531481 1.1285503 0.5531481      1   1  99.69983
2 4.2973225 1.4665922 1.4665922      0   1 118.40727
3 1.5884110 0.4704796 0.4704796      0   1 115.34026
4 1.7404946 1.2284359 1.2284359      0   1 103.65464
5 0.2765550 0.8633771 0.2765550      1   1 135.44182
6 1.5803203 0.6912997 0.6912997      0   0 116.68776
> # The regression coefficients of the AFT model
> # can be tranformed into log(hazard ratios):
> #  coef.coxWeibull = - coef.weibull / shape.weibull
> ## Not run: 
> ##D     set.seed(17)
> ##D     da <- sim(ma,1000)
> ##D     library(survival)
> ##D     fa <- coxph(Surv(time,status)~sex+sbp,data=da)
> ##D     coef(fa)
> ##D     c(0.7,-0.008)/0.7
> ## End(Not run)
> 
> 
> # The Weibull parameters are related as follows:
> # shape.coxWeibull = 1/shape.weibull
> # scale.coxWeibull = exp(-scale.weibull/shape.weibull)
> # scale.AFT = log(scale.coxWeibull) / shape.coxWeibull
> # Thus, the following are equivalent parametrizations
> # which produce exactly the same random numbers:
> 
> model.aft <- lvm()
> distribution(model.aft,"eventtime") <- weibull.lvm(scale=-log(1/100)/2,shape=0.5)
> distribution(model.aft,"censtime") <- weibull.lvm(scale=-log(1/100)/2,shape=0.5)
> set.seed(17)
> sim(model.aft,6)
  eventtime  censtime
1  2.890253 3.1436842
2  2.981019 0.4127472
3  1.147834 2.0056626
4  2.948915 1.1571425
5  2.103028 2.1804985
6  5.680430 1.8107400
> 
> model.cox <- lvm()
> distribution(model.cox,"eventtime") <- coxWeibull.lvm(scale=1/100,shape=2)
> distribution(model.cox,"censtime") <- coxWeibull.lvm(scale=1/100,shape=2)
> set.seed(17)
> sim(model.cox,6)
  eventtime  censtime
1 12.552208 13.652847
2 12.946401  1.792538
3  4.984980  8.710482
4 12.806975  5.025406
5  9.133336  9.469785
6 24.669793  7.863944
> 
> # The minimum of multiple latent times one of them still
> # being a censoring time, yield
> # right censored competing risks data
> 
> mc <- lvm()
> distribution(mc,~X2) <- binomial.lvm()
> regression(mc) <- T1~f(X1,-.5)+f(X2,0.3)
> regression(mc) <- T2~f(X2,0.6)
> distribution(mc,~T1) <- coxWeibull.lvm(scale=1/100)
> distribution(mc,~T2) <- coxWeibull.lvm(scale=1/100)
> distribution(mc,~C) <- coxWeibull.lvm(scale=1/100)
> mc <- eventTime(mc,time~min(T1=1,T2=2,C=0),"event")
> sim(mc,6)
  X2       T1        X1        T2         C     time event
1  0 7.087078 0.9728744 11.814841  3.204723 3.204723     0
2  0 9.105653 1.7165340  7.716248 15.423587 7.716248     2
3  1 5.069985 0.2552370  8.918131  7.444486 5.069985     1
4  0 5.150130 0.3665811 11.629342  7.433594 5.150130     1
5  1 6.171934 1.1807892  1.627404  2.395786 1.627404     2
6  0 7.931261 0.6431921  3.930187 12.323722 3.930187     2
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("fplot")
> ### * fplot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fplot
> ### Title: fplot
> ### Aliases: fplot
> 
> ### ** Examples
> 
> if (interactive()) {
+ data(iris)
+ fplot(Sepal.Length ~ Petal.Length+Species, data=iris, size=2, type="s")
+ }
> 
> 
> 
> cleanEx()
> nameEx("gof")
> ### * gof
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gof
> ### Title: Extract model summaries and GOF statistics for model object
> ### Aliases: gof gof.lvmfit moments moments.lvm information
> ###   information.lvmfit score score.lvmfit logLik.lvmfit
> ### Keywords: methods models
> 
> ### ** Examples
> 
> m <- lvm(list(y~v1+v2+v3+v4,c(v1,v2,v3,v4)~x))
> set.seed(1)
> dd <- sim(m,1000)
> e <- estimate(m, dd)
> gof(e,all=TRUE,rmsea.threshold=0.05,level=0.9)

 Number of observations = 1000 
 BIC = 14585.57 
 AIC = 14468.26 
 log-Likelihood of model = -7216.128 

 log-Likelihood of saturated model = -7212.5 
 Chi-squared statistic: q = 7.254653 , df = 7 
  P(Q>q) = 0.4028559 

 RMSEA (90% CI): 0.006 (0;0.0397)
  P(RMSEA<0.05)=0.9916145
 TLI = 0.9998998 
 CFI = 0.9999532 
 NFI = 0.9986715 
 SRMR = 0.008682085 

rank(Information) = 18 (p=18)
condition(Information) = 0.0963832
mean(score^2) = 4.216767e-09 
> 
> 
> set.seed(1)
> m <- lvm(list(c(y1,y2,y3)~u,y1~x)); latent(m) <- ~u
> regression(m,c(y2,y3)~u) <- "b"
> d <- sim(m,1000)
> e <- estimate(m,d)
> rsq(e)
$`R-squared`
          y1           y2           y3            u 
6.714238e-01 5.109812e-01 5.276472e-01 2.220446e-16 

$`Variance explained by 'u'`
       y1        y2        y3 
0.3697894 0.5109812 0.5276472 

> ##'
> rr <- rsq(e,TRUE)
> rr

R-squared:

    Estimate    Std.Err      2.5%     97.5%       P-value
y1 0.6666507 0.02449714 0.6186372 0.7146642 4.506818e-163
y2 0.5062724 0.02751655 0.4523409 0.5602038  1.342309e-75
y3 0.5319590 0.02627482 0.4804613 0.5834567  3.855758e-91
> estimate(rr,contrast=rbind(c(1,-1,0),c(1,0,-1),c(0,1,-1)))
            Estimate Std.Err    2.5%  97.5%  P-value
[y1] - [y2]   0.1604  0.0404  0.0812 0.2396 0.000072
[y1] - [y3]   0.1347  0.0388  0.0586 0.2108 0.000524
[y2] - [y3]  -0.0257  0.0279 -0.0803 0.0289 0.356497

 Null Hypothesis: 
  [y1] - [y2] = 0
  [y1] - [y3] = 0
  [y2] - [y3] = 0 
 
chisq = 16.2844, df = 2, p-value = 0.000291
> 
> 
> 
> 
> cleanEx()
> nameEx("iid")
> ### * iid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: iid
> ### Title: Extract i.i.d. decomposition (influence function) from model
> ###   object
> ### Aliases: iid iid.default
> 
> ### ** Examples
> 
> m <- lvm(y~x+z)
> distribution(m, ~y+z) <- binomial.lvm("logit")
> d <- sim(m,1e3)
> g <- glm(y~x+z,data=d,family=binomial)
> crossprod(iid(g))
              (Intercept)            x             z
(Intercept)  0.0094250279 0.0007984604 -0.0092037631
x            0.0007984604 0.0065395382  0.0009412776
z           -0.0092037631 0.0009412776  0.0213763938
> 
> 
> 
> 
> cleanEx()
> nameEx("images")
> ### * images
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: images
> ### Title: Organize several image calls (for visualizing categorical data)
> ### Aliases: images
> 
> ### ** Examples
> 
> X <- matrix(rbinom(400,3,0.5),20)
> group <- rep(1:4,each=5)
> images(X,colorbar=0,zlim=c(0,3))
> images(X,group=group,zlim=c(0,3))
> ## Not run: 
> ##D images(X,group=group,col=list(RColorBrewer::brewer.pal(4,"Purples"),
> ##D                                RColorBrewer::brewer.pal(4,"Greys"),
> ##D                                RColorBrewer::brewer.pal(4,"YlGn"),
> ##D                                RColorBrewer::brewer.pal(4,"PuBuGn")),colorbar=2,zlim=c(0,3))
> ## End(Not run)
> images(list(X,X,X,X),group=group,zlim=c(0,3))
> images(list(X,X,X,X),ncol=1,group=group,zlim=c(0,3))
> images(list(X,X),group,axis2=c(FALSE,FALSE),axis1=c(FALSE,FALSE),
+       mar=list(c(0,0,0,0),c(0,0,0,0)),yaxs="i",xaxs="i",zlim=c(0,3))
> 
> 
> 
> cleanEx()
> nameEx("intercept")
> ### * intercept
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: intercept
> ### Title: Fix mean parameters in 'lvm'-object
> ### Aliases: intercept intercept<- intercept.lvm intercept<-.lvm intfix
> ###   intfix<- intfix.lvm intfix<-.lvm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> 
> ## A multivariate model
> m <- lvm(c(y1,y2) ~ f(x1,beta)+x2)
> regression(m) <- y3 ~ f(x1,beta)
> intercept(m) <- y1 ~ f(mu)
> intercept(m, ~y2+y3) <- list(2,"mu")
> intercept(m) ## Examine intercepts of model (NA translates to free/unique paramete##r)
Intercept parameters:
    y1 y2 y3
    mu 2  mu
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("kill")
> ### * kill
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kill
> ### Title: Remove variables from (model) object.
> ### Aliases: kill kill<- rmvar rmvar<-
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm()
> addvar(m) <- ~y1+y2+x
> covariance(m) <- y1~y2
> regression(m) <- c(y1,y2) ~ x
> ### Cancel the covariance between the residuals of y1 and y2
> cancel(m) <- y1~y2
> ### Remove y2 from the model
> rmvar(m) <- ~y2
> 
> 
> 
> 
> cleanEx()
> nameEx("ksmooth2")
> ### * ksmooth2
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ksmooth2
> ### Title: Plot/estimate surface
> ### Aliases: ksmooth2 surface
> 
> ### ** Examples
> 
> ksmooth2(rmvn(1e4,sigma=diag(2)*.5+.5),c(-3.5,3.5),h=1,
+         rgl=FALSE,theta=30)
> 
> if (interactive()) {
+     ksmooth2(rmvn(1e4,sigma=diag(2)*.5+.5),c(-3.5,3.5),h=1)
+     ksmooth2(function(x,y) x^2+y^2, c(-20,20))
+     ksmooth2(function(x,y) x^2+y^2, xlim=c(-5,5), ylim=c(0,10))
+ 
+     f <- function(x,y) 1-sqrt(x^2+y^2)
+     surface(f,xlim=c(-1,1),alpha=0.9,aspect=c(1,1,0.75))
+     surface(f,xlim=c(-1,1),clut=heat.colors(128))
+     ##play3d(spin3d(axis=c(0,0,1), rpm=8), duration=5)
+ }
> 
> if (interactive()) {
+     surface(function(x) dmvn(x,sigma=diag(2)),c(-3,3),lit=FALSE,smooth=FALSE,box=FALSE,alpha=0.8)
+     surface(function(x) dmvn(x,sigma=diag(2)),c(-3,3),box=FALSE,specular="black")##' 
+ }
> 
> if (!inherits(try(find.package("fields"),silent=TRUE),"try-error")) {
+     f <- function(x,y) 1-sqrt(x^2+y^2)
+     ksmooth2(f,c(-1,1),rgl=FALSE,image=fields::image.plot)
+ }
> 
> 
> 
> cleanEx()
> nameEx("labels-set")
> ### * labels-set
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: labels<-
> ### Title: Define labels of graph
> ### Aliases: labels<- labels labels<-.default labels.lvm labels.lvmfit
> ###   labels.graphNEL edgelabels edgelabels<- edgelabels<-.lvm nodecolor
> ###   nodecolor<- nodecolor<-.default
> ### Keywords: aplot graphs
> 
> ### ** Examples
> 
> m <- lvm(c(y,v)~x+z)
> regression(m) <- c(v,x)~z
> labels(m) <- c(y=expression(psi), z=expression(zeta))
> nodecolor(m,~y+z+x,border=c("white","white","black"),
+           labcol="white", lwd=c(1,1,5),
+           lty=c(1,2)) <-  c("orange","indianred","lightgreen")
> edgelabels(m,y~z+x, cex=c(2,1.5), col=c("orange","black"),labcol="darkblue",
+            arrowhead=c("tee","dot"),
+            lwd=c(3,1)) <- expression(phi,rho)
> edgelabels(m,c(v,x)~z, labcol="red", cex=0.8,arrowhead="none") <- 2
> if (interactive()) {
+     plot(m,addstyle=FALSE)
+ }
> 
> m <- lvm(y~x)
> labels(m) <- list(x="multiple\nlines")
> if (interactive()) {
+ op <- par(mfrow=c(1,2))
+ plot(m,plain=TRUE)
+ plot(m)
+ par(op)
+ 
+ d <- sim(m,100)
+ e <- estimate(m,d)
+ plot(e,type="sd")
+ }
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("lava-package")
> ### * lava-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lava-package
> ### Title: Estimation and simulation of latent variable models
> ### Aliases: lava-package lava
> ### Keywords: package
> 
> ### ** Examples
> 
> 
> lava()
> 
> 
> 
> 
> cleanEx()
> nameEx("lava.options")
> ### * lava.options
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lava.options
> ### Title: Set global options for 'lava'
> ### Aliases: lava.options
> ### Keywords: models
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D lava.options(iter.max=100,silent=TRUE)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("lvm")
> ### * lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lvm
> ### Title: Initialize new latent variable model
> ### Aliases: lvm print.lvm summary.lvm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm() # Empty model
> m1 <- lvm(y~x) # Simple linear regression
> m2 <- lvm(~y1+y2) # Model with two independent variables (argument)
> m3 <- lvm(list(c(y1,y2,y3)~u,u~x+z)) # SEM with three items
> 
> 
> 
> 
> cleanEx()
> nameEx("measurement.error")
> ### * measurement.error
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: measurement.error
> ### Title: Two-stage (non-linear) measurement error
> ### Aliases: measurement.error
> 
> ### ** Examples
> 
> m <- lvm(c(y1,y2,y3)~u,c(y3,y4,y5)~v,u~~v,c(u,v)~x)
> transform(m,u2~u) <- function(x) x^2
> transform(m,uv~u+v) <- prod
> regression(m) <- z~u2+u+v+uv+x
> set.seed(1)
> d <- sim(m,1000,p=c("u,u"=1))
> 
> ## Stage 1
> m1 <- lvm(c(y1[0:s],y2[0:s],y3[0:s])~1*u,c(y3[0:s],y4[0:s],y5[0:s])~1*v,u~b*x,u~~v)
> latent(m1) <- ~u+v
> e1 <- estimate(m1,d)
> 
> pp <- function(mu,var,data,...) {
+     cbind(u=mu[,"u"],u2=mu[,"u"]^2+var["u","u"],v=mu[,"v"],uv=mu[,"u"]*mu[,"v"]+var["u","v"])
+ }
> (e <- measurement.error(e1, z~1+x, data=d, predictfun=pp))
            Estimate Std.Err    2.5% 97.5%  P-value
(Intercept)    0.136  0.1185 -0.0964 0.368 2.52e-01
x              1.129  0.1181  0.8972 1.360 1.21e-21
u              0.944  0.1217  0.7052 1.182 8.90e-15
u2             0.937  0.0965  0.7483 1.127 2.61e-22
v              1.138  0.1010  0.9406 1.336 1.75e-29
uv             1.038  0.1079  0.8261 1.249 6.81e-22
> 
> ## uu <- seq(-1,1,length.out=100)
> ## pp <- estimate(e,function(p,...) p["(Intercept)"]+p["u"]*uu+p["u2"]*uu^2)$coefmat
> if (interactive()) {
+     plot(e,intercept=TRUE,vline=0)
+ 
+     f <- function(p) p[1]+p["u"]*u+p["u2"]*u^2
+     u <- seq(-1,1,length.out=100)
+     plot(e, f, data=data.frame(u), ylim=c(-.5,2.5))
+ }
> 
> 
> 
> cleanEx()
> nameEx("missingdata")
> ### * missingdata
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: missingdata
> ### Title: Missing data example
> ### Aliases: missingdata
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(missingdata)
> e0 <- estimate(lvm(c(y1,y2)~b*x,y1~~y2),missingdata[[1]]) ## No missing
> e1 <- estimate(lvm(c(y1,y2)~b*x,y1~~y2),missingdata[[2]]) ## CC (MCAR)
> e2 <- estimate(lvm(c(y1,y2)~b*x,y1~~y2),missingdata[[2]],missing=TRUE) ## MCAR
> e3 <- estimate(lvm(c(y1,y2)~b*x,y1~~y2),missingdata[[3]]) ## CC (MAR)
> e4 <- estimate(lvm(c(y1,y2)~b*x,y1~~y2),missingdata[[3]],missing=TRUE) ## MAR
> 
> 
> 
> cleanEx()
> nameEx("modelsearch")
> ### * modelsearch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: modelsearch
> ### Title: Model searching
> ### Aliases: modelsearch
> ### Keywords: htest
> 
> ### ** Examples
> 
> 
> m <- lvm();
> regression(m) <- c(y1,y2,y3) ~ eta; latent(m) <- ~eta
> regression(m) <- eta ~ x
> m0 <- m; regression(m0) <- y2 ~ x
> dd <- sim(m0,100)[,manifest(m0)]
> e <- estimate(m,dd);
> modelsearch(e,silent=TRUE)
 Score: S P(S>s)    Index  holm     BH       
 1.035    0.3089    y1~~x  1        0.3089   
 1.035    0.3089    y1~x   1        0.3089   
 1.035    0.3089    x~y1   1        0.3089   
 1.035    0.3089    y2~~y3 1        0.3089   
 1.035    0.3089    y2~y3  1        0.3089   
 1.035    0.3089    y3~y2  1        0.3089   
 4.221    0.03992   y1~~y2 0.479    0.05987  
 4.221    0.03992   y1~y2  0.479    0.05987  
 4.221    0.03992   y2~y1  0.479    0.05987  
 4.221    0.03992   y3~~x  0.479    0.05987  
 4.221    0.03992   y3~x   0.479    0.05987  
 4.221    0.03992   x~y3   0.479    0.05987  
 14.52    0.0001386 y1~~y3 0.002495 0.0004159
 14.52    0.0001386 y1~y3  0.002495 0.0004159
 14.52    0.0001386 y3~y1  0.002495 0.0004159
 14.52    0.0001386 y2~~x  0.002495 0.0004159
 14.52    0.0001386 y2~x   0.002495 0.0004159
 14.52    0.0001386 x~y2   0.002495 0.0004159
> modelsearch(e,silent=TRUE,type="cor")
 Score: S P(S>s)    Index  holm      BH       
 1.035    0.3089    y1~~x  0.6178    0.3089   
 1.035    0.3089    y2~~y3 0.6178    0.3089   
 4.221    0.03992   y1~~y2 0.1597    0.05987  
 4.221    0.03992   y3~~x  0.1597    0.05987  
 14.52    0.0001386 y1~~y3 0.0008318 0.0004159
 14.52    0.0001386 y2~~x  0.0008318 0.0004159
> 
> 
> 
> cleanEx()
> nameEx("multinomial")
> ### * multinomial
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multinomial
> ### Title: Estimate probabilities in contingency table
> ### Aliases: multinomial kappa.multinomial kappa.table gkgamma
> 
> ### ** Examples
> 
> set.seed(1)
> breaks <- c(-Inf,-1,0,Inf)
> m <- lvm(); covariance(m,pairwise=TRUE) <- ~y1+y2+y3+y4
> d <- transform(sim(m,5e2),
+               z1=cut(y1,breaks=breaks),
+               z2=cut(y2,breaks=breaks),
+               z3=cut(y3,breaks=breaks),
+               z4=cut(y4,breaks=breaks))
> 
> multinomial(d[,5])
Call: NULL

Estimates:
x
(-Inf,-1]    (-1,0]  (0, Inf] 
    0.154     0.350     0.496 

   Estimate Std.Err  2.5% 97.5%   P-value
p1    0.154  0.0161 0.122 0.186  1.42e-21
p2    0.350  0.0213 0.308 0.392  1.67e-60
p3    0.496  0.0224 0.452 0.540 5.07e-109
> (a1 <- multinomial(d[,5:6]))
Call: multinomial(x = d[, 5:6])

Estimates:
           z2
z1          (-Inf,-1] (-1,0] (0, Inf]
  (-Inf,-1]     0.064  0.062    0.028
  (-1,0]        0.066  0.146    0.138
  (0, Inf]      0.040  0.154    0.302

    Estimate Std.Err   2.5%  97.5%  P-value
p11    0.064 0.01095 0.0425 0.0855 5.00e-09
p21    0.066 0.01110 0.0442 0.0878 2.78e-09
p31    0.040 0.00876 0.0228 0.0572 5.01e-06
p12    0.062 0.01078 0.0409 0.0831 8.99e-09
p22    0.146 0.01579 0.1150 0.1770 2.34e-20
p32    0.154 0.01614 0.1224 0.1856 1.42e-21
p13    0.028 0.00738 0.0135 0.0425 1.48e-04
p23    0.138 0.01542 0.1078 0.1682 3.66e-19
p33    0.302 0.02053 0.2618 0.3422 5.71e-49
> (K1 <- kappa(a1)) ## Cohen's kappa
      Estimate Std.Err  2.5% 97.5%  P-value
kappa    0.206  0.0355 0.137 0.276 5.81e-09
> 
> K2 <- kappa(d[,7:8])
> ## Testing difference K1-K2:
> estimate(merge(K1,K2,id=TRUE),diff)
        Estimate Std.Err    2.5% 97.5% P-value
kappa.1   0.0576  0.0478 -0.0361 0.151   0.228
> 
> estimate(merge(K1,K2,id=FALSE),diff) ## Wrong std.err ignoring dependence
        Estimate Std.Err    2.5% 97.5% P-value
kappa.1   0.0576    0.05 -0.0404 0.155   0.249
> sqrt(vcov(K1)+vcov(K2))
           kappa
kappa 0.04996804
> 
> ## Average of the two kappas:
> estimate(merge(K1,K2,id=TRUE),function(x) mean(x))
   Estimate Std.Err  2.5% 97.5%  P-value
p1    0.235   0.026 0.184 0.286 1.57e-19
> estimate(merge(K1,K2,id=FALSE),function(x) mean(x)) ## Independence
   Estimate Std.Err  2.5% 97.5%  P-value
p1    0.235   0.025 0.186 0.284 4.64e-21
> ##'
> ## Goodman-Kruskal's gamma
> m2 <- lvm(); covariance(m2) <- y1~y2
> breaks1 <- c(-Inf,-1,0,Inf)
> breaks2 <- c(-Inf,0,Inf)
> d2 <- transform(sim(m2,5e2),
+               z1=cut(y1,breaks=breaks1),
+               z2=cut(y2,breaks=breaks2))
> 
> (g1 <- gkgamma(d2[,3:4]))
Call: gkgamma(x = d2[, 3:4])
__________________________________________________
n = 500

      Estimate Std.Err   2.5%  97.5%  P-value
C       0.2665 0.01390 0.2393 0.2938 5.52e-82
D       0.0662 0.00797 0.0506 0.0818 1.03e-16
gamma   0.6021 0.05380 0.4967 0.7076 4.41e-29
> ## same as
> ## Not run: 
> ##D gkgamma(table(d2[,3:4]))
> ##D gkgamma(multinomial(d2[,3:4]))
> ## End(Not run)
> 
> ##partial gamma
> d2$x <- rbinom(nrow(d2),2,0.5)
> gkgamma(z1~z2|x,data=d2)
Call: gkgamma(x = z1 ~ z2 | x, data = d2)
__________________________________________________
Strata:

0 (n=112):
  Estimate Std.Err   2.5%  97.5%  P-value
C   0.2946  0.0300 0.2359 0.3534 8.76e-23
D   0.0462  0.0138 0.0192 0.0733 7.98e-04

1 (n=248):
  Estimate Std.Err   2.5% 97.5%  P-value
C   0.2434  0.0196 0.2050 0.282 1.80e-35
D   0.0813  0.0126 0.0566 0.106 9.79e-11

2 (n=140):
  Estimate Std.Err   2.5%  97.5%  P-value
C   0.2852  0.0260 0.2342 0.3362 6.42e-28
D   0.0581  0.0144 0.0299 0.0862 5.26e-05

__________________________________________________

n = 500

Gamma coefficient:

       Estimate Std.Err  2.5% 97.5%  P-value
γ:0       0.729  0.0906 0.551 0.906 8.89e-16
γ:1       0.499  0.0865 0.330 0.669 7.74e-09
γ:2       0.662  0.0929 0.480 0.844 1.08e-12
pgamma    0.566  0.0607 0.447 0.685 1.13e-20
> 
> 
> 
> cleanEx()
> nameEx("op_concat")
> ### * op_concat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: %++%
> ### Title: Concatenation operator
> ### Aliases: %++%
> ### Keywords: misc utilities
> 
> ### ** Examples
> 
> matrix(rnorm(25),5)%++%matrix(rnorm(25),5)
             [,1]        [,2]       [,3]       [,4]       [,5]       [,6]
 [1,] -0.05612874  1.35867955 -0.4149946 -0.1645236 -0.7074952  0.0000000
 [2,] -0.15579551 -0.10278773 -0.3942900 -0.2533617  0.3645820  0.0000000
 [3,] -1.47075238  0.38767161 -0.0593134  0.6969634  0.7685329  0.0000000
 [4,] -0.47815006 -0.05380504  1.1000254  0.5566632 -0.1123462  0.0000000
 [5,]  0.41794156 -1.37705956  0.7631757 -0.6887557  0.8811077  0.0000000
 [6,]  0.00000000  0.00000000  0.0000000  0.0000000  0.0000000 -0.6264538
 [7,]  0.00000000  0.00000000  0.0000000  0.0000000  0.0000000  0.1836433
 [8,]  0.00000000  0.00000000  0.0000000  0.0000000  0.0000000 -0.8356286
 [9,]  0.00000000  0.00000000  0.0000000  0.0000000  0.0000000  1.5952808
[10,]  0.00000000  0.00000000  0.0000000  0.0000000  0.0000000  0.3295078
            [,7]       [,8]        [,9]       [,10]
 [1,]  0.0000000  0.0000000  0.00000000  0.00000000
 [2,]  0.0000000  0.0000000  0.00000000  0.00000000
 [3,]  0.0000000  0.0000000  0.00000000  0.00000000
 [4,]  0.0000000  0.0000000  0.00000000  0.00000000
 [5,]  0.0000000  0.0000000  0.00000000  0.00000000
 [6,] -0.8204684  1.5117812 -0.04493361  0.91897737
 [7,]  0.4874291  0.3898432 -0.01619026  0.78213630
 [8,]  0.7383247 -0.6212406  0.94383621  0.07456498
 [9,]  0.5757814 -2.2146999  0.82122120 -1.98935170
[10,] -0.3053884  1.1249309  0.59390132  0.61982575
> "Hello "%++%" World"
[1] "Hello  World"
> 
> 
> 
> cleanEx()
> nameEx("op_match")
> ### * op_match
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: %ni%
> ### Title: Matching operator (x not in y) oposed to the '%in%'-operator (x
> ###   in y)
> ### Aliases: %ni%
> ### Keywords: misc utilities
> 
> ### ** Examples
> 
> 
> 1:10 %ni% c(1,5,10)
 [1] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE
> 
> 
> 
> 
> cleanEx()
> nameEx("ordreg")
> ### * ordreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ordreg
> ### Title: Univariate cumulative link regression models
> ### Aliases: ordreg
> 
> ### ** Examples
> 
> m <- lvm(y~x)
> ordinal(m,K=3) <- ~y
> d <- sim(m,100)
> e <- ordreg(y~x,d)
> 
> 
> 
> cleanEx()
> nameEx("partialcor")
> ### * partialcor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: partialcor
> ### Title: Calculate partial correlations
> ### Aliases: partialcor
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm(c(y1,y2,y3)~x1+x2)
> covariance(m) <- c(y1,y2,y3)~y1+y2+y3
> d <- sim(m,500)
> partialcor(~x1+x2,d)
            cor        z         pval   lowerCI   upperCI
y1~y2 0.4511319 10.80457 3.274951e-27 0.3781829 0.5185127
y1~y3 0.4627410 11.13069 8.894188e-29 0.3906870 0.5291591
y2~y3 0.4301737 10.22645 1.509499e-24 0.3556760 0.4992396
> 
> 
> 
> 
> cleanEx()
> nameEx("path")
> ### * path
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: path
> ### Title: Extract pathways in model graph
> ### Aliases: path effects path.lvm effects.lvmfit totaleffects
> ### Keywords: graphs methods models
> 
> ### ** Examples
> 
> 
> m <- lvm(c(y1,y2,y3)~eta)
> regression(m) <- y2~x1
> latent(m) <- ~eta
> regression(m) <- eta~x1+x2
> d <- sim(m,500)
> e <- estimate(m,d)
> 
> path(Model(e),y2~x1)
[[1]]
[1] "x1" "y2"

[[2]]
[1] "x1"  "eta" "y2" 

> parents(Model(e), ~y2)
[1] "eta" "x1" 
> children(Model(e), ~x2)
[1] "eta"
> children(Model(e), ~x2+eta)
[1] "eta" "y1"  "y2"  "y3" 
> effects(e,y2~x1)

Total effect of 'x1' on 'y2':
		1.972585 (Approx. Std.Err = 0.06657177)
Direct effect of 'x1' on 'y2':
		0.915604 (Approx. Std.Err = 0.07419149)
Total indirect effect of 'x1' on 'y2':
		1.056981 (Approx. Std.Err = 0.07413519)
Indirect effects:
	Effect of 'x1' via x1->eta->y2:
		1.056981 (Approx. Std.Err = 0.07413519)

> ## All simple paths (undirected)
> path(m,y1~x1,all=TRUE)
[[1]]
[1] "x1"  "y2"  "eta" "y1" 

[[2]]
[1] "x1"  "eta" "y1" 

> 
> 
> 
> 
> cleanEx()
> nameEx("plot.lvm")
> ### * plot.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.lvm
> ### Title: Plot path diagram
> ### Aliases: plot.lvm plot.lvmfit
> ### Keywords: hplot regression
> 
> ### ** Examples
> 
> 
> if (interactive()) {
+ m <- lvm(c(y1,y2) ~ eta)
+ regression(m) <- eta ~ z+x2
+ regression(m) <- c(eta,z) ~ x1
+ latent(m) <- ~eta
+ labels(m) <- c(y1=expression(y[scriptscriptstyle(1)]),
+ y2=expression(y[scriptscriptstyle(2)]),
+ x1=expression(x[scriptscriptstyle(1)]),
+ x2=expression(x[scriptscriptstyle(2)]),
+ eta=expression(eta))
+ edgelabels(m, eta ~ z+x1+x2, cex=2, lwd=3,
+            col=c("orange","lightblue","lightblue")) <- expression(rho,phi,psi)
+ nodecolor(m, vars(m), border="white", labcol="darkblue") <- NA
+ nodecolor(m, ~y1+y2+z, labcol=c("white","white","black")) <- NA
+ plot(m,cex=1.5)
+ 
+ d <- sim(m,100)
+ e <- estimate(m,d)
+ plot(e)
+ 
+ m <- lvm(c(y1,y2) ~ eta)
+ regression(m) <- eta ~ z+x2
+ regression(m) <- c(eta,z) ~ x1
+ latent(m) <- ~eta
+ plot(lava:::beautify(m,edgecol=FALSE))
+ }
> 
> 
> 
> cleanEx()
> nameEx("plotConf")
> ### * plotConf
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plotConf
> ### Title: Plot regression lines
> ### Aliases: plotConf
> ### Keywords: hplot, regression
> 
> ### ** Examples
> 
> n <- 100
> x0 <- rnorm(n)
> x1 <- seq(-3,3, length.out=n)
> x2 <- factor(rep(c(1,2),each=n/2), labels=c("A","B"))
> y <- 5 + 2*x0 + 0.5*x1 + -1*(x2=="B")*x1 + 0.5*(x2=="B") + rnorm(n, sd=0.25)
> dd <- data.frame(y=y, x1=x1, x2=x2)
> lm0 <- lm(y ~ x0 + x1*x2, dd)
> plotConf(lm0, var1="x1", var2="x2")
> abline(a=5,b=0.5,col="red")
> abline(a=5.5,b=-0.5,col="red")
> ### points(5+0.5*x1 -1*(x2=="B")*x1 + 0.5*(x2=="B") ~ x1, cex=2)
> 
> data(iris)
> l <- lm(Sepal.Length ~ Sepal.Width*Species,iris)
> plotConf(l,var2="Species")
> plotConf(l,var1="Sepal.Width",var2="Species")
> 
> 
> 
> cleanEx()
> nameEx("predict.lvm")
> ### * predict.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.lvm
> ### Title: Prediction in structural equation models
> ### Aliases: predict.lvm predict.lvmfit
> 
> ### ** Examples
> 
> m <- lvm(list(c(y1,y2,y3)~u,u~x)); latent(m) <- ~u
> d <- sim(m,100)
> e <- estimate(m,d)
> 
> ## Conditional mean (and variance as attribute) given covariates
> r <- predict(e)
> ## Best linear unbiased predictor (BLUP)
> r <- predict(e,vars(e))
> ##  Conditional mean of y3 giving covariates and y1,y2
> r <- predict(e,y3~y1+y2)
> ##  Conditional mean  gives covariates and y1
> r <- predict(e,~y1+y2)
> ##  Predicted residuals (conditional on all observed variables)
> r <- predict(e,vars(e),residual=TRUE)
> 
> 
> 
> 
> cleanEx()
> nameEx("predictlvm")
> ### * predictlvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predictlvm
> ### Title: Predict function for latent variable models
> ### Aliases: predictlvm
> 
> ### ** Examples
> 
> m <- lvm(c(x1,x2,x3)~u1,u1~z,
+          c(y1,y2,y3)~u2,u2~u1+z)
> latent(m) <- ~u1+u2
> d <- simulate(m,10,"u2,u2"=2,"u1,u1"=0.5,seed=123)
> e <- estimate(m,d)
> 
> ## Conditional mean given covariates
> predictlvm(e,c(x1,x2)~1)$mean
               x1           x2
 [1,] -0.17634038  0.001097242
 [2,]  0.22409175  0.370702775
 [3,] -0.64578819 -0.432210919
 [4,]  2.17930239  2.175394823
 [5,]  1.38879089  1.445739518
 [6,] -0.52874258 -0.324175875
 [7,]  0.06371187  0.222669470
 [8,]  0.01125438  0.174250335
 [9,]  1.03672161  1.120773697
[10,]  0.32654483  0.465268679
> ## Conditional variance of u1,y1 given x1,x2
> predictlvm(e,c(u1,y1)~x1+x2)$var
           u1         y1
u1 0.17501758 0.09920436
y1 0.09920436 0.89761525
> 
> 
> 
> cleanEx()
> nameEx("regression-set")
> ### * regression-set
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: regression<-
> ### Title: Add regression association to latent variable model
> ### Aliases: regression<- regression regression<-.lvm regression.lvm regfix
> ###   regfix<- regfix.lvm regfix<-.lvm
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm() ## Initialize empty lvm-object
> ### E(y1|z,v) = beta1*z + beta2*v
> regression(m) <- y1 ~ z + v
> ### E(y2|x,z,v) = beta*x + beta*z + 2*v + beta3*u
> regression(m) <- y2 ~ f(x,beta) + f(z,beta)  + f(v,2) + u
> ### Clear restriction on association between y and
> ### fix slope coefficient of u to beta
> regression(m, y2 ~ v+u) <- list(NA,"beta")
> 
> regression(m) ## Examine current linear parameter constraints
Regression parameters:
      y1 z    v y2 x    u   
   y1    *    *             
   y2    beta *    beta beta
> 
> ## ## A multivariate model, E(yi|x1,x2) = beta[1i]*x1 + beta[2i]*x2:
> m2 <- lvm(c(y1,y2) ~ x1+x2)
> 
> 
> 
> 
> cleanEx()
> nameEx("scheffe")
> ### * scheffe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scheffe
> ### Title: Calculate simultaneous confidence limits by Scheffe's method
> ### Aliases: scheffe
> 
> ### ** Examples
> 
> x <- rnorm(100)
> d <- data.frame(y=rnorm(length(x),x),x=x)
> l <- lm(y~x,d)
> plot(y~x,d)
> abline(l)
> d0 <- data.frame(x=seq(-5,5,length.out=100))
> d1 <- cbind(d0,predict(l,newdata=d0,interval="confidence"))
> d2 <- cbind(d0,scheffe(l,d0))
> lines(lwr~x,d1,lty=2,col="red")
> lines(upr~x,d1,lty=2,col="red")
> lines(lwr~x,d2,lty=2,col="blue")
> lines(upr~x,d2,lty=2,col="blue")
> 
> 
> 
> cleanEx()
> nameEx("sim")
> ### * sim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim
> ### Title: Simulate model
> ### Aliases: sim sim.lvmfit sim.lvm simulate.lvmfit simulate.lvm
> ###   transform<- transform<-.lvm transform.lvm functional functional<-
> ###   functional.lvm functional<-.lvm distribution distribution<-
> ###   distribution.lvm distribution<-.lvm heavytail heavytail<- weibull.lvm
> ###   binomial.lvm poisson.lvm uniform.lvm beta.lvm normal.lvm
> ###   lognormal.lvm gaussian.lvm GM2.lvm GM3.lvm probit.lvm logit.lvm
> ###   pareto.lvm student.lvm chisq.lvm coxGompertz.lvm coxWeibull.lvm
> ###   coxExponential.lvm aalenExponential.lvm Gamma.lvm gamma.lvm
> ###   loggamma.lvm categorical categorical<- threshold.lvm ones.lvm
> ###   sequence.lvm
> ### Keywords: datagen models regression
> 
> ### ** Examples
> 
> ##################################################
> ## Logistic regression
> ##################################################
> m <- lvm(y~x+z)
> regression(m) <- x~z
> distribution(m,~y+z) <- binomial.lvm("logit")
> d <- sim(m,1e3)
> head(d)
  y          x z
1 0  0.3735462 1
2 1  1.1836433 1
3 1  0.1643714 1
4 1  1.5952808 0
5 0  0.3295078 0
6 1 -0.8204684 0
> e <- estimate(m,d,estimator="glm")
> e
             Estimate Std. Error  Z-value   P-value
Regressions:                                       
   y~x        0.93881    0.08639 10.86652    <1e-12
   y~z        0.97485    0.17399  5.60283 2.109e-08
    x~z       1.08979    0.06547 16.64649    <1e-12
Intercepts:                                        
   y          0.07844    0.09723  0.80678    0.4198
   x         -0.05484    0.04483 -1.22311    0.2213
Dispersion:                                        
   x          1.07011                              
> ## Simulate a few observation from estimated model
> sim(e,n=5)
  y          x z
1 0 -0.9715229 0
2 1 -0.9535433 1
3 1  2.7104679 1
4 1  1.5721163 1
5 1  0.9771778 1
> ##################################################
> ## Poisson
> ##################################################
> distribution(m,~y) <- poisson.lvm()
> d <- sim(m,1e4,p=c(y=-1,"y~x"=2,z=1))
> head(d)
   y           x z
1  4  0.68721342 1
2  0 -0.05823571 1
3 18  1.41722360 1
4  6  0.68454847 1
5  0  0.82554913 0
6 92  2.29127204 1
> estimate(m,d,estimator="glm")
               Estimate Std. Error    Z-value  P-value
Regressions:                                          
   y~x          1.99879    0.00175 1142.71360   <1e-12
   y~z          0.99246    0.01317   75.38047   <1e-12
    x~z         0.99794    0.02244   44.47779   <1e-12
Intercepts:                                           
   y           -0.98876    0.01325  -74.59565   <1e-12
   x           -0.00521    0.01912   -0.27235   0.7854
Dispersion:                                           
   x            1.00155                               
> mean(d$z); lava:::expit(1)
[1] 0.7292
[1] 0.7310586
> summary(lm(y~x,sim(lvm(y[1:2]~4*x),1e3)))

Call:
lm(formula = y ~ x, data = sim(lvm(y[1:2] ~ 4 * x), 1000))

Residuals:
    Min      1Q  Median      3Q     Max 
-4.6214 -0.9253  0.0806  0.9279  3.6946 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  1.03157    0.04435   23.26   <2e-16 ***
x            4.07299    0.04294   94.84   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.401 on 998 degrees of freedom
Multiple R-squared:  0.9001,	Adjusted R-squared:    0.9 
F-statistic:  8995 on 1 and 998 DF,  p-value: < 2.2e-16

> ##################################################
> ### Gamma distribution
> ##################################################
> m <- lvm(y~x)
> distribution(m,~y+x) <- list(Gamma.lvm(shape=2),binomial.lvm())
> intercept(m,~y) <- 0.5
> d <- sim(m,1e4)
> summary(g <- glm(y~x,family=Gamma(),data=d))

Call:
glm(formula = y ~ x, family = Gamma(), data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0856  -0.6587  -0.1638   0.3219   2.2762  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.501081   0.005042   99.39   <2e-16 ***
x           0.994880   0.015661   63.53   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for Gamma family taken to be 0.498568)

    Null deviance: 8390.3  on 9999  degrees of freedom
Residual deviance: 5523.6  on 9998  degrees of freedom
AIC: 20556

Number of Fisher Scoring iterations: 6

> ## Not run: MASS::gamma.shape(g)
> args(lava::Gamma.lvm)
function (link = "inverse", shape, rate, unit = FALSE, var = FALSE, 
    log = FALSE, ...) 
NULL
> distribution(m,~y) <- Gamma.lvm(shape=2,log=TRUE)
> sim(m,10,p=c(y=0.5))[,"y"]
 [1] -0.4652060  0.1505743  1.1428073 -1.0401750 -1.8809976 -1.2001996
 [7]  0.3498964 -1.6747524 -1.0179529 -0.4650479
> ##################################################
> ### Beta
> ##################################################
> m <- lvm()
> distribution(m,~y) <- beta.lvm(alpha=2,beta=1)
> var(sim(m,100,"y,y"=2))
         y
y 1.049548
> distribution(m,~y) <- beta.lvm(alpha=2,beta=1,scale=FALSE)
> var(sim(m,100))
           y
y 0.05931322
> ##################################################
> ### Transform
> ##################################################
> m <- lvm()
> transform(m,xz~x+z) <- function(x) x[1]*(x[2]>0)
> regression(m) <- y~x+z+xz
> d <- sim(m,1e3)
> summary(lm(y~x+z + x*I(z>0),d))

Call:
lm(formula = y ~ x + z + x * I(z > 0), data = d)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2268 -0.6639 -0.0048  0.6916  2.8938 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)    -0.05250    0.06153  -0.853    0.394    
x               0.97574    0.04502  21.673   <2e-16 ***
z               0.96933    0.05210  18.607   <2e-16 ***
I(z > 0)TRUE    0.09675    0.10390   0.931    0.352    
x:I(z > 0)TRUE  1.03194    0.06462  15.970   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.993 on 995 degrees of freedom
Multiple R-squared:  0.7641,	Adjusted R-squared:  0.7632 
F-statistic: 805.8 on 4 and 995 DF,  p-value: < 2.2e-16

> ##################################################
> ### Non-random variables
> ##################################################
> m <- lvm()
> distribution(m,~x+z+v+w) <- list(sequence.lvm(0,5),## Seq. 0 to 5 by 1/n
+                                ones.lvm(),       ## Vector of ones
+                                ones.lvm(0.5),    ##  0.8n 0, 0.2n 1
+                                ones.lvm(interval=list(c(0.3,0.5),c(0.8,1))))
> sim(m,10)
           x z v w
1  0.0000000 1 0 0
2  0.5555556 1 0 0
3  1.1111111 1 0 1
4  1.6666667 1 0 1
5  2.2222222 1 0 1
6  2.7777778 1 1 0
7  3.3333333 1 1 0
8  3.8888889 1 1 1
9  4.4444444 1 1 1
10 5.0000000 1 1 1
> ##################################################
> ### Cox model
> ### piecewise constant hazard
> ################################################
> m <- lvm(t~x)
> rates <- c(1,0.5); cuts <- c(0,5)
> ## Constant rate: 1 in [0,5), 0.5 in [5,Inf)
> distribution(m,~t) <- coxExponential.lvm(rate=rates,timecut=cuts)
> ## Not run: 
> ##D     d <- sim(m,2e4,p=c("t~x"=0.1)); d$status <- TRUE
> ##D     plot(timereg::aalen(survival::Surv(t,status)~x,data=d,
> ##D                         resample.iid=0,robust=0),spec=1)
> ##D     L <- approxfun(c(cuts,max(d$t)),f=1,
> ##D                    cumsum(c(0,rates*diff(c(cuts,max(d$t))))),
> ##D                    method="linear")
> ##D     curve(L,0,100,add=TRUE,col="blue")
> ## End(Not run)
> ##################################################
> ### Cox model
> ### piecewise constant hazard, gamma frailty
> ##################################################
> m <- lvm(y~x+z)
> rates <- c(0.3,0.5); cuts <- c(0,5)
> distribution(m,~y+z) <- list(coxExponential.lvm(rate=rates,timecut=cuts),
+                              loggamma.lvm(rate=1,shape=1))
> ## Not run: 
> ##D     d <- sim(m,2e4,p=c("y~x"=0,"y~z"=0)); d$status <- TRUE
> ##D     plot(timereg::aalen(survival::Surv(y,status)~x,data=d,
> ##D                         resample.iid=0,robust=0),spec=1)
> ##D     L <- approxfun(c(cuts,max(d$y)),f=1,
> ##D                    cumsum(c(0,rates*diff(c(cuts,max(d$y))))),
> ##D                    method="linear")
> ##D     curve(L,0,100,add=TRUE,col="blue")
> ## End(Not run)
> ## Equivalent via transform (here with Aalens additive hazard model)
> m <- lvm(y~x)
> distribution(m,~y) <- aalenExponential.lvm(rate=rates,timecut=cuts)
> distribution(m,~z) <- Gamma.lvm(rate=1,shape=1)
> transform(m,t~y+z) <- prod
> sim(m,10)
             y          x          z             t
1  -33.3294064 -0.3929213 3.17678276 -105.88028346
2   -7.8655637 -0.4164117 0.52879729   -4.15928878
3   -3.8152106 -0.6778896 0.03157971   -0.12048324
4   10.9544765 -0.1988490 1.49029510   16.32540265
5    0.5406456  0.4773814 0.49321064    0.26665216
6   -0.5367728 -1.1599667 0.94755473   -0.50862161
7   -3.0999901 -0.6649567 2.89512841   -8.97486935
8   -0.7408980 -0.6462346 0.09973339   -0.07389227
9    0.3180487  0.3098764 0.22402775    0.07125174
10   0.3798249  0.4695661 1.92169832    0.72990878
> ## Shared frailty
> m <- lvm(c(t1,t2)~x+z)
> rates <- c(1,0.5); cuts <- c(0,5)
> distribution(m,~y) <- aalenExponential.lvm(rate=rates,timecut=cuts)
> distribution(m,~z) <- loggamma.lvm(rate=1,shape=1)
> ## Not run: 
> ##D mets::fast.reshape(sim(m,100),varying="t")
> ## End(Not run)
> ##################################################
> ### General multivariate distributions
> ##################################################
> ## Not run: 
> ##D m <- lvm()
> ##D distribution(m,~y1+y2,oratio=4) <- VGAM::rbiplackcop
> ##D ksmooth2(sim(m,1e4),rgl=FALSE,theta=-20,phi=25)
> ##D m <- lvm()
> ##D distribution(m,~z1+z2,"or1") <- VGAM::rbiplackcop
> ##D distribution(m,~y1+y2,"or2") <- VGAM::rbiplackcop
> ##D sim(m,10,p=c(or1=0.1,or2=4))
> ## End(Not run)
> m <- lvm()
> distribution(m,~y1+y2+y3,TRUE) <- function(n,...) rmvn(n,sigma=diag(3)+1)
> var(sim(m,100))
          y1        y2        y3
y1 1.3696587 0.7553374 0.7483329
y2 0.7553374 1.3792446 0.8766283
y3 0.7483329 0.8766283 1.7116013
> ## Syntax also useful for univariate generators, e.g.
> m <- lvm(y~x+z)
> distribution(m,~y,TRUE) <- function(n) rnorm(n,mean=1000)
> sim(m,5)
          y          x            z
1 1001.4561  2.1228046  0.457217169
2  997.2569 -1.7149654 -0.432492216
3  999.3911 -0.2134243  0.004930436
4 1000.5808 -0.8566886  0.658816887
5  998.3482 -0.7746736 -1.131384743
> distribution(m,~y,"m1",0) <- rnorm
> sim(m,5)
           y          x          z
1 -1.4773633  0.4969118 -1.3708991
2  0.9233462 -0.2507800  0.6506011
3 -2.8289503 -1.2978990 -0.6079915
4  1.3758867 -0.1766202 -0.1071758
5 -2.5949512 -1.9199932 -0.6471151
> sim(m,5,p=c(m1=100))
          y          x           z
1 101.61566  0.8227685 -0.12340449
2  98.99602 -0.6014433  0.49460763
3  98.29964  0.4697672 -0.56273573
4 100.97198  1.0422235 -0.76874242
5  99.23308 -0.5415025 -0.05691879
> ##################################################
> ### Regression design in other parameters
> ##################################################
> ## Variance heterogeneity
> m <- lvm(y~x)
> distribution(m,~y) <- function(n,mean,x) rnorm(n,mean,exp(x)^.5)
> if (interactive()) plot(y~x,sim(m,1e3))
> ## Alternaively, calculate the standard error directly
> addvar(m) <- ~sd ## If 'sd' should be part of the resulting data.frame
> constrain(m,sd~x) <- function(x) exp(x)^.5
> distribution(m,~y) <- function(n,mean,sd) rnorm(n,mean,sd)
> if (interactive()) plot(y~x,sim(m,1e3))
> ## Regression on variance parameter
> m <- lvm()
> regression(m) <- y~x
> regression(m) <- v~x
> ##distribution(m,~v) <- 0 # No stochastic term
> ## Alternative:
> ## regression(m) <- v[NA:0]~x
> distribution(m,~y) <- function(n,mean,v) rnorm(n,mean,exp(v)^.5)
> if (interactive()) plot(y~x,sim(m,1e3))
> ## Regression on shape parameter in Weibull model
> m <- lvm()
> regression(m) <- y ~ z+v
> regression(m) <- s ~ exp(0.6*x-0.5*z)
> distribution(m,~x+z) <- binomial.lvm()
> distribution(m,~cens) <- coxWeibull.lvm(scale=1)
> distribution(m,~y) <- coxWeibull.lvm(scale=0.1,shape=~s)
> eventTime(m) <- time ~ min(y=1,cens=0)
> if (interactive()) {
+     d <- sim(m,1e3)
+     require(survival)
+     (cc <- coxph(Surv(time,status)~v+strata(x,z),data=d))
+     plot(survfit(cc) ,col=1:4,mark.time=FALSE)
+ }
> ##################################################
> ### Categorical predictor
> ##################################################
> m <- lvm()
> ## categorical(m,K=3) <- "v"
> categorical(m,labels=c("A","B","C")) <- "v"
> regression(m,additive=FALSE) <- y~v
> ## Not run: 
> ##D plot(y~v,sim(m,1000,p=c("y~v:2"=3)))
> ## End(Not run)
> m <- lvm()
> categorical(m,labels=c("A","B","C"),p=c(0.5,0.3)) <- "v"
> regression(m,additive=FALSE,beta=c(0,2,-1)) <- y~v
> ## equivalent to:
> ## regression(m,y~v,additive=FALSE) <- c(0,2,-1)
> regression(m,additive=FALSE,beta=c(0,4,-1)) <- z~v
> table(sim(m,1e4)$v)

   A    B    C 
4962 3067 1971 
> glm(y~v, data=sim(m,1e4))

Call:  glm(formula = y ~ v, data = sim(m, 10000))

Coefficients:
(Intercept)           vB           vC  
     0.0398       1.9496      -1.0231  

Degrees of Freedom: 9999 Total (i.e. Null);  9997 Residual
Null Deviance:	    21910 
Residual Deviance: 9737 	AIC: 28120
> glm(y~v, data=sim(m,1e4,p=c("y~v:1"=3)))

Call:  glm(formula = y ~ v, data = sim(m, 10000, p = c(`y~v:1` = 3)))

Coefficients:
(Intercept)           vB           vC  
  0.0002468    3.0076020   -1.0363420  

Degrees of Freedom: 9999 Total (i.e. Null);  9997 Residual
Null Deviance:	    34330 
Residual Deviance: 9863 	AIC: 28250
> 
> transform(m,v2~v) <- function(x) x=='A'
> sim(m,10)
   v           y          z    v2
1  A  0.03741642 -2.4698639  TRUE
2  A  1.57956452  0.8377181  TRUE
3  A -1.73961416  0.3377552  TRUE
4  C -0.05745277 -2.4926607 FALSE
5  B -0.07760844  3.8629309 FALSE
6  A  1.41290840  1.0933859  TRUE
7  B  1.37987719  5.5822172 FALSE
8  C  0.34651143 -0.9941847 FALSE
9  B  1.77621510  3.4916885 FALSE
10 C -0.43414989 -1.6168770 FALSE
> 
> ##################################################
> ### Pre-calculate object
> ##################################################
> m <- lvm(y~x)
> m2 <- sim(m,'y~x'=2)
> sim(m,10,'y~x'=2)
            y          x
1   1.8965495  0.2419910
2  -1.2782818 -0.6074227
3   0.5221788  0.4174805
4   0.6080455  0.6225067
5  -0.6999894  0.0605951
6   0.9787804  0.4677878
7   4.5422007  1.4097596
8   1.5334800  0.3668357
9   2.6844918  1.8739976
10  0.3504188 -0.2874095
> sim(m2,10) ## Faster
            y          x
1  -0.9247279  0.1908440
2   0.6149229  0.5921454
3  -1.7496668 -0.8469927
4  -0.2861444  0.8591927
5  -0.2902616 -0.4694311
6   2.6675922  0.4881480
7   0.1122491 -1.0808903
8   0.1736444  0.5364615
9   0.3195652  0.7347469
10  3.7999734  0.8379644
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.default")
> ### * sim.default
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.default
> ### Title: Wrapper function for mclapply
> ### Aliases: sim.default summary.sim
> 
> ### ** Examples
> 
> m <- lvm(y~x+e)
> distribution(m,~y) <- 0
> distribution(m,~x) <- uniform.lvm(a=-1.1,b=1.1)
> transform(m,e~x) <- function(x) (1*x^4)*rnorm(length(x),sd=1)
> 
> onerun <- function(iter=NULL,...,n=2e3,b0=1,idx=2) {
+     d <- sim(m,n,p=c("y~x"=b0))
+     l <- lm(y~x,d)
+     res <- c(coef(summary(l))[idx,1:2],
+              confint(l)[idx,],
+              estimate(l,only.coef=TRUE)[idx,2:4])
+     names(res) <- c("Estimate","Model.se","Model.lo","Model.hi",
+                     "Sandwich.se","Sandwich.lo","Sandwich.hi")
+     res
+ }
> val <- sim(onerun,R=10,b0=1,messages=0,mc.cores=1)
> val
   Estimate Model.se Model.lo Model.hi Sandwich.se Sandwich.lo Sandwich.hi
1  1.003901 0.015619 0.973270 1.034532 0.021637    0.961493    1.046309   
2  1.029852 0.026711 0.977467 1.082237 0.036469    0.958374    1.101330   
3  1.029042 0.018065 0.993614 1.064469 0.025720    0.978632    1.079452   
4  0.973448 0.018570 0.937028 1.009867 0.026177    0.922142    1.024754   
5  0.991726 0.015324 0.961674 1.021778 0.021300    0.949978    1.033474   
6  0.996784 0.007383 0.982306 1.011263 0.010429    0.976344    1.017225   
7  0.999952 0.010400 0.979556 1.020348 0.014493    0.971546    1.028358   
8  1.005265 0.007559 0.990440 1.020089 0.010642    0.984406    1.026123   
9  1.038505 0.022065 0.995232 1.081777 0.031402    0.976958    1.100051   
10 1.035017 0.016936 1.001803 1.068231 0.023722    0.988523    1.081510   
> 
> val <- sim(val,R=40,b0=1,mc.cores=1) ## append results
  |                                                |                                        |   0%  |                                                |==                                      |   5%  |                                                |====                                    |  10%  |                                                |======                                  |  15%  |                                                |========                                |  20%  |                                                |==========                              |  25%  |                                                |============                            |  30%  |                                                |==============                          |  35%  |                                                |================                        |  40%  |                                                |==================                      |  45%  |                                                |====================                    |  50%  |                                                |======================                  |  55%  |                                                |========================                |  60%  |                                                |==========================              |  65%  |                                                |============================            |  70%  |                                                |==============================          |  75%  |                                                |================================        |  80%  |                                                |==================================      |  85%  |                                                |====================================    |  90%  |                                                |======================================  |  95%  |                                                |========================================| 100%
> summary(val,estimate=c(1,1),confint=c(3,4,6,7),true=c(1,1))
50 replications					Time: 3.184s

          Estimate Estimate.1
Mean     1.0023683  1.0023683
SD       0.0193543  0.0193543
                             
Min      0.9376183  0.9376183
2.5%     0.9705260  0.9705260
50%      1.0005084  1.0005084
97.5%    1.0377198  1.0377198
Max      1.0483889  1.0483889
                             
Missing  0.0000000  0.0000000
                             
True     1.0000000  1.0000000
Bias     0.0023683  0.0023683
RMSE     0.0194987  0.0194987
                             
Coverage 0.7800000  0.9200000

> 
> summary(val,estimate=c(1,1),se=c(2,5),names=c("Model","Sandwich"))
50 replications					Time: 3.184s

           Model Sandwich
Mean    1.002368 1.002368
SD      0.019354 0.019354
SE      0.011602 0.016288
SE/SD   0.599469 0.841568
                         
Min     0.937618 0.937618
2.5%    0.970526 0.970526
50%     1.000508 1.000508
97.5%   1.037720 1.037720
Max     1.048389 1.048389
                         
Missing 0.000000 0.000000

> summary(val,estimate=c(1,1),se=c(2,5),true=c(1,1),names=c("Model","Sandwich"),confint=TRUE)
50 replications					Time: 3.184s

             Model  Sandwich
Mean     1.0023683 1.0023683
SD       0.0193543 0.0193543
SE       0.0116023 0.0162880
SE/SD    0.5994693 0.8415681
                            
Min      0.9376183 0.9376183
2.5%     0.9705260 0.9705260
50%      1.0005084 1.0005084
97.5%    1.0377198 1.0377198
Max      1.0483889 1.0483889
                            
Missing  0.0000000 0.0000000
                            
True     1.0000000 1.0000000
Bias     0.0023683 0.0023683
RMSE     0.0194987 0.0194987
                            
Coverage 0.7800000 0.9200000

> 
> if (interactive()) {
+     plot(val,estimate=1,c(2,5),true=1,names=c("Model","Sandwich"),polygon=FALSE)
+     plot(val,estimate=c(1,1),se=c(2,5),main=NULL,
+          true=c(1,1),names=c("Model","Sandwich"),
+          line.lwd=1,density.col=c("gray20","gray60"),
+          rug=FALSE)
+     plot(val,estimate=c(1,1),se=c(2,5),true=c(1,1),
+          names=c("Model","Sandwich"))
+ }
> 
> f <- function(a=1,b=1) {
+   rep(a*b,5)
+ }
> R <- Expand(a=1:3,b=1:3)
> sim(f,R,type=0)
  [,1] [,2] [,3] [,4] [,5]
1 1    1    1    1    1   
2 2    2    2    2    2   
3 3    3    3    3    3   
4 2    2    2    2    2   
5 4    4    4    4    4   
6 6    6    6    6    6   
7 3    3    3    3    3   
8 6    6    6    6    6   
9 9    9    9    9    9   
> sim(function(a,b) f(a,b), 3, args=c(a=5,b=5),type=0)
  [,1]
1 NA  
2 NA  
3 NA  
> sim(function(iter=1,a=5,b=5) iter*f(a,b), type=0, iter=TRUE, R=5)
  [,1] [,2] [,3] [,4] [,5]
1 25   25   25   25   25  
2 25   25   25   25   25  
3 25   25   25   25   25  
4 25   25   25   25   25  
5 25   25   25   25   25  
> 
> 
> 
> cleanEx()
> nameEx("spaghetti")
> ### * spaghetti
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spaghetti
> ### Title: Spaghetti plot
> ### Aliases: spaghetti
> 
> ### ** Examples
> 
> if (interactive() & requireNamespace("mets")) {
+ K <- 5
+ y <- "y"%++%seq(K)
+ m <- lvm()
+ regression(m,y=y,x=~u) <- 1
+ regression(m,y=y,x=~s) <- seq(K)-1
+ regression(m,y=y,x=~x) <- "b"
+ N <- 50
+ d <- sim(m,N); d$z <- rbinom(N,1,0.5)
+ dd <- mets::fast.reshape(d); dd$num <- dd$num+3
+ spaghetti(y~num,dd,id="id",lty=1,col=Col(1,.4),
+           trend.formula=~factor(num),trend=TRUE,trend.col="darkblue")
+ dd$num <- dd$num+rnorm(nrow(dd),sd=0.5) ## Unbalance
+ spaghetti(y~num,dd,id="id",lty=1,col=Col(1,.4),
+           trend=TRUE,trend.col="darkblue")
+ spaghetti(y~num,dd,id="id",lty=1,col=Col(1,.4),
+            trend.formula=~num+I(num^2),trend=TRUE,trend.col="darkblue")
+ }
> 
> 
> 
> cleanEx()
> nameEx("subset.lvm")
> ### * subset.lvm
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: subset.lvm
> ### Title: Extract subset of latent variable model
> ### Aliases: subset.lvm measurement
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> m <- lvm(c(y1,y2)~x1+x2)
> subset(m,~y1+x1)
Latent Variable Model
                    
  y1 ~ x1   gaussian

Exogenous variables:                    
  x1        gaussian

> 
> 
> 
> 
> cleanEx()
> nameEx("timedep")
> ### * timedep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: timedep
> ### Title: Time-dependent parameters
> ### Aliases: timedep timedep<-
> 
> ### ** Examples
> 
> 
> ## Piecewise constant hazard
> m <- lvm(y~1)
> m <- timedep(m,y~1,timecut=c(0,5),rate=c(0.5,0.3))
> 
> ## Not run: 
> ##D d <- sim(m,1e4); d$status <- TRUE
> ##D dd <- mets::lifetable(Surv(y,status)~1,data=d,breaks=c(0,5,10));
> ##D exp(coef(glm(events ~ offset(log(atrisk)) + -1 + interval, dd, family=poisson)))
> ## End(Not run)
> 
> 
> ## Piecewise constant hazard and time-varying effect of z1
> m <- lvm(y~1)
> distribution(m,~z1) <- ones.lvm(0.5)
> R <- log(cbind(c(0.2,0.7,0.9),c(0.5,0.3,0.3)))
> m <- timedep(m,y~z1,timecut=c(0,3,5),rate=R)
> 
> ## Not run: 
> ##D d <- sim(m,1e4); d$status <- TRUE
> ##D dd <- mets::lifetable(Surv(y,status)~z1,data=d,breaks=c(0,3,5,Inf));
> ##D exp(coef(glm(events ~ offset(log(atrisk)) + -1 + interval+z1:interval, dd, family=poisson)))
> ## End(Not run)
> 
> 
> 
> ## Explicit simulation of time-varying effects
> m <- lvm(y~1)
> distribution(m,~z1) <- ones.lvm(0.5)
> distribution(m,~z2) <- binomial.lvm(p=0.5)
> #variance(m,~m1+m2) <- 0
> #regression(m,m1[m1:0] ~ z1) <- log(0.5)
> #regression(m,m2[m2:0] ~ z1) <- log(0.3)
> regression(m,m1 ~ z1,variance=0) <- log(0.5)
> regression(m,m2 ~ z1,variance=0) <- log(0.3)
> intercept(m,~m1+m2) <- c(-0.5,0)
> m <- timedep(m,y~m1+m2,timecut=c(0,5))
> 
> ## Not run: 
> ##D d <- sim(m,1e5); d$status <- TRUE
> ##D dd <- mets::lifetable(Surv(y,status)~z1,data=d,breaks=c(0,5,Inf))
> ##D exp(coef(glm(events ~ offset(log(atrisk)) + -1 + interval + interval:z1, dd, family=poisson)))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("toformula")
> ### * toformula
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: toformula
> ### Title: Converts strings to formula
> ### Aliases: toformula
> ### Keywords: models utilities
> 
> ### ** Examples
> 
> 
> toformula(c("age","gender"), "weight")
c(age, gender) ~ weight
<environment: 0x55b9fb69ea08>
> 
> 
> 
> 
> cleanEx()
> nameEx("tr")
> ### * tr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tr
> ### Title: Trace operator
> ### Aliases: tr
> ### Keywords: algebra math
> 
> ### ** Examples
> 
> 
> tr(diag(1:5))
[1] 15
> 
> 
> 
> cleanEx()
> nameEx("twostage.lvmfit")
> ### * twostage.lvmfit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: twostage.lvmfit
> ### Title: Two-stage estimator (non-linear SEM)
> ### Aliases: twostage.lvmfit twostage.lvm twostage.lvm.mixture
> ###   twostage.estimate
> 
> ### ** Examples
> 
> m <- lvm(c(x1,x2,x3)~f1,f1~z,
+          c(y1,y2,y3)~f2,f2~f1+z)
> latent(m) <- ~f1+f2
> d <- simulate(m,100,p=c("f2,f2"=2,"f1,f1"=0.5),seed=1)
> 
> ## Full MLE
> ee <- estimate(m,d)
> 
> ## Manual two-stage
> ## Not run: 
> ##D m1 <- lvm(c(x1,x2,x3)~f1,f1~z); latent(m1) <- ~f1
> ##D e1 <- estimate(m1,d)
> ##D pp1 <- predict(e1,f1~x1+x2+x3)
> ##D 
> ##D d$u1 <- pp1[,]
> ##D d$u2 <- pp1[,]^2+attr(pp1,"cond.var")
> ##D m2 <- lvm(c(y1,y2,y3)~eta,c(y1,eta)~u1+u2+z); latent(m2) <- ~eta
> ##D e2 <- estimate(m2,d)
> ## End(Not run)
> 
> ## Two-stage
> m1 <- lvm(c(x1,x2,x3)~f1,f1~z); latent(m1) <- ~f1
> m2 <- lvm(c(y1,y2,y3)~eta,c(y1,eta)~u1+u2+z); latent(m2) <- ~eta
> pred <- function(mu,var,data,...)
+     cbind("u1"=mu[,1],"u2"=mu[,1]^2+var[1])
> (mm <- twostage(m1,model2=m2,data=d,predictfun=pred))
                    Estimate Std. Error  Z-value   P-value
Measurements:                                             
   y2~eta            0.96270    0.12462  7.72502    <1e-12
   y3~eta            0.97886    0.12478  7.84483    <1e-12
Regressions:                                              
   y1~u1            -0.10923    0.24755 -0.44124     0.659
   y1~u2            -0.00916    0.02941 -0.31149    0.7554
   y1~z             -0.09088    0.25698 -0.35364    0.7236
    eta~u1           1.23462    0.24891  4.96005 7.048e-07
    eta~u2           0.00912    0.02417  0.37737    0.7059
    eta~z            0.84531    0.27943  3.02506  0.002486
Intercepts:                                               
   y2               -0.19048    0.16526 -1.15257    0.2491
   y3                0.00979    0.18282  0.05354    0.9573
   eta              -0.16805    0.22950 -0.73226     0.464
Residual Variances:                                       
   y1                1.15103    0.24220  4.75248          
   y2                0.97707    0.20212  4.83411          
   y3                1.13661    0.20506  5.54288          
   eta               1.58985    0.37736  4.21305          
> 
> if (interactive()) {
+     pf <- function(p) p["eta"]+p["eta~u1"]*u + p["eta~u2"]*u^2
+     plot(mm,f=pf,data=data.frame(u=seq(-2,2,length.out=100)),lwd=2)
+ }
> 
> 
> 
> cleanEx()
> nameEx("vars")
> ### * vars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vars
> ### Title: Extract variable names from latent variable model
> ### Aliases: vars vars.lvm vars.lvmfit latent latent<- latent.lvm
> ###   latent<-.lvm latent.lvmfit latent.multigroup manifest manifest.lvm
> ###   manifest.lvmfit manifest.multigroup exogenous exogenous<-
> ###   exogenous.lvm exogenous<-.lvm exogenous.lvmfit exogenous.multigroup
> ###   endogenous endogenous.lvm endogenous.lvmfit endogenous.multigroup
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
> g <- lvm(eta1 ~ x1+x2)
> regression(g) <- c(y1,y2,y3) ~ eta1
> latent(g) <- ~eta1
> endogenous(g)
[1] "y1" "y2" "y3"
> exogenous(g)
[1] "x1" "x2"
> identical(latent(g), setdiff(vars(g),manifest(g)))
[1] TRUE
> 
> 
> 
> 
> cleanEx()
> nameEx("wrapvec")
> ### * wrapvec
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wrapvec
> ### Title: Wrap vector
> ### Aliases: wrapvec
> 
> ### ** Examples
> 
> wrapvec(5,2)
[1] 3 4 5 1 2
> 
> 
> 
> cleanEx()
> nameEx("zibreg")
> ### * zibreg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: zibreg
> ### Title: Regression model for binomial data with unkown group of
> ###   immortals
> ### Aliases: zibreg
> 
> ### ** Examples
> 
> 
> ## Simulation
> n <- 2e3
> x <- runif(n,0,20)
> age <- runif(n,10,30)
> z0 <- rnorm(n,mean=-1+0.05*age)
> z <- cut(z0,breaks=c(-Inf,-1,0,1,Inf))
> p0 <- lava:::expit(model.matrix(~z+age) %*% c(-.4, -.4, 0.2, 2, -0.05))
> y <- (runif(n)<lava:::tigol(-1+0.25*x-0*age))*1
> u <- runif(n)<p0
> y[u==0] <- 0
> d <- data.frame(y=y,x=x,u=u*1,z=z,age=age)
> head(d)
  y         x u         z      age
1 1  5.310173 1    (-1,0] 27.43610
2 0  7.442478 0 (-Inf,-1] 29.34394
3 0 11.457067 0  (1, Inf] 27.33833
4 1 18.164156 1     (0,1] 18.75431
5 0  4.033639 1    (-1,0] 13.83876
6 1 17.967794 1     (0,1] 11.64589
> 
> ## Estimation
> e0 <- zibreg(y~x*z,~1+z+age,data=d)
> e <- zibreg(y~x,~1+z+age,data=d)
> compare(e,e0)

	- Likelihood ratio test -

data:  
chisq = 5.0664, df = 6, p-value = 0.5353
sample estimates:
log likelihood (model 1) log likelihood (model 2) 
               -845.8552                -843.3220 

> e
                  Estimate        2.5%       97.5%      P-value
(Intercept)    -0.79311848 -1.26547420 -0.32076276 9.986123e-04
x               0.17870814  0.08529604  0.27212024 1.770903e-04
pr:(Intercept)  0.09076588 -0.57838813  0.75991988 7.903510e-01
pr:z(-1,0]     -0.04218406 -0.47555036  0.39118224 8.486950e-01
pr:z(0,1]       0.62408319  0.19865575  1.04951062 4.037969e-03
pr:z(1, Inf]    2.92694978  2.20174865  3.65215091 2.563214e-15
pr:age         -0.09277388 -0.12060813 -0.06493962 6.458472e-11

Prevalence probabilities:
                             Estimate      2.5%     97.5%
{(Intercept)}               0.5226759 0.3593036 0.6813363
{(Intercept)} + {z(-1,0]}   0.5121431 0.3520800 0.6697539
{(Intercept)} + {z(0,1]}    0.6714717 0.5020871 0.8055499
{(Intercept)} + {z(1, Inf]} 0.9533681 0.8683512 0.9844645
{(Intercept)} + {age}       0.4994980 0.3431070 0.6559873
> PD(e0,intercept=c(1,3),slope=c(2,6))
    Estimate  Std.Err      2.5%    97.5%
50% 3.754552 1.763937 0.2972997 7.211805
attr(,"b")
[1] -1.4012212  0.3732059
> 
> B <- rbind(c(1,0,0,0,20),
+            c(1,1,0,0,20),
+            c(1,0,1,0,20),
+            c(1,0,0,1,20))
> prev <- summary(e,pr.contrast=B)$prevalence
> 
> x <- seq(0,100,length.out=100)
> newdata <- expand.grid(x=x,age=20,z=levels(d$z))
> fit <- predict(e,newdata=newdata)
> plot(0,0,type="n",xlim=c(0,101),ylim=c(0,1),xlab="x",ylab="Probability(Event)")
> count <- 0
> for (i in levels(newdata$z)) {
+   count <- count+1
+   lines(x,fit[which(newdata$z==i)],col="darkblue",lty=count)
+ }
> abline(h=prev[3:4,1],lty=3:4,col="gray")
> abline(h=prev[3:4,2],lty=3:4,col="lightgray")
> abline(h=prev[3:4,3],lty=3:4,col="lightgray")
> legend("topleft",levels(d$z),col="darkblue",lty=seq_len(length(levels(d$z))))
> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  88.992 1.324 90.568 0.004 0.092 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
