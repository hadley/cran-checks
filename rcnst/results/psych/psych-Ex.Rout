
R Under development (unstable) (2017-08-14 r73093) -- "Unsuffered Consequences"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "psych"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('psych')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("00.psych-package")
> ### * 00.psych-package
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: 00.psych
> ### Title: A package for personality, psychometric, and psychological
> ###   research
> ### Aliases: psych psych-package 00.psych-package
> ### Keywords: package multivariate models cluster
> 
> ### ** Examples
> 
> #See the separate man pages 
> #to test most of the psych package run the following
> #test.psych()   
> 
> 
> 
> cleanEx()
> nameEx("Gleser")
> ### * Gleser
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gleser
> ### Title: Example data from Gleser, Cronbach and Rajaratnam (1965) to show
> ###   basic principles of generalizability theory.
> ### Aliases: Gleser
> ### Keywords: datasets
> 
> ### ** Examples
> 
> #Find the MS for each component:
> #First, stack the data
> data(Gleser)
> stack.g <- stack(Gleser)
> st.gc.df <- data.frame(stack.g,Persons=rep(letters[1:12],12),
+ Items=rep(letters[1:6],each=24),Judges=rep(letters[1:2],each=12))
> #now do the ANOVA
> anov <- aov(values ~ (Persons*Judges*Items),data=st.gc.df)
> summary(anov)
                     Df Sum Sq Mean Sq
Persons              11  84.17   7.652
Judges                1   1.00   1.000
Items                 5  64.67  12.933
Persons:Judges       11  19.50   1.773
Persons:Items        55  81.17   1.476
Judges:Items          5   4.33   0.867
Persons:Judges:Items 55  34.17   0.621
> 
> 
> 
> cleanEx()
> nameEx("Gorsuch")
> ### * Gorsuch
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Gorsuch
> ### Title: Example data set from Gorsuch (1997) for an example factor
> ###   extension.
> ### Aliases: Gorsuch
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Gorsuch)
> 
> Ro <- Gorsuch[1:6,1:6]
> Roe <- Gorsuch[1:6,7:10]
> fo <- fa(Ro,2,rotate="none")
> fa.extension(Roe,fo,correct=FALSE)

Call: fa.extension(Roe = Roe, fo = fo, correct = FALSE)
Standardized loadings (pattern matrix) based upon correlation matrix
           MR1  MR2   h2   u2
info2     0.72 0.34 0.63 0.37
tension2 -0.40 0.54 0.45 0.55
v123      0.82 0.41 0.84 0.16
v564     -0.45 0.71 0.71 0.29

                       MR1  MR2
SS loadings           1.55 1.08
Proportion Var        0.39 0.27
Cumulative Var        0.39 0.66
Proportion Explained  0.59 0.41
Cumulative Proportion 0.59 1.00
> 
> 
> 
> cleanEx()
> nameEx("Harman.5")
> ### * Harman.5
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Harman.5
> ### Title: 5 socio-economic variables from Harman (1967)
> ### Aliases: Harman.5
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Harman.5)
> if(require('GPArotation')){
+ pc2 <- principal(Harman.5,2,scores=TRUE)
+ pc2$residual
+ biplot(pc2,main="Biplot of the Harman 5 socio-demographic variables") }
Loading required package: GPArotation
> 
> 
> 
> cleanEx()

detaching ‘package:GPArotation’

> nameEx("Harman.8")
> ### * Harman.8
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Harman.8
> ### Title: Correlations of eight physical variables (from Harman, 1966)
> ### Aliases: Harman.8
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Harman.8)
> cor.plot(Harman.8)
> fa(Harman.8,2,rotate="none")  #the minres solution
Factor Analysis using method =  minres
Call: fa(r = Harman.8, nfactors = 2, rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
             MR1   MR2   h2   u2 com
Height      0.86 -0.32 0.84 0.16 1.3
Arm span    0.85 -0.41 0.89 0.11 1.4
Forearm     0.81 -0.41 0.82 0.18 1.5
Leg length  0.83 -0.34 0.81 0.19 1.3
Weight      0.75  0.57 0.89 0.11 1.9
Hips        0.63  0.49 0.64 0.36 1.9
Chest girth 0.57  0.51 0.58 0.42 2.0
Chest width 0.61  0.35 0.49 0.51 1.6

                       MR1  MR2
SS loadings           4.45 1.51
Proportion Var        0.56 0.19
Cumulative Var        0.56 0.74
Proportion Explained  0.75 0.25
Cumulative Proportion 0.75 1.00

Mean item complexity =  1.6
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  28  and the objective function was  6.94
The degrees of freedom for the model are 13  and the objective function was  0.26 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR2
Correlation of scores with factors             0.98 0.94
Multiple R square of scores with factors       0.96 0.89
Minimum correlation of possible factor scores  0.93 0.78
> fa(Harman.8,2,rotate="none",fm="pa") #the principal axis solution
Factor Analysis using method =  pa
Call: fa(r = Harman.8, nfactors = 2, rotate = "none", fm = "pa")
Standardized loadings (pattern matrix) based upon correlation matrix
             PA1   PA2   h2   u2 com
Height      0.86 -0.32 0.84 0.16 1.3
Arm span    0.85 -0.41 0.89 0.11 1.4
Forearm     0.81 -0.41 0.82 0.18 1.5
Leg length  0.83 -0.34 0.81 0.19 1.3
Weight      0.75  0.57 0.89 0.11 1.9
Hips        0.63  0.49 0.64 0.36 1.9
Chest girth 0.57  0.51 0.58 0.42 2.0
Chest width 0.61  0.35 0.49 0.51 1.6

                       PA1  PA2
SS loadings           4.45 1.51
Proportion Var        0.56 0.19
Cumulative Var        0.56 0.74
Proportion Explained  0.75 0.25
Cumulative Proportion 0.75 1.00

Mean item complexity =  1.6
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  28  and the objective function was  6.94
The degrees of freedom for the model are 13  and the objective function was  0.26 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                PA1  PA2
Correlation of scores with factors             0.98 0.94
Multiple R square of scores with factors       0.96 0.89
Minimum correlation of possible factor scores  0.93 0.77
> 
> 
> 
> 
> cleanEx()
> nameEx("Harman")
> ### * Harman
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Harman
> ### Title: Two data sets from Harman (1967). 9 cognitive variables from
> ###   Holzinger and 8 emotional variables from Burt
> ### Aliases: Harman Harman.Burt Harman.Holzinger
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Harman)
> cor.plot(Harman.Holzinger)
> cor.plot(Harman.Burt)  
> smc(Harman.Burt)  #note how this produces impossible results
Warning in cor.smooth(R) :
  Matrix was not positive definite, smoothing was done
Sociability      Sorrow  Tenderness         Joy      Wonder     Disgust 
  1.0000000   1.0000000   1.0000000   1.0000000   1.0000000   1.0000000 
      Anger        Fear 
  1.0000000   0.9987481 
> 
> 
> 
> cleanEx()
> nameEx("Harman.political")
> ### * Harman.political
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Harman.political
> ### Title: Eight political variables used by Harman (1967) as example 8.17
> ### Aliases: Harman.political
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Harman.political)
> KMO(Harman.political)
Kaiser-Meyer-Olkin factor adequacy
Call: KMO(r = Harman.political)
Overall MSA =  0.81
MSA for each item = 
        Lewis     Roosevelt  Party Voting Median Rental Homeownership 
         0.73          0.76          0.84          0.87          0.53 
 Unemployment      Mobility     Education 
         0.93          0.78          0.86 
> 
> 
> 
> cleanEx()
> nameEx("ICC")
> ### * ICC
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ICC
> ### Title: Intraclass Correlations (ICC1, ICC2, ICC3 from Shrout and
> ###   Fleiss)
> ### Aliases: ICC
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> sf <- matrix(c(9,    2,   5,    8,
+ 6,    1,   3,    2,
+ 8,    4,   6,    8,
+ 7,    1,   2,    6,
+ 10,   5,   6,    9,
+ 6,   2,   4,    7),ncol=4,byrow=TRUE)
> colnames(sf) <- paste("J",1:4,sep="")
> rownames(sf) <- paste("S",1:6,sep="")
> sf  #example from Shrout and Fleiss (1979)
   J1 J2 J3 J4
S1  9  2  5  8
S2  6  1  3  2
S3  8  4  6  8
S4  7  1  2  6
S5 10  5  6  9
S6  6  2  4  7
> ICC(sf)
Call: ICC(x = sf)

Intraclass correlation coefficients 
                         type  ICC    F df1 df2       p lower bound upper bound
Single_raters_absolute   ICC1 0.17  1.8   5  18 0.16477      -0.133        0.72
Single_random_raters     ICC2 0.29 11.0   5  15 0.00013       0.019        0.76
Single_fixed_raters      ICC3 0.71 11.0   5  15 0.00013       0.342        0.95
Average_raters_absolute ICC1k 0.44  1.8   5  18 0.16477      -0.884        0.91
Average_random_raters   ICC2k 0.62 11.0   5  15 0.00013       0.071        0.93
Average_fixed_raters    ICC3k 0.91 11.0   5  15 0.00013       0.676        0.99

 Number of subjects = 6     Number of Judges =  4> 
> 
> 
> cleanEx()
> nameEx("ICLUST")
> ### * ICLUST
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: iclust
> ### Title: iclust: Item Cluster Analysis - Hierarchical cluster analysis
> ###   using psychometric principles
> ### Aliases: ICLUST iclust
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
> test.data <- Harman74.cor$cov
> ic.out <- iclust(test.data,title="ICLUST of the Harman data")
> summary(ic.out)
ICLUST (Item Cluster Analysis)Call: iclust(r.mat = test.data, title = "ICLUST of the Harman data")
ICLUST of the Harman data 

Purified Alpha:
[1] 0.91

 Guttman Lambda6* 
[1] 0.94

Original Beta:
[1] 0.63

Cluster size:
[1] 24

Purified scale intercorrelations
 reliabilities on diagonal
 correlations corrected for attenuation above diagonal: 
     [,1]
[1,] 0.91
> 
> #use all defaults and stop at 4 clusters
> ic.out4 <- iclust(test.data,nclusters =4,title="Force 4 clusters")  
> summary(ic.out4)
ICLUST (Item Cluster Analysis)Call: iclust(r.mat = test.data, nclusters = 4, title = "Force 4 clusters")
Force 4 clusters 

Purified Alpha:
 C13  C20  C15   V2 
0.90 0.84 0.81 0.48 

 Guttman Lambda6* 
 C13  C20   V2  C15 
0.90 0.87 0.52 0.84 

Original Beta:
 C13  C20   V2  C15 
0.79 0.74   NA 0.74 

Cluster size:
C13 C20 C15  V2 
  5  12   5   2 

Purified scale intercorrelations
 reliabilities on diagonal
 correlations corrected for attenuation above diagonal: 
     C13  C20  C15   V2
C13 0.90 0.69 0.54 0.52
C20 0.60 0.84 0.71 0.75
C15 0.46 0.59 0.81 0.39
V2  0.34 0.48 0.24 0.48
> ic.out1 <- iclust(test.data,beta=3,beta.size=3)  #use more stringent criteria
> ic.out  #more complete output 
ICLUST (Item Cluster Analysis)
Call: iclust(r.mat = test.data, title = "ICLUST of the Harman data")

Purified Alpha:
[1] 0.91

G6* reliability:
[1] 1

Original Beta:
[1] 0.63

Cluster size:
[1] 24

Item by Cluster Structure matrix:
                       [,1]
VisualPerception       0.60
Cubes                  0.38
PaperFormBoard         0.43
Flags                  0.48
GeneralInformation     0.67
PargraphComprehension  0.66
SentenceCompletion     0.64
WordClassification     0.66
WordMeaning            0.66
Addition               0.47
Code                   0.57
CountingDots           0.48
StraightCurvedCapitals 0.61
WordRecognition        0.43
NumberRecognition      0.40
FigureRecognition      0.53
ObjectNumber           0.48
NumberFigure           0.54
FigureWord             0.46
Deduction              0.62
NumericalPuzzles       0.61
ProblemReasoning       0.61
SeriesCompletion       0.69
ArithmeticProblems     0.66

With eigenvalues of:
[1] 7.6

Purified scale intercorrelations
 reliabilities on diagonal
 correlations corrected for attenuation above diagonal: 
     [,1]
[1,] 0.91

Cluster fit =  0.8   Pattern fit =  0.94  RMSR =  0.1 
> plot(ic.out4)    #this shows the spatial representation
Use ICLUST.diagram to see the  hierarchical structure
> #use a dot graphics viewer on the out.file
> dot.graph <- ICLUST.graph(ic.out,out.file="test.ICLUST.graph.dot")  
>  #show the equivalent of a factor solution 
> fa.diagram(ic.out4$pattern,Phi=ic.out4$Phi,main="Pattern taken from iclust") 
> 
> 
> 
> 
> cleanEx()
> nameEx("ICLUST.graph")
> ### * ICLUST.graph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ICLUST.graph
> ### Title: create control code for ICLUST graphical output
> ### Aliases: ICLUST.graph iclust.graph
> ### Keywords: multivariate cluster hplot
> 
> ### ** Examples
> 
> ## Not run: 
> ##D test.data <- Harman74.cor$cov
> ##D ic.out <- ICLUST(test.data)
> ##D out.file <- file.choose(new=TRUE)   #create a new file to write the plot commands to 
> ##D ICLUST.graph(ic.out,out.file)   
> ##D now go to graphviz (outside of R) and open the out.file you created
> ##D print(ic.out,digits=2)
> ## End(Not run)
> 
>  
> #test.data <- Harman74.cor$cov 
> #my.iclust <- ICLUST(test.data)
> #ICLUST.graph(my.iclust)
> #
> #
> #digraph ICLUST {
> #  rankdir=RL;
> #  size="8,8";
> #  node [fontname="Helvetica" fontsize=14 shape=box, width=2];
> #  edge [fontname="Helvetica" fontsize=12];
> # label = "ICLUST";
> #	fontsize=20;
> #V1  [label = VisualPerception];
> #V2  [label = Cubes];
> #V3  [label = PaperFormBoard];
> #V4  [label = Flags];
> #V5  [label = GeneralInformation];
> #V6  [label = PargraphComprehension];
> #V7  [label = SentenceCompletion];
> #V8  [label = WordClassification];
> #V9  [label = WordMeaning];
> #V10  [label = Addition];
> #V11  [label = Code];
> #V12  [label = CountingDots];
> #V13  [label = StraightCurvedCapitals];
> #V14  [label = WordRecognition];
> #V15  [label = NumberRecognition];
> #V16  [label = FigureRecognition];
> #V17  [label = ObjectNumber];
> #V18  [label = NumberFigure];
> #V19  [label = FigureWord];
> #V20  [label = Deduction];
> #V21  [label = NumericalPuzzles];
> #V22  [label = ProblemReasoning];
> #V23  [label = SeriesCompletion];
> #V24  [label = ArithmeticProblems];
> #node [shape=ellipse, width ="1"];
> #C1-> V9 [ label = 0.78 ];
> #C1-> V5 [ label = 0.78 ];
> #C2-> V12 [ label = 0.66 ];
> #C2-> V10 [ label = 0.66 ];
> #C3-> V18 [ label = 0.53 ];
> #C3-> V17 [ label = 0.53 ];
> #C4-> V23 [ label = 0.59 ];
> #C4-> V20 [ label = 0.59 ];
> #C5-> V13 [ label = 0.61 ];
> #C5-> V11 [ label = 0.61 ];
> #C6-> V7 [ label = 0.78 ];
> #C6-> V6 [ label = 0.78 ];
> #C7-> V4 [ label = 0.55 ];
> #C7-> V1 [ label = 0.55 ];
> #C8-> V16 [ label = 0.5 ];
> #C8-> V14 [ label = 0.49 ];
> #C9-> C1 [ label = 0.86 ];
> #C9-> C6 [ label = 0.86 ];
> #C10-> C4 [ label = 0.71 ];
> #C10-> V22 [ label = 0.62 ];
> #C11-> V21 [ label = 0.56 ];
> #C11-> V24 [ label = 0.58 ];
> #C12-> C10 [ label = 0.76 ];
> #C12-> C11 [ label = 0.67 ];
> #C13-> C8 [ label = 0.61 ];
> #C13-> V15 [ label = 0.49 ];
> #C14-> C2 [ label = 0.74 ];
> #C14-> C5 [ label = 0.72 ];
> #C15-> V3 [ label = 0.48 ];
> #C15-> C7 [ label = 0.65 ];
> #C16-> V19 [ label = 0.48 ];
> #C16-> C3 [ label = 0.64 ];
> #C17-> V8 [ label = 0.62 ];
> #C17-> C12 [ label = 0.8 ];
> #C18-> C17 [ label = 0.82 ];
> #C18-> C15 [ label = 0.68 ];
> #C19-> C16 [ label = 0.66 ];
> #C19-> C13 [ label = 0.65 ];
> #C20-> C19 [ label = 0.72 ];
> #C20-> C18 [ label = 0.83 ];
> #C21-> C20 [ label = 0.87 ];
> #C21-> C9 [ label = 0.76 ];
> #C22-> 0 [ label = 0 ];
> #C22-> 0 [ label = 0 ];
> #C23-> 0 [ label = 0 ];
> #C23-> 0 [ label = 0 ];
> #C1  [label =   "C1\n  alpha= 0.84\n beta=  0.84\nN= 2"] ;
> #C2  [label =   "C2\n  alpha= 0.74\n beta=  0.74\nN= 2"] ;
> #C3  [label =   "C3\n  alpha= 0.62\n beta=  0.62\nN= 2"] ;
> #C4  [label =   "C4\n  alpha= 0.67\n beta=  0.67\nN= 2"] ;
> #C5  [label =   "C5\n  alpha= 0.7\n beta=  0.7\nN= 2"] ;
> #C6  [label =   "C6\n  alpha= 0.84\n beta=  0.84\nN= 2"] ;
> #C7  [label =   "C7\n  alpha= 0.64\n beta=  0.64\nN= 2"] ;
> #C8  [label =   "C8\n  alpha= 0.58\n beta=  0.58\nN= 2"] ;
> #C9  [label =   "C9\n  alpha= 0.9\n beta=  0.87\nN= 4"] ;
> #C10  [label =   "C10\n  alpha= 0.74\n beta=  0.71\nN= 3"] ;
> #C11  [label =   "C11\n  alpha= 0.62\n beta=  0.62\nN= 2"] ;
> #C12  [label =   "C12\n  alpha= 0.79\n beta=  0.74\nN= 5"] ;
> #C13  [label =   "C13\n  alpha= 0.64\n beta=  0.59\nN= 3"] ;
> #C14  [label =   "C14\n  alpha= 0.79\n beta=  0.74\nN= 4"] ;
> #C15  [label =   "C15\n  alpha= 0.66\n beta=  0.58\nN= 3"] ;
> #C16  [label =   "C16\n  alpha= 0.65\n beta=  0.57\nN= 3"] ;
> #C17  [label =   "C17\n  alpha= 0.81\n beta=  0.71\nN= 6"] ;
> #C18  [label =   "C18\n  alpha= 0.84\n beta=  0.75\nN= 9"] ;
> #C19  [label =   "C19\n  alpha= 0.74\n beta=  0.65\nN= 6"] ;
> #C20  [label =   "C20\n  alpha= 0.87\n beta=  0.74\nN= 15"] ;
> #C21  [label =   "C21\n  alpha= 0.9\n beta=  0.77\nN= 19"] ;
> #C22  [label =   "C22\n  alpha= 0\n beta=  0\nN= 0"] ;
> #C23  [label =   "C23\n  alpha= 0\n beta=  0\nN= 0"] ;
> #{ rank=same;
> #V1;V2;V3;V4;V5;V6;V7;V8;V9;V10;V11;V12;V13;V14;V15;V16;V17;V18;V19;V20;V21;V22;V23;V24;}}
> #
> #copy the above output to Graphviz and draw it
> #see \url{http://personality-project.org/r/r.ICLUST.html} for an example.
> 
> 
> 
> 
> cleanEx()
> nameEx("ICLUST.rgraph")
> ### * ICLUST.rgraph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ICLUST.rgraph
> ### Title: Draw an ICLUST graph using the Rgraphviz package
> ### Aliases: ICLUST.rgraph
> ### Keywords: multivariate cluster hplot
> 
> ### ** Examples
> 
> test.data <- Harman74.cor$cov
> ic.out <- ICLUST(test.data)   #uses iclust.diagram instead 
> 
> 
> 
> cleanEx()
> nameEx("KMO")
> ### * KMO
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: KMO
> ### Title: Find the Kaiser, Meyer, Olkin Measure of Sampling Adequacy
> ### Aliases: KMO
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> KMO(Thurstone)
Kaiser-Meyer-Olkin factor adequacy
Call: KMO(r = Thurstone)
Overall MSA =  0.88
MSA for each item = 
        Sentences        Vocabulary   Sent.Completion     First.Letters 
             0.86              0.86              0.90              0.86 
Four.Letter.Words          Suffixes     Letter.Series         Pedigrees 
             0.88              0.92              0.85              0.93 
     Letter.Group 
             0.87 
> KMO(Harman.political)   #compare to the results in Dziuban and Shirkey (1974)
Kaiser-Meyer-Olkin factor adequacy
Call: KMO(r = Harman.political)
Overall MSA =  0.81
MSA for each item = 
        Lewis     Roosevelt  Party Voting Median Rental Homeownership 
         0.73          0.76          0.84          0.87          0.53 
 Unemployment      Mobility     Education 
         0.93          0.78          0.86 
> 
> 
> 
> 
> cleanEx()
> nameEx("Promax")
> ### * Promax
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Promax
> ### Title: Perform bifactor, promax or targeted rotations and return the
> ###   inter factor angles.
> ### Aliases: Promax TargetQ target.rot bifactor biquartimin varimin
> ###   vgQ.bimin vgQ.targetQ vgQ.varimin equamax
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> jen <- sim.hierarchical()
> f3 <- fa(jen,3,rotate="varimax")
> f3   #not a very clean solution
Factor Analysis using method =  minres
Call: fa(r = jen, nfactors = 3, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
    MR1  MR3  MR2   h2   u2 com
V1 0.70 0.30 0.26 0.64 0.36 1.6
V2 0.61 0.26 0.22 0.49 0.51 1.6
V3 0.52 0.22 0.19 0.36 0.64 1.6
V4 0.24 0.63 0.18 0.49 0.51 1.5
V5 0.21 0.54 0.16 0.36 0.64 1.5
V6 0.17 0.45 0.13 0.25 0.75 1.5
V7 0.17 0.15 0.56 0.36 0.64 1.3
V8 0.14 0.12 0.46 0.25 0.75 1.3
V9 0.11 0.10 0.37 0.16 0.84 1.3

                       MR1  MR3  MR2
SS loadings           1.32 1.15 0.89
Proportion Var        0.15 0.13 0.10
Cumulative Var        0.15 0.27 0.37
Proportion Explained  0.39 0.34 0.27
Cumulative Proportion 0.39 0.73 1.00

Mean item complexity =  1.5
Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  1.71
The degrees of freedom for the model are 12  and the objective function was  0 

The root mean square of the residuals (RMSR) is  0 
The df corrected root mean square of the residuals is  0 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR3   MR2
Correlation of scores with factors             0.78 0.73  0.67
Multiple R square of scores with factors       0.61 0.53  0.45
Minimum correlation of possible factor scores  0.22 0.07 -0.10
> Promax(f3)

Call: NULL
Standardized loadings (pattern matrix) based upon correlation matrix
    MR1  MR3  MR2   h2   u2
V1 0.76 0.04 0.03 0.64 0.36
V2 0.66 0.03 0.02 0.49 0.51
V3 0.57 0.03 0.02 0.36 0.64
V4 0.01 0.69 0.01 0.49 0.51
V5 0.01 0.59 0.01 0.36 0.64
V6 0.01 0.49 0.01 0.25 0.75
V7 0.00 0.00 0.60 0.36 0.64
V8 0.00 0.00 0.50 0.25 0.75
V9 0.00 0.00 0.40 0.16 0.84

                       MR1  MR3  MR2
SS loadings           1.43 1.13 0.80
Proportion Var        0.16 0.13 0.09
Cumulative Var        0.16 0.28 0.37
Proportion Explained  0.42 0.34 0.24
Cumulative Proportion 0.42 0.76 1.00
     MR1  MR3  MR2
MR1 1.00 0.68 0.60
MR3 0.68 1.00 0.55
MR2 0.60 0.55 1.00
> target.rot(f3)

Call: NULL
Standardized loadings (pattern matrix) based upon correlation matrix
   MR1 MR3 MR2   h2   u2
V1 0.8 0.0 0.0 0.64 0.36
V2 0.7 0.0 0.0 0.49 0.51
V3 0.6 0.0 0.0 0.36 0.64
V4 0.0 0.7 0.0 0.49 0.51
V5 0.0 0.6 0.0 0.36 0.64
V6 0.0 0.5 0.0 0.25 0.75
V7 0.0 0.0 0.6 0.36 0.64
V8 0.0 0.0 0.5 0.25 0.75
V9 0.0 0.0 0.4 0.16 0.84

                       MR1  MR3  MR2
SS loadings           1.49 1.10 0.77
Proportion Var        0.17 0.12 0.09
Cumulative Var        0.17 0.29 0.37
Proportion Explained  0.44 0.33 0.23
Cumulative Proportion 0.44 0.77 1.00
     MR1  MR3  MR2
MR1 1.00 0.72 0.63
MR3 0.72 1.00 0.56
MR2 0.63 0.56 1.00
> m3 <- fa(jen,nfactors=3)
> Promax(m3)  #example of taking the output from factanal

Call: NULL
Standardized loadings (pattern matrix) based upon correlation matrix
   MR1 MR3 MR2   h2   u2
V1 0.8 0.0 0.0 0.64 0.36
V2 0.7 0.0 0.0 0.49 0.51
V3 0.6 0.0 0.0 0.36 0.64
V4 0.0 0.7 0.0 0.49 0.51
V5 0.0 0.6 0.0 0.36 0.64
V6 0.0 0.5 0.0 0.25 0.75
V7 0.0 0.0 0.6 0.36 0.64
V8 0.0 0.0 0.5 0.25 0.75
V9 0.0 0.0 0.4 0.16 0.84

                       MR1  MR3  MR2
SS loadings           1.49 1.10 0.77
Proportion Var        0.17 0.12 0.09
Cumulative Var        0.17 0.29 0.37
Proportion Explained  0.44 0.33 0.23
Cumulative Proportion 0.44 0.77 1.00
    MR1 MR3 MR2
MR1   1   0   0
MR3   0   1   0
MR2   0   0   1
> #compare this rotation with the solution from a targeted rotation aimed for 
> #an independent cluster solution
> target.rot(m3)

Call: NULL
Standardized loadings (pattern matrix) based upon correlation matrix
   MR1 MR3 MR2   h2   u2
V1 0.8 0.0 0.0 0.64 0.36
V2 0.7 0.0 0.0 0.49 0.51
V3 0.6 0.0 0.0 0.36 0.64
V4 0.0 0.7 0.0 0.49 0.51
V5 0.0 0.6 0.0 0.36 0.64
V6 0.0 0.5 0.0 0.25 0.75
V7 0.0 0.0 0.6 0.36 0.64
V8 0.0 0.0 0.5 0.25 0.75
V9 0.0 0.0 0.4 0.16 0.84

                       MR1  MR3  MR2
SS loadings           1.49 1.10 0.77
Proportion Var        0.17 0.12 0.09
Cumulative Var        0.17 0.29 0.37
Proportion Explained  0.44 0.33 0.23
Cumulative Proportion 0.44 0.77 1.00
    MR1 MR3 MR2
MR1   1   0   0
MR3   0   1   0
MR2   0   0   1
> #now try a bifactor solution
> fb <-fa(jen,3,rotate="bifactor")
> fq <- fa(jen,3,rotate="biquartimin")
> #Suitbert Ertel has suggested varimin
> fm <-  fa(jen,3,rotate="varimin") #the Ertel varimin
> fn <- fa(jen,3,rotate="none")  #just the unrotated factors
> #compare them
> factor.congruence(list(f3,fb,fq,fm,fn))
      MR1   MR3  MR2  MR1   MR3   MR2   MR1   MR3   MR2  MR1   MR2   MR3  MR1
MR1  1.00  0.71 0.66 0.87  0.93 -0.02  0.92  0.93 -0.01 0.87  0.78  0.63 0.92
MR3  0.71  1.00 0.61 0.92  0.42 -0.36  0.89  0.42 -0.34 0.88  0.75 -0.08 0.89
MR2  0.66  0.61 1.00 0.84  0.41  0.52  0.82  0.41  0.54 0.88  0.16  0.40 0.82
MR1  0.87  0.92 0.84 1.00  0.61  0.00  0.99  0.61  0.01 1.00  0.65  0.27 0.99
MR3  0.93  0.42 0.41 0.61  1.00  0.00  0.71  1.00  0.00 0.62  0.73  0.80 0.71
MR2 -0.02 -0.36 0.52 0.00  0.00  1.00 -0.01  0.00  1.00 0.09 -0.63  0.53 0.00
MR1  0.92  0.89 0.82 0.99  0.71 -0.01  1.00  0.71  0.01 0.99  0.70  0.37 1.00
MR3  0.93  0.42 0.41 0.61  1.00  0.00  0.71  1.00  0.00 0.62  0.73  0.80 0.71
MR2 -0.01 -0.34 0.54 0.01  0.00  1.00  0.01  0.00  1.00 0.11 -0.63  0.52 0.01
MR1  0.87  0.88 0.88 1.00  0.62  0.09  0.99  0.62  0.11 1.00  0.59  0.33 0.99
MR2  0.78  0.75 0.16 0.65  0.73 -0.63  0.70  0.73 -0.63 0.59  1.00  0.18 0.70
MR3  0.63 -0.08 0.40 0.27  0.80  0.53  0.37  0.80  0.52 0.33  0.18  1.00 0.37
MR1  0.92  0.89 0.82 0.99  0.71  0.00  1.00  0.71  0.01 0.99  0.70  0.37 1.00
MR2 -0.01 -0.36 0.52 0.00  0.02  1.00  0.00  0.02  1.00 0.09 -0.62  0.55 0.00
MR3 -0.38  0.28 0.25 0.13 -0.70  0.02  0.00 -0.70  0.03 0.12 -0.35 -0.75 0.00
      MR2   MR3
MR1 -0.01 -0.38
MR3 -0.36  0.28
MR2  0.52  0.25
MR1  0.00  0.13
MR3  0.02 -0.70
MR2  1.00  0.02
MR1  0.00  0.00
MR3  0.02 -0.70
MR2  1.00  0.03
MR1  0.09  0.12
MR2 -0.62 -0.35
MR3  0.55 -0.75
MR1  0.00  0.00
MR2  1.00  0.00
MR3  0.00  1.00
> # compare an oblimin with a target rotation using the Browne algorithm
>  #note that we are changing the factor #order (this is for demonstration only)
>  Targ <- make.keys(9,list(f1=1:3,f2=7:9,f3=4:6)) 
>  Targ <- scrub(Targ,isvalue=1)  #fix the 0s, allow the NAs to be estimated
>  Targ <- list(Targ)  #input must be a list
> #show the target
>  Targ
[[1]]
      f1 f2 f3
 [1,] NA  0  0
 [2,] NA  0  0
 [3,] NA  0  0
 [4,]  0  0 NA
 [5,]  0  0 NA
 [6,]  0  0 NA
 [7,]  0 NA  0
 [8,]  0 NA  0
 [9,]  0 NA  0

>  fa(Thurstone,3,rotate="TargetQ",Target=Targ)  #targeted rotation
Factor Analysis using method =  minres
Call: fa(r = Thurstone, nfactors = 3, rotate = "TargetQ", Target = Targ)
Standardized loadings (pattern matrix) based upon correlation matrix
                    MR1   MR3   MR2   h2   u2 com
Sentences          0.86 -0.01  0.08 0.82 0.18 1.0
Vocabulary         0.85  0.10  0.00 0.84 0.16 1.0
Sent.Completion    0.80  0.06  0.04 0.74 0.26 1.0
First.Letters     -0.01  0.86  0.00 0.73 0.27 1.0
Four.Letter.Words -0.04  0.75  0.11 0.63 0.37 1.0
Suffixes           0.17  0.64 -0.08 0.50 0.50 1.2
Letter.Series     -0.05 -0.07  0.92 0.73 0.27 1.0
Pedigrees          0.32 -0.07  0.52 0.51 0.49 1.7
Letter.Group      -0.12  0.17  0.68 0.52 0.48 1.2

                       MR1  MR3  MR2
SS loadings           2.41 1.88 1.71
Proportion Var        0.27 0.21 0.19
Cumulative Var        0.27 0.48 0.67
Proportion Explained  0.40 0.31 0.28
Cumulative Proportion 0.40 0.72 1.00

 With factor correlations of 
     MR1  MR3  MR2
MR1 1.00 0.57 0.58
MR3 0.57 1.00 0.59
MR2 0.58 0.59 1.00

Mean item complexity =  1.1
Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  5.2
The degrees of freedom for the model are 12  and the objective function was  0.01 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.01 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR3  MR2
Correlation of scores with factors             0.96 0.92 0.92
Multiple R square of scores with factors       0.92 0.86 0.84
Minimum correlation of possible factor scores  0.84 0.71 0.69
> #compare with oblimin
> fa(Thurstone,3)
Factor Analysis using method =  minres
Call: fa(r = Thurstone, nfactors = 3)
Standardized loadings (pattern matrix) based upon correlation matrix
                    MR1   MR2   MR3   h2   u2 com
Sentences          0.90 -0.03  0.04 0.82 0.18 1.0
Vocabulary         0.89  0.06 -0.03 0.84 0.16 1.0
Sent.Completion    0.84  0.03  0.00 0.74 0.26 1.0
First.Letters      0.00  0.85  0.00 0.73 0.27 1.0
Four.Letter.Words -0.02  0.75  0.10 0.63 0.37 1.0
Suffixes           0.18  0.63 -0.08 0.50 0.50 1.2
Letter.Series      0.03 -0.01  0.84 0.73 0.27 1.0
Pedigrees          0.38 -0.05  0.46 0.51 0.49 2.0
Letter.Group      -0.06  0.21  0.63 0.52 0.48 1.2

                       MR1  MR2  MR3
SS loadings           2.65 1.87 1.49
Proportion Var        0.29 0.21 0.17
Cumulative Var        0.29 0.50 0.67
Proportion Explained  0.44 0.31 0.25
Cumulative Proportion 0.44 0.75 1.00

 With factor correlations of 
     MR1  MR2  MR3
MR1 1.00 0.59 0.53
MR2 0.59 1.00 0.52
MR3 0.53 0.52 1.00

Mean item complexity =  1.2
Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  5.2
The degrees of freedom for the model are 12  and the objective function was  0.01 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.01 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR2  MR3
Correlation of scores with factors             0.96 0.92 0.90
Multiple R square of scores with factors       0.93 0.85 0.82
Minimum correlation of possible factor scores  0.86 0.71 0.63
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("SD")
> ### * SD
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: SD
> ### Title: Find the Standard deviation for a vector, matrix, or data.frame
> ###   - do not return error if there are no cases
> ### Aliases: SD
> ### Keywords: models
> 
> ### ** Examples
> 
> data(attitude)
> apply(attitude,2,sd) #all complete
    rating complaints privileges   learning     raises   critical    advance 
 12.172562  13.314757  12.235430  11.737013  10.397226   9.894908  10.288706 
> attitude[,1] <- NA
> SD(attitude) #missing a column
    rating complaints privileges   learning     raises   critical    advance 
        NA  13.314757  12.235430  11.737013  10.397226   9.894908  10.288706 
> describe(attitude)
Warning in FUN(newX[, i], ...) :
  no non-missing arguments to min; returning Inf
Warning in FUN(newX[, i], ...) :
  no non-missing arguments to max; returning -Inf
           vars  n  mean    sd median trimmed   mad min  max range  skew
rating*       1  0   NaN    NA     NA     NaN    NA Inf -Inf  -Inf    NA
complaints    2 30 66.60 13.31   65.0   67.08 14.83  37   90    53 -0.22
privileges    3 30 53.13 12.24   51.5   52.75 10.38  30   83    53  0.38
learning      4 30 56.37 11.74   56.5   56.58 14.83  34   75    41 -0.05
raises        5 30 64.63 10.40   63.5   64.50 11.12  43   88    45  0.20
critical      6 30 74.77  9.89   77.5   75.83  7.41  49   92    43 -0.87
advance       7 30 42.93 10.29   41.0   41.83  8.90  25   72    47  0.85
           kurtosis   se
rating*          NA   NA
complaints    -0.68 2.43
privileges    -0.41 2.23
learning      -1.22 2.14
raises        -0.60 1.90
critical       0.17 1.81
advance        0.47 1.88
> 
> 
> 
> cleanEx()
> nameEx("Schmid.Leiman")
> ### * Schmid.Leiman
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Schmid
> ### Title: 12 variables created by Schmid and Leiman to show the
> ###   Schmid-Leiman Transformation
> ### Aliases: Schmid schmid.leiman West Chen
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Schmid)
> cor.plot(Schmid,TRUE)
> print(fa(Schmid,6,rotate="oblimin"),cut=0)  #shows an oblique solution
Factor Analysis using method =  minres
Call: fa(r = Schmid, nfactors = 6, rotate = "oblimin")
Standardized loadings (pattern matrix) based upon correlation matrix
      MR1  MR4   MR6   MR2   MR3   MR5    h2   u2 com
V1   0.80 0.00  0.00  0.00  0.00  0.00 0.637 0.36   1
V2   0.90 0.00  0.00  0.00  0.00  0.00 0.814 0.19   1
V3   0.00 0.00  0.70  0.00  0.00  0.00 0.485 0.52   1
V4   0.00 0.00  0.61  0.00  0.00  0.00 0.364 0.64   1
V5   0.00 0.00  0.00  0.00  0.77  0.00 0.599 0.40   1
V6   0.00 0.00  0.00  0.00  0.42 -0.01 0.171 0.83   1
V7   0.00 0.00  0.00  0.00  0.00  0.77 0.589 0.41   1
V8   0.00 0.00  0.00  0.00  0.01  0.18 0.033 0.97   1
V9   0.02 0.78  0.01  0.02  0.00  0.01 0.635 0.36   1
V10 -0.04 0.60 -0.02 -0.03 -0.01 -0.01 0.326 0.67   1
V11  0.00 0.00  0.00  0.60  0.00  0.00 0.363 0.64   1
V12  0.00 0.00  0.00  0.70  0.00  0.00 0.486 0.51   1

                       MR1  MR4  MR6  MR2  MR3  MR5
SS loadings           1.45 0.96 0.85 0.85 0.77 0.62
Proportion Var        0.12 0.08 0.07 0.07 0.06 0.05
Cumulative Var        0.12 0.20 0.27 0.34 0.41 0.46
Proportion Explained  0.26 0.17 0.15 0.15 0.14 0.11
Cumulative Proportion 0.26 0.44 0.59 0.75 0.89 1.00

 With factor correlations of 
     MR1  MR4  MR6  MR2  MR3  MR5
MR1 1.00 0.43 0.56 0.14 0.16 0.21
MR4 0.43 1.00 0.38 0.29 0.17 0.22
MR6 0.56 0.38 1.00 0.12 0.14 0.19
MR2 0.14 0.29 0.12 1.00 0.05 0.07
MR3 0.16 0.17 0.14 0.05 1.00 0.23
MR5 0.21 0.22 0.19 0.07 0.23 1.00

Mean item complexity =  1
Test of the hypothesis that 6 factors are sufficient.

The degrees of freedom for the null model are  66  and the objective function was  1.9
The degrees of freedom for the model are 9  and the objective function was  0 

The root mean square of the residuals (RMSR) is  0 
The df corrected root mean square of the residuals is  0 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR4  MR6  MR2  MR3  MR5
Correlation of scores with factors             0.93 0.84 0.81 0.78 0.80 0.78
Multiple R square of scores with factors       0.87 0.71 0.66 0.61 0.64 0.61
Minimum correlation of possible factor scores  0.73 0.43 0.32 0.23 0.27 0.22
> round(cov2cor(schmid.leiman),2)
      V1   V2   V3   V4   V5   V6   V7   V8   V9  V10  V11  V12
V1  1.00 1.00 0.56 0.56 0.15 0.15 0.23 0.23 0.40 0.40 0.13 0.13
V2  1.00 1.00 0.56 0.56 0.15 0.15 0.23 0.23 0.40 0.40 0.13 0.13
V3  0.56 0.56 1.00 1.00 0.13 0.13 0.20 0.20 0.35 0.35 0.12 0.12
V4  0.56 0.56 1.00 1.00 0.13 0.13 0.20 0.20 0.35 0.35 0.12 0.12
V5  0.15 0.15 0.13 0.13 1.00 1.00 0.24 0.24 0.15 0.15 0.05 0.05
V6  0.15 0.15 0.13 0.13 1.00 1.00 0.24 0.24 0.15 0.15 0.05 0.05
V7  0.23 0.23 0.20 0.20 0.24 0.24 1.00 1.00 0.23 0.23 0.08 0.08
V8  0.23 0.23 0.20 0.20 0.24 0.24 1.00 1.00 0.23 0.23 0.08 0.08
V9  0.40 0.40 0.35 0.35 0.15 0.15 0.23 0.23 1.00 1.00 0.27 0.27
V10 0.40 0.40 0.35 0.35 0.15 0.15 0.23 0.23 1.00 1.00 0.27 0.27
V11 0.13 0.13 0.12 0.12 0.05 0.05 0.08 0.08 0.27 0.27 1.00 1.00
V12 0.13 0.13 0.12 0.12 0.05 0.05 0.08 0.08 0.27 0.27 1.00 1.00
> cor.plot(cov2cor(West),TRUE)
> 
> 
> 
> cleanEx()
> nameEx("Schutz")
> ### * Schutz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Schutz
> ### Title: The Schutz correlation matrix example from Shapiro and ten Berge
> ### Aliases: Schutz
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Schutz)
> corPlot(Schutz,numbers=TRUE,upper=FALSE)
> #f4min <- fa(Schutz,4,fm="minrank")  #for an example of minimum rank factor Analysis
> #compare to
> #f4 <- fa(Schutz,4,fm="mle")  #for the maximum likelihood solution which has a Heywood case 
> 
> 
> 
> cleanEx()
> nameEx("Tucker")
> ### * Tucker
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Tucker
> ### Title: 9 Cognitive variables discussed by Tucker and Lewis (1973)
> ### Aliases: Tucker
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Tucker)
> fa(Tucker,2,n.obs=710)
Factor Analysis using method =  minres
Call: fa(r = Tucker, nfactors = 2, n.obs = 710)
Standardized loadings (pattern matrix) based upon correlation matrix
      MR1   MR2   h2   u2 com
t42 -0.04  0.71 0.48 0.52 1.0
t54  0.02  0.71 0.51 0.49 1.0
t45  0.92 -0.04 0.82 0.18 1.0
t46  0.87 -0.06 0.71 0.29 1.0
t23 -0.02  0.71 0.50 0.50 1.0
t24  0.00  0.74 0.55 0.45 1.0
t27  0.11  0.59 0.42 0.58 1.1
t10  0.76  0.13 0.68 0.32 1.1
t51  0.78  0.05 0.65 0.35 1.0

                       MR1  MR2
SS loadings           2.84 2.47
Proportion Var        0.32 0.27
Cumulative Var        0.32 0.59
Proportion Explained  0.54 0.46
Cumulative Proportion 0.54 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.43
MR2 0.43 1.00

Mean item complexity =  1
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  4.49 with Chi Square of  3165.93
The degrees of freedom for the model are 19  and the objective function was  0.07 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

The harmonic number of observations is  710 with the empirical chi square  22.23  with prob <  0.27 
The total number of observations was  710  with Likelihood Chi Square =  50.45  with prob <  0.00011 

Tucker Lewis Index of factoring reliability =  0.981
RMSEA index =  0.049  and the 90 % confidence intervals are  0.032 0.065
BIC =  -74.29
Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR2
Correlation of scores with factors             0.96 0.91
Multiple R square of scores with factors       0.92 0.84
Minimum correlation of possible factor scores  0.83 0.67
> omega(Tucker,2)

Three factors are required for identification -- general factor loadings set to be equal. 
Proceed with caution. 
Think about redoing the analysis with alternative values of the 'option' setting.

Omega 
Call: omega(m = Tucker, nfactors = 2)
Alpha:                 0.86 
G.6:                   0.88 
Omega Hierarchical:    0.54 
Omega H asymptotic:    0.6 
Omega Total            0.9 

Schmid Leiman Factor loadings greater than  0.2 
       g   F1*   F2*   h2   u2   p2
t42 0.44        0.53 0.48 0.52 0.40
t54 0.48        0.54 0.51 0.49 0.44
t45 0.58  0.69       0.82 0.18 0.41
t46 0.53  0.65       0.71 0.29 0.39
t23 0.46        0.54 0.50 0.50 0.42
t24 0.48        0.56 0.55 0.45 0.43
t27 0.46        0.45 0.42 0.58 0.51
t10 0.58  0.57       0.68 0.32 0.50
t51 0.55  0.59       0.65 0.35 0.46

With eigenvalues of:
  g F1* F2* 
2.3 1.6 1.4 

general/max  1.46   max/min =   1.15
mean percent general =  0.44    with sd =  0.04 and cv of  0.1 
Explained Common Variance of the general factor =  0.44 

The degrees of freedom are 19  and the fit is  0.07 

The root mean square of the residuals is  0.02 
The df corrected root mean square of the residuals is  0.03

Compare this with the adequacy of just a general factor and no group factors
The degrees of freedom for just the general factor are 27  and the fit is  1.94 

The root mean square of the residuals is  0.22 
The df corrected root mean square of the residuals is  0.25 

Measures of factor score adequacy             
                                                 g  F1*  F2*
Correlation of scores with factors            0.74 0.78 0.74
Multiple R square of scores with factors      0.55 0.60 0.55
Minimum correlation of factor score estimates 0.10 0.21 0.10

 Total, General and Subset omega for each subset
                                                 g  F1*  F2*
Omega total for total scores and subscales    0.90 0.91 0.83
Omega general for total scores and subscales  0.54 0.40 0.37
Omega group for total scores and subscales    0.34 0.51 0.46
> 
> 
> 
> cleanEx()
> nameEx("VSS")
> ### * VSS
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: VSS
> ### Title: Apply the Very Simple Structure, MAP, and other criteria to
> ###   determine the appropriate number of factors.
> ### Aliases: vss VSS MAP nfactors
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> #test.data <- Harman74.cor$cov
> #my.vss <- VSS(test.data,title="VSS of 24 mental tests")      
> #print(my.vss[,1:12],digits =2) 
> #VSS.plot(my.vss, title="VSS of 24 mental tests")
> 
> #now, some simulated data with two factors
> #VSS(sim.circ(nvar=24),fm="minres" ,title="VSS of 24 circumplex variables")
> VSS(sim.item(nvar=24),fm="minres" ,title="VSS of 24 simple structure variables")

Very Simple Structure of  VSS of 24 simple structure variables 
Call: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, 
    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor)
VSS complexity 1 achieves a maximimum of 0.84  with  4  factors
VSS complexity 2 achieves a maximimum of 0.87  with  7  factors

The Velicer MAP achieves a minimum of 0.01  with  2  factors 
BIC achieves a minimum of  -1156.52  with  2  factors
Sample Size adjusted BIC achieves a minimum of  -429.66  with  2  factors

Statistics by number of factors 
  vss1 vss2    map dof chisq     prob sqresid  fit  RMSEA   BIC SABIC complex
1 0.48 0.00 0.0480 252  1949 1.3e-259    32.1 0.48 0.1175   383  1183     1.0
2 0.84 0.84 0.0062 229   267  4.4e-02     9.8 0.84 0.0194 -1157  -430     1.0
3 0.84 0.85 0.0085 207   224  2.0e-01     9.3 0.85 0.0146 -1062  -405     1.1
4 0.84 0.85 0.0109 186   189  4.2e-01     8.8 0.86 0.0092  -967  -376     1.1
5 0.79 0.86 0.0138 166   157  6.8e-01     8.3 0.87 0.0000  -875  -348     1.2
6 0.79 0.86 0.0170 147   132  8.1e-01     7.9 0.87 0.0000  -782  -315     1.3
7 0.74 0.87 0.0204 129   105  9.4e-01     7.5 0.88 0.0000  -696  -287     1.3
8 0.78 0.87 0.0242 112    90  9.4e-01     7.3 0.88 0.0000  -606  -250     1.4
  eChisq  SRMR eCRMS  eBIC
1   8179 0.172 0.180  6613
2    205 0.027 0.030 -1218
3    165 0.024 0.028 -1121
4    133 0.022 0.027 -1023
5    109 0.020 0.026  -923
6     89 0.018 0.025  -825
7     71 0.016 0.023  -731
8     58 0.014 0.023  -638
> 
> 
> 
> cleanEx()
> nameEx("VSS.parallel")
> ### * VSS.parallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: VSS.parallel
> ### Title: Compare real and random VSS solutions
> ### Aliases: VSS.parallel
> ### Keywords: models models
> 
> ### ** Examples
> 
> #VSS.plot(VSS.parallel(200,24))
> 
> 
> 
> cleanEx()
> nameEx("VSS.plot")
> ### * VSS.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: VSS.plot
> ### Title: Plot VSS fits
> ### Aliases: VSS.plot
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> test.data <- Harman74.cor$cov
> my.vss <- VSS(test.data)         #suggests that 4 factor complexity two solution is optimal
n.obs was not specified and was arbitrarily set to 1000.  This only affects the chi square values.
> VSS.plot(my.vss,title="VSS of Holzinger-Harmon problem")                 #see the graphics window
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("VSS.scree")
> ### * VSS.scree
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: VSS.scree
> ### Title: Plot the successive eigen values for a scree test
> ### Aliases: VSS.scree scree
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> scree(attitude)
> #VSS.scree(cor(attitude)
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("Yule")
> ### * Yule
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Yule
> ### Title: From a two by two table, find the Yule coefficients of
> ###   association, convert to phi, or tetrachoric, recreate table the table
> ###   to create the Yule coefficient.
> ### Aliases: Yule Yule.inv Yule2phi Yule2tetra Yule2poly YuleBonett YuleCor
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> Nach <- matrix(c(40,10,20,50),ncol=2,byrow=TRUE)
> Yule(Nach)
[1] 0.8181818
> Yule.inv(.81818,c(50,60),n=120)
         [,1]     [,2]
[1,] 39.99994 10.00006
[2,] 20.00006 49.99994
> Yule2phi(.81818,c(50,60),n=120)
[1] 0.5070905
> Yule2tetra(.81818,c(50,60),n=120)
[1] 0.7230638
> phi(Nach)  #much less
[1] 0.51
> #or express as percents and do not specify n
> Nach <- matrix(c(40,10,20,50),ncol=2,byrow=TRUE)
> Nach/120
          [,1]       [,2]
[1,] 0.3333333 0.08333333
[2,] 0.1666667 0.41666667
> Yule(Nach)
[1] 0.8181818
> Yule.inv(.81818,c(.41667,.5))
          [,1]       [,2]
[1,] 0.3333391 0.08333086
[2,] 0.1666609 0.41666914
> Yule2phi(.81818,c(.41667,.5))
[1] 0.5071088
> Yule2tetra(.81818,c(.41667,.5))
[1] 0.7230836
> phi(Nach)  #much less
[1] 0.51
> YuleCor(ability[,1:4],,TRUE)
Yule and Generalized Yule coefficients
Call: YuleCor(x = ability[, 1:4], bonett = TRUE)

Yule coefficient 
          reason.4 reason.16 reason.17 reason.19
reason.4      1.00      0.28      0.40      0.30
reason.16     0.28      1.00      0.32      0.25
reason.17     0.40      0.32      1.00      0.33
reason.19     0.30      0.25      0.33      1.00

Upper and Lower Confidence Intervals = 
          reason.4 reason.16 reason.17 reason.19
reason.4      1.00      0.33      0.44      0.35
reason.16     0.23      1.00      0.37      0.30
reason.17     0.35      0.27      1.00      0.38
reason.19     0.25      0.20      0.28      1.00
> YuleBonett(Nach,1)  #Yule Q
Yule and Generalized Yule coefficients
Lower CI  Yule coefficient Upper CI 
[1] 0.61 0.82 0.92
> YuleBonett(Nach,.5)  #Yule Y
Yule and Generalized Yule coefficients
Lower CI  Yule coefficient Upper CI 
[1] 0.34 0.52 0.66
> YuleBonett(Nach,.75)  #Digby H
Yule and Generalized Yule coefficients
Lower CI  Yule coefficient Upper CI 
[1] 0.49 0.70 0.83
> YuleBonett(Nach,,TRUE)  #Yule* is a generalized Yule
Yule and Generalized Yule coefficients
Lower CI  Yule coefficient Upper CI 
[1] 0.34 0.51 0.65
> 
> 
> 
> 
> cleanEx()
> nameEx("ability")
> ### * ability
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ability
> ### Title: 16 ability items scored as correct or incorrect.
> ### Aliases: ability
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(ability)
> #not run
> # ability.irt <- irt.fa(ability)
> # ability.scores <- score.irt(ability.irt,ability)
> 
> 
> 
> cleanEx()
> nameEx("affect")
> ### * affect
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: affect
> ### Title: Two data sets of affect and arousal scores as a function of
> ###   personality and movie conditions
> ### Aliases: affect maps flat
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(affect)
> describeBy(affect[-1],group="Film")

 Descriptive statistics by group 
group: 1
         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis
Film        1 83  1.00  0.00    1.0    1.00  0.00   1   1     0   NaN      NaN
ext         2 83 13.40  4.35   13.0   13.43  4.45   2  22    20 -0.06    -0.58
neur        3 83 10.21  4.90   11.0   10.17  5.93   1  22    21  0.06    -0.76
imp         4 83  4.34  1.97    4.0    4.30  1.48   0   9     9  0.10    -0.54
soc         5 83  7.64  2.77    8.0    7.78  2.97   0  13    13 -0.42    -0.53
lie         6 83  2.36  1.50    2.0    2.30  1.48   0   7     7  0.42    -0.14
traitanx    7 83 39.91  9.60   39.0   39.55 10.38  23  61    38  0.31    -0.81
state1      8 83 39.81 10.75   38.0   39.22 10.38  20  69    49  0.56    -0.17
EA1         9 83  9.27  7.21    8.0    8.70  7.41   0  29    29  0.59    -0.48
TA1        10 83 12.49  4.80   12.9   12.35  4.30   0  26    26  0.37     0.74
PA1        11 83  9.46  7.28    7.5    8.77  5.19   0  29    29  0.80    -0.14
NA1        12 83  3.50  3.73    2.0    2.94  2.97   0  17    17  1.41     1.90
EA2        13 83 10.02  6.09    9.0    9.72  7.41   0  23    23  0.30    -0.60
TA2        14 83 17.48  4.74   18.0   17.55  4.45   8  27    19 -0.20    -0.77
PA2        15 83  7.27  5.93    6.0    6.63  5.93   0  23    23  0.78    -0.25
NA2        16 83  8.67  5.35    8.0    8.21  5.93   0  22    22  0.60    -0.43
state2     17 41 48.66 11.00   49.0   49.21 10.38  20  71    51 -0.45     0.01
MEQ        18 41 38.39 11.49   40.0   38.00 13.34  16  65    49  0.26    -0.40
BDI        19 42 10.69  6.73   11.0   10.59  7.41   0  23    23  0.05    -1.25
           se
Film     0.00
ext      0.48
neur     0.54
imp      0.22
soc      0.30
lie      0.16
traitanx 1.05
state1   1.18
EA1      0.79
TA1      0.53
PA1      0.80
NA1      0.41
EA2      0.67
TA2      0.52
PA2      0.65
NA2      0.59
state2   1.72
MEQ      1.79
BDI      1.04
------------------------------------------------------------ 
group: 2
         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis
Film        1 78  2.00  0.00    2.0    2.00  0.00   2   2     0   NaN      NaN
ext         2 78 12.38  4.55   12.0   12.47  2.97   1  21    20 -0.21    -0.37
neur        3 78  9.88  4.61   10.0    9.80  4.45   0  21    21  0.17    -0.35
imp         4 78  3.88  2.05    4.0    3.86  1.48   0   8     8  0.15    -0.78
soc         5 78  7.15  2.71    7.5    7.31  2.22   1  12    11 -0.49    -0.42
lie         6 78  2.15  1.47    2.0    2.02  1.48   0   7     7  0.97     1.12
traitanx    7 78 39.87 10.14   38.0   39.33 10.38  22  71    49  0.69     0.46
state1      8 78 41.44 10.41   41.0   41.03 10.38  22  79    57  0.67     1.15
EA1         9 78  8.40  6.54    7.5    7.97  8.15   0  26    26  0.45    -0.65
TA1        10 78 12.62  3.84   13.0   12.51  2.97   5  25    20  0.45     0.65
PA1        11 78  7.48  5.89    7.0    6.95  5.93   0  22    22  0.68    -0.31
NA1        12 78  3.36  4.45    2.0    2.57  2.97   0  28    28  2.73    10.66
EA2        13 78 11.65  6.66   11.0   11.51  5.93   0  28    28  0.17    -0.62
TA2        14 78 18.33  5.15   19.0   18.34  4.45   6  31    25 -0.01    -0.15
PA2        15 78  8.08  6.18    7.0    7.56  7.41   0  26    26  0.74     0.15
NA2        16 78  5.37  5.18    4.0    4.66  4.45   0  30    30  1.80     5.06
state2     17 41 46.55  9.63   46.0   46.63  8.90  25  67    42 -0.01    -0.21
MEQ        18 41 39.63 10.04   39.0   39.58 10.38  16  58    42 -0.01    -0.76
BDI        19 37 10.65  7.19   11.0   10.48  8.90   0  26    26  0.11    -1.05
           se
Film     0.00
ext      0.52
neur     0.52
imp      0.23
soc      0.31
lie      0.17
traitanx 1.15
state1   1.18
EA1      0.74
TA1      0.44
PA1      0.67
NA1      0.50
EA2      0.75
TA2      0.58
PA2      0.70
NA2      0.59
state2   1.50
MEQ      1.57
BDI      1.18
------------------------------------------------------------ 
group: 3
         vars  n  mean    sd median trimmed   mad min  max range  skew kurtosis
Film        1 85  3.00  0.00    3.0    3.00  0.00   3  3.0   0.0   NaN      NaN
ext         2 85 12.78  4.60   13.0   13.12  4.45   1 20.0  19.0 -0.64    -0.12
neur        3 85 10.61  5.14   10.0   10.30  4.45   0 23.0  23.0  0.46    -0.20
imp         4 85  4.12  1.98    4.0    4.07  1.48   0  8.0   8.0  0.23    -0.87
soc         5 85  7.31  3.15    8.0    7.60  2.97   0 12.0  12.0 -0.75    -0.16
lie         6 85  2.34  1.41    2.0    2.28  1.48   0  6.0   6.0  0.42    -0.35
traitanx    7 85 39.42  9.62   39.0   38.70  7.41  23 71.0  48.0  0.74     0.51
state1      8 85 40.72 11.25   40.0   39.89 10.38  22 76.0  54.0  0.67     0.33
EA1         9 85  9.42  7.24    8.0    8.87  8.90   0 29.0  29.0  0.55    -0.59
TA1        10 85 13.16  4.60   13.0   13.00  4.45   3 24.5  21.5  0.25    -0.19
PA1        11 85  8.95  6.91    8.0    8.29  5.93   0 28.0  28.0  0.77     0.00
NA1        12 85  3.92  4.66    2.0    3.12  2.97   0 23.0  23.0  1.93     4.61
EA2        13 85  9.07  6.90    7.0    8.66  8.90   0 27.0  27.0  0.46    -0.87
TA2        14 85 13.30  4.26   14.0   13.20  4.45   3 25.0  22.0  0.20    -0.08
PA2        15 85  8.75  6.45    8.0    8.23  7.41   0 26.0  26.0  0.57    -0.42
NA2        16 85  2.89  4.13    1.0    2.01  1.48   0 18.0  18.0  1.77     2.44
state2     17 42 39.58  9.24   40.0   39.89  9.64  20 57.0  37.0 -0.20    -0.83
MEQ        18 42 38.64 10.20   38.5   38.82  8.15  15 59.0  44.0 -0.20    -0.27
BDI        19 43 10.81  7.99    9.0   10.37 10.38   0 29.0  29.0  0.40    -0.93
           se
Film     0.00
ext      0.50
neur     0.56
imp      0.22
soc      0.34
lie      0.15
traitanx 1.04
state1   1.22
EA1      0.78
TA1      0.50
PA1      0.75
NA1      0.50
EA2      0.75
TA2      0.46
PA2      0.70
NA2      0.45
state2   1.43
MEQ      1.57
BDI      1.22
------------------------------------------------------------ 
group: 4
         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis
Film        1 84  4.00  0.00    4.0    4.00  0.00   4   4     0   NaN      NaN
ext         2 84 14.01  4.27   14.5   14.28  3.71   0  22    22 -0.64     0.28
neur        3 84 10.13  5.51   10.0   10.11  5.93   0  23    23  0.04    -0.81
imp         4 84  4.81  1.81    5.0    4.93  1.48   0   8     8 -0.54     0.01
soc         5 84  7.91  2.78    9.0    8.17  2.97   0  13    13 -0.77     0.11
lie         6 84  2.31  1.57    2.0    2.19  1.48   0   7     7  0.86     0.67
traitanx    7 84 38.89  9.87   37.0   38.07 10.38  23  68    45  0.74     0.20
state1      8 84 41.39 10.71   40.0   40.99 11.12  21  66    45  0.38    -0.73
EA1         9 84  9.62  7.47    8.0    9.22  8.90   0  26    26  0.38    -1.14
TA1        10 84 13.35  4.37   13.0   13.21  4.45   4  24    20  0.38    -0.07
PA1        11 84  9.56  6.88    9.0    9.15  8.15   0  27    27  0.36    -0.70
NA1        12 84  3.96  4.40    2.0    3.18  2.97   0  18    18  1.45     1.60
EA2        13 84 13.35  7.02   14.0   13.46  7.41   0  29    29 -0.13    -0.81
TA2        14 84 13.74  3.14   14.0   13.89  2.97   6  20    14 -0.45    -0.28
PA2        15 84 11.75  6.31   12.0   11.65  5.93   0  29    29  0.11    -0.38
NA2        16 84  1.79  2.88    0.0    1.16  0.00   0  12    12  1.81     2.54
state2     17 46 35.89  8.05   36.0   35.71  7.41  20  54    34  0.22    -0.67
MEQ        18 46 40.63  9.59   41.5   40.97  9.64  20  59    39 -0.32    -0.53
BDI        19 38 10.55  7.25   10.0   10.28  8.90   0  26    26  0.33    -0.98
           se
Film     0.00
ext      0.47
neur     0.60
imp      0.20
soc      0.30
lie      0.17
traitanx 1.08
state1   1.17
EA1      0.81
TA1      0.48
PA1      0.75
NA1      0.48
EA2      0.77
TA2      0.34
PA2      0.69
NA2      0.31
state2   1.19
MEQ      1.41
BDI      1.18
> pairs.panels(affect[14:17],bg=c("red","black","white","blue")[affect$Film],pch=21,
+     main="Affect varies by movies ")
> errorCircles("EA2","TA2",data=affect,group="Film",labels=c("Sad","Fear","Neutral","Humor")
+ , main="Enegetic and Tense Arousal by Movie condition")
Warning in cor(data, use = use, method = method) :
  the standard deviation is zero
Warning in sqrt(xvals$nw - 2) : NaNs produced
Warning in cor(new.data[, (nvar + 1):ncol(new.data)], diffs, use = "pairwise",  :
  the standard deviation is zero
> errorCircles(x="PA2",y="NA2",data=affect,group="Film",labels=c("Sad","Fear","Neutral","
+ Humor"),  main="Positive and Negative Affect by Movie condition")
Warning in cor(data, use = use, method = method) :
  the standard deviation is zero
Warning in sqrt(xvals$nw - 2) : NaNs produced
Warning in cor(new.data[, (nvar + 1):ncol(new.data)], diffs, use = "pairwise",  :
  the standard deviation is zero
> 
> 
> 
> 
> cleanEx()
> nameEx("alpha")
> ### * alpha
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: alpha
> ### Title: Find two estimates of reliability: Cronbach's alpha and
> ###   Guttman's Lambda 6.
> ### Aliases: alpha alpha.scale
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> set.seed(42) #keep the same starting values
> #four congeneric measures
> r4 <- sim.congeneric()
> alpha(r4)

Reliability analysis   
Call: alpha(x = r4)

  raw_alpha std.alpha G6(smc) average_r S/N
      0.74      0.74    0.69      0.42 2.9

 Reliability if an item is dropped:
   raw_alpha std.alpha G6(smc) average_r S/N
V1      0.62      0.62    0.53      0.36 1.7
V2      0.66      0.66    0.57      0.39 1.9
V3      0.70      0.70    0.62      0.44 2.3
V4      0.74      0.74    0.66      0.49 2.8

 Item statistics 
      r r.cor r.drop
V1 0.81  0.74   0.64
V2 0.78  0.67   0.57
V3 0.73  0.59   0.51
V4 0.68  0.50   0.43
> #nine hierarchical measures -- should actually use omega
> r9 <- sim.hierarchical()
> alpha(r9)

Reliability analysis   
Call: alpha(x = r9)

  raw_alpha std.alpha G6(smc) average_r S/N
      0.76      0.76    0.76      0.26 3.2

 Reliability if an item is dropped:
   raw_alpha std.alpha G6(smc) average_r S/N
V1      0.71      0.71    0.70      0.24 2.5
V2      0.72      0.72    0.71      0.25 2.6
V3      0.74      0.74    0.73      0.26 2.8
V4      0.73      0.73    0.72      0.25 2.7
V5      0.74      0.74    0.73      0.26 2.9
V6      0.75      0.75    0.74      0.27 3.0
V7      0.75      0.75    0.74      0.27 3.0
V8      0.76      0.76    0.75      0.28 3.1
V9      0.77      0.77    0.76      0.29 3.3

 Item statistics 
      r r.cor r.drop
V1 0.72  0.71   0.61
V2 0.67  0.63   0.54
V3 0.61  0.55   0.47
V4 0.65  0.59   0.51
V5 0.59  0.52   0.45
V6 0.53  0.43   0.38
V7 0.56  0.46   0.40
V8 0.50  0.39   0.34
V9 0.45  0.32   0.28
> 
> # examples of two independent factors that produce reasonable alphas
> #this is a case where alpha is a poor indicator of unidimensionality
> two.f <- sim.item(8)
> #specify which items to reverse key by name
>  alpha(two.f,keys=c("V1","V2","V7","V8"))

Reliability analysis   
Call: alpha(x = two.f, keys = c("V1", "V2", "V7", "V8"))

  raw_alpha std.alpha G6(smc) average_r S/N   ase  mean   sd
      0.58      0.58    0.62      0.15 1.4 0.029 0.031 0.51

 lower alpha upper     95% confidence boundaries
0.53 0.58 0.64 

 Reliability if an item is dropped:
    raw_alpha std.alpha G6(smc) average_r S/N alpha se
V1-      0.55      0.55    0.58      0.15 1.2    0.031
V2-      0.53      0.53    0.57      0.14 1.1    0.033
V3       0.55      0.55    0.58      0.15 1.2    0.031
V4       0.55      0.54    0.58      0.15 1.2    0.032
V5       0.56      0.56    0.59      0.15 1.3    0.031
V6       0.56      0.56    0.59      0.15 1.3    0.031
V7-      0.53      0.53    0.56      0.14 1.1    0.033
V8-      0.56      0.57    0.59      0.16 1.3    0.030

 Item statistics 
      n raw.r std.r r.cor r.drop     mean   sd
V1- 500  0.50  0.50  0.38   0.28  0.09148 1.01
V2- 500  0.55  0.55  0.46   0.34  0.10501 1.00
V3  500  0.50  0.50  0.39   0.27 -0.04114 1.05
V4  500  0.51  0.51  0.41   0.30 -0.04698 0.99
V5  500  0.48  0.48  0.36   0.26  0.00021 1.01
V6  500  0.48  0.48  0.36   0.26 -0.00965 1.00
V7- 500  0.56  0.56  0.48   0.35  0.08102 1.00
V8- 500  0.45  0.46  0.33   0.23  0.07120 0.96
>  #by location
>  alpha(two.f,keys=c(1,2,7,8))

Reliability analysis   
Call: alpha(x = two.f, keys = c(1, 2, 7, 8))

  raw_alpha std.alpha G6(smc) average_r S/N   ase  mean   sd
      0.58      0.58    0.62      0.15 1.4 0.029 0.031 0.51

 lower alpha upper     95% confidence boundaries
0.53 0.58 0.64 

 Reliability if an item is dropped:
    raw_alpha std.alpha G6(smc) average_r S/N alpha se
V1-      0.55      0.55    0.58      0.15 1.2    0.031
V2-      0.53      0.53    0.57      0.14 1.1    0.033
V3       0.55      0.55    0.58      0.15 1.2    0.031
V4       0.55      0.54    0.58      0.15 1.2    0.032
V5       0.56      0.56    0.59      0.15 1.3    0.031
V6       0.56      0.56    0.59      0.15 1.3    0.031
V7-      0.53      0.53    0.56      0.14 1.1    0.033
V8-      0.56      0.57    0.59      0.16 1.3    0.030

 Item statistics 
      n raw.r std.r r.cor r.drop     mean   sd
V1- 500  0.50  0.50  0.38   0.28  0.09148 1.01
V2- 500  0.55  0.55  0.46   0.34  0.10501 1.00
V3  500  0.50  0.50  0.39   0.27 -0.04114 1.05
V4  500  0.51  0.51  0.41   0.30 -0.04698 0.99
V5  500  0.48  0.48  0.36   0.26  0.00021 1.01
V6  500  0.48  0.48  0.36   0.26 -0.00965 1.00
V7- 500  0.56  0.56  0.48   0.35  0.08102 1.00
V8- 500  0.45  0.46  0.33   0.23  0.07120 0.96
>  #automatic reversal base upon first component
> alpha(two.f)  
Warning in alpha(two.f) :
  Some items were negatively correlated with the total scale and probably 
should be reversed.  
To do this, run the function again with the 'check.keys=TRUE' option
Some items ( V3 V4 V5 V6 ) were negatively correlated with the total scale and 
probably should be reversed.  
To do this, run the function again with the 'check.keys=TRUE' optionWarning in sqrt(Vtc) : NaNs produced

Reliability analysis   
Call: alpha(x = two.f)

  raw_alpha std.alpha G6(smc) average_r   S/N  ase    mean   sd
     -0.58     -0.58   -0.17    -0.048 -0.37 0.11 -0.0042 0.29

 lower alpha upper     95% confidence boundaries
-0.8 -0.58 -0.37 

 Reliability if an item is dropped:
   raw_alpha std.alpha G6(smc) average_r   S/N alpha se
V1     -0.54     -0.53  -0.147    -0.052 -0.35    0.106
V2     -0.52     -0.52  -0.149    -0.051 -0.34    0.105
V3     -0.37     -0.36  -0.041    -0.039 -0.27    0.094
V4     -0.40     -0.37  -0.049    -0.040 -0.27    0.097
V5     -0.49     -0.48  -0.119    -0.049 -0.32    0.103
V6     -0.56     -0.56  -0.172    -0.054 -0.36    0.107
V7     -0.39     -0.40  -0.083    -0.043 -0.29    0.097
V8     -0.57     -0.57  -0.177    -0.055 -0.36    0.109

 Item statistics 
     n raw.r std.r r.cor r.drop     mean   sd
V1 500  0.33  0.33   NaN -0.112  0.01170 1.01
V2 500  0.32  0.32   NaN -0.123 -0.00183 1.00
V3 500  0.24  0.21   NaN -0.219 -0.04114 1.05
V4 500  0.22  0.22   NaN -0.204 -0.04698 0.99
V5 500  0.30  0.30   NaN -0.144  0.00021 1.01
V6 500  0.34  0.34   NaN -0.100 -0.00965 1.00
V7 500  0.23  0.24   NaN -0.205  0.02217 1.00
V8 500  0.33  0.35   NaN -0.091  0.03198 0.96
> #an example with discrete item responses  -- show the frequencies
> items <- sim.congeneric(N=500,short=FALSE,low=-2,high=2,
+         categorical=TRUE) #500 responses to 4 discrete items with 5 categories
> a4 <- alpha(items$observed)  #item response analysis of congeneric measures
> a4

Reliability analysis   
Call: alpha(x = items$observed)

  raw_alpha std.alpha G6(smc) average_r S/N  ase   mean   sd
      0.73      0.73    0.68       0.4 2.7 0.02 -0.013 0.76

 lower alpha upper     95% confidence boundaries
0.69 0.73 0.77 

 Reliability if an item is dropped:
   raw_alpha std.alpha G6(smc) average_r S/N alpha se
V1      0.61      0.61    0.52      0.34 1.6    0.031
V2      0.64      0.64    0.55      0.37 1.8    0.028
V3      0.68      0.68    0.60      0.41 2.1    0.025
V4      0.73      0.73    0.65      0.48 2.8    0.021

 Item statistics 
     n raw.r std.r r.cor r.drop   mean   sd
V1 500  0.80  0.80  0.73   0.62  0.050 1.00
V2 500  0.77  0.77  0.67   0.57 -0.022 1.03
V3 500  0.72  0.73  0.58   0.50 -0.028 0.99
V4 500  0.67  0.66  0.46   0.40 -0.050 1.05

Non missing response frequency for each item
     -2   -1    0    1    2 miss
V1 0.06 0.24 0.38 0.25 0.07    0
V2 0.07 0.26 0.35 0.25 0.07    0
V3 0.05 0.27 0.38 0.22 0.07    0
V4 0.10 0.22 0.39 0.22 0.07    0
> #summary just gives Alpha
> summary(a4)

Reliability analysis   
 raw_alpha std.alpha G6(smc) average_r S/N  ase   mean   sd
      0.73      0.73    0.68       0.4 2.7 0.02 -0.013 0.76
> 
> 
> 
> cleanEx()
> nameEx("best.scales")
> ### * best.scales
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bestScales
> ### Title: A set of functions for factorial and empirical scale
> ###   construction
> ### Aliases: bestItems bestScales lookup fa.lookup item.lookup keys.lookup
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> bs <- bestScales(bfi,criteria=c("gender","education","age"),dictionary=bfi.dictionary)
> bs
The items most correlated with the criteria yield r's of 
          correlation n.items
gender           0.32       9
education        0.14       1
age              0.24       9

The best items, their correlations and content  are 
$gender
  Row.names     gender ItemLabel                                      Item
1        N5  0.2106171    q_1505                             Panic easily.
2        A2  0.1820202    q_1162         Inquire about others' well-being.
3        A1 -0.1571373     q_146 Am indifferent to the feelings of others.
4        A3  0.1401320    q_1206               Know how to comfort others.
5        A4  0.1261747    q_1364                            Love children.
6        E1 -0.1261018     q_712                         Don't talk a lot.
7        N3  0.1207718    q_1099                Have frequent mood swings.
8        O1 -0.1031059     q_128                         Am full of ideas.
9        A5  0.1007091    q_1419                 Make people feel at ease.
      Giant3                Big6    Little12 Keying IPIP100
1  Stability Emotional Stability     Balance     -1    B5:N
2   Cohesion       Agreeableness  Compassion      1    B5:A
3   Cohesion       Agreeableness  Compassion     -1    B5:A
4   Cohesion       Agreeableness  Compassion      1    B5:A
5   Cohesion       Agreeableness  Compassion      1    B5:A
6 Plasticity        Extraversion Sociability     -1    B5:E
7  Stability Emotional Stability     Balance     -1    B5:N
8 Plasticity            Openness   Intellect      1    B5:O
9   Cohesion       Agreeableness  Compassion      1    B5:A

$education
  Row.names  education ItemLabel                                      Item
1        A1 -0.1415734     q_146 Am indifferent to the feelings of others.
    Giant3          Big6   Little12 Keying IPIP100
1 Cohesion Agreeableness Compassion     -1    B5:A

$age
  Row.names        age ItemLabel                                      Item
1        A1 -0.1609637     q_146 Am indifferent to the feelings of others.
2        C4 -0.1482659     q_626           Do things in a half-way manner.
3        A4  0.1442473    q_1364                            Love children.
4        A5  0.1290935    q_1419                 Make people feel at ease.
5        E5  0.1146922    q_1768                              Take charge.
6        A2  0.1142767    q_1162         Inquire about others' well-being.
7        N3 -0.1108648    q_1099                Have frequent mood swings.
8        E2 -0.1051612     q_901     Find it difficult to approach others.
9        N5 -0.1043322    q_1505                             Panic easily.
      Giant3                Big6        Little12 Keying IPIP100
1   Cohesion       Agreeableness      Compassion     -1    B5:A
2  Stability   Conscientiousness Industriousness     -1    B5:C
3   Cohesion       Agreeableness      Compassion      1    B5:A
4   Cohesion       Agreeableness      Compassion      1    B5:A
5 Plasticity        Extraversion   Assertiveness      1    B5:E
6   Cohesion       Agreeableness      Compassion      1    B5:A
7  Stability Emotional Stability         Balance     -1    B5:N
8 Plasticity        Extraversion     Sociability     -1    B5:E
9  Stability Emotional Stability         Balance     -1    B5:N

> f5 <- fa(bfi,5)
> m <- colMeans(bfi,na.rm=TRUE)
> item.lookup(f5,m,dictionary=bfi.dictionary[2])
      MR2   MR1   MR3   MR5   MR4 means  com   h2
N1   0.78  0.09  0.00 -0.12 -0.03  2.93 1.08 0.61
N5   0.53 -0.15  0.01  0.21 -0.17  2.97 1.70 0.37
N4   0.50 -0.36 -0.13  0.10  0.09  3.19 2.15 0.48
N3   0.73 -0.06 -0.03  0.06  0.02  3.22 1.03 0.56
N2   0.76  0.03  0.01 -0.09  0.02  3.51 1.03 0.58
E1  -0.03 -0.54  0.10 -0.07 -0.09  2.97 1.17 0.32
E2   0.15 -0.64 -0.02 -0.03 -0.06  3.14 1.13 0.51
N41  0.50 -0.36 -0.13  0.10  0.09  3.19 2.15 0.48
E3   0.08  0.49  0.00  0.16  0.29  4.00 1.93 0.45
E4   0.00  0.67  0.02  0.18 -0.07  4.42 1.17 0.55
E5   0.12  0.41  0.27  0.03  0.22  4.42 2.58 0.39
A5  -0.10  0.32  0.01  0.46  0.05  4.56 1.93 0.45
C4   0.18  0.06 -0.62 -0.01 -0.04  2.55 1.20 0.46
C5   0.20 -0.12 -0.56  0.01  0.10  3.30 1.43 0.43
C3   0.04 -0.05  0.56  0.08 -0.06  4.30 1.09 0.31
C2   0.16 -0.05  0.66  0.04  0.04  4.37 1.15 0.44
C1   0.07 -0.02  0.55 -0.04  0.15  4.50 1.21 0.33
A1   0.20  0.19  0.07 -0.51 -0.05  2.41 1.67 0.28
A51 -0.10  0.32  0.01  0.46  0.05  4.56 1.93 0.45
A3   0.00  0.23  0.03  0.58  0.03  4.60 1.31 0.49
A4  -0.04  0.13  0.20  0.40 -0.14  4.70 2.03 0.28
A2   0.01  0.07  0.08  0.63  0.03  4.80 1.07 0.47
O5   0.13  0.14 -0.03 -0.02 -0.52  2.49 1.28 0.29
O2   0.20  0.11 -0.08  0.10 -0.44  2.71 1.77 0.24
O3   0.03  0.17  0.02  0.05  0.62  4.44 1.18 0.47
O1   0.01  0.13  0.07 -0.03  0.52  4.82 1.17 0.33
O4   0.16 -0.28 -0.02  0.18  0.37  4.89 2.83 0.24
                                         Item
N1                          Get angry easily.
N5                              Panic easily.
N4                           Often feel blue.
N3                 Have frequent mood swings.
N2                      Get irritated easily.
E1                          Don't talk a lot.
E2      Find it difficult to approach others.
N41                          Often feel blue.
E3              Know how to captivate people.
E4                       Make friends easily.
E5                               Take charge.
A5                  Make people feel at ease.
C4            Do things in a half-way manner.
C5                             Waste my time.
C3             Do things according to a plan.
C2      Continue until everything is perfect.
C1                    Am exacting in my work.
A1  Am indifferent to the feelings of others.
A51                 Make people feel at ease.
A3                Know how to comfort others.
A4                             Love children.
A2          Inquire about others' well-being.
O5      Will not probe deeply into a subject.
O2          Avoid difficult reading material.
O3  Carry the conversation to a higher level.
O1                          Am full of ideas.
O4           Spend time reflecting on things.
> fa.lookup(f5,dictionary=bfi.dictionary[2])  #just show the item content, not the source of the items
            MR2   MR1   MR3   MR5   MR4  com   h2
N1         0.78  0.09  0.00 -0.12 -0.03 1.08 0.61
N2         0.76  0.03  0.01 -0.09  0.02 1.03 0.58
N3         0.73 -0.06 -0.03  0.06  0.02 1.03 0.56
N5         0.53 -0.15  0.01  0.21 -0.17 1.70 0.37
N4         0.50 -0.36 -0.13  0.10  0.09 2.15 0.48
E4         0.00  0.67  0.02  0.18 -0.07 1.17 0.55
E2         0.15 -0.64 -0.02 -0.03 -0.06 1.13 0.51
E1        -0.03 -0.54  0.10 -0.07 -0.09 1.17 0.32
E3         0.08  0.49  0.00  0.16  0.29 1.93 0.45
E5         0.12  0.41  0.27  0.03  0.22 2.58 0.39
C2         0.16 -0.05  0.66  0.04  0.04 1.15 0.44
C4         0.18  0.06 -0.62 -0.01 -0.04 1.20 0.46
C3         0.04 -0.05  0.56  0.08 -0.06 1.09 0.31
C5         0.20 -0.12 -0.56  0.01  0.10 1.43 0.43
C1         0.07 -0.02  0.55 -0.04  0.15 1.21 0.33
A2         0.01  0.07  0.08  0.63  0.03 1.07 0.47
A3         0.00  0.23  0.03  0.58  0.03 1.31 0.49
A1         0.20  0.19  0.07 -0.51 -0.05 1.67 0.28
A5        -0.10  0.32  0.01  0.46  0.05 1.93 0.45
A4        -0.04  0.13  0.20  0.40 -0.14 2.03 0.28
gender     0.14  0.00  0.11  0.29 -0.16 2.42 0.12
age       -0.15 -0.12  0.08  0.19  0.08 3.49 0.08
O3         0.03  0.17  0.02  0.05  0.62 1.18 0.47
O1         0.01  0.13  0.07 -0.03  0.52 1.17 0.33
O5         0.13  0.14 -0.03 -0.02 -0.52 1.28 0.29
O2         0.20  0.11 -0.08  0.10 -0.44 1.77 0.24
O4         0.16 -0.28 -0.02  0.18  0.37 2.83 0.24
education -0.10 -0.14 -0.01  0.10  0.15 3.47 0.05
                                                Item
N1                                 Get angry easily.
N2                             Get irritated easily.
N3                        Have frequent mood swings.
N5                                     Panic easily.
N4                                  Often feel blue.
E4                              Make friends easily.
E2             Find it difficult to approach others.
E1                                 Don't talk a lot.
E3                     Know how to captivate people.
E5                                      Take charge.
C2             Continue until everything is perfect.
C4                   Do things in a half-way manner.
C3                    Do things according to a plan.
C5                                    Waste my time.
C1                           Am exacting in my work.
A2                 Inquire about others' well-being.
A3                       Know how to comfort others.
A1         Am indifferent to the feelings of others.
A5                         Make people feel at ease.
A4                                    Love children.
gender                            males=1, females=2
age                                     age in years
O3         Carry the conversation to a higher level.
O1                                 Am full of ideas.
O5             Will not probe deeply into a subject.
O2                 Avoid difficult reading material.
O4                  Spend time reflecting on things.
education in HS, fin HS, coll,  coll grad , grad deg
> 
> 
> 
> 
> cleanEx()
> nameEx("bfi")
> ### * bfi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bfi
> ### Title: 25 Personality items representing 5 factors
> ### Aliases: bfi bfi.dictionary
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(bfi)
> describe(bfi)
          vars    n  mean    sd median trimmed   mad min max range  skew
A1           1 2784  2.41  1.41      2    2.23  1.48   1   6     5  0.83
A2           2 2773  4.80  1.17      5    4.98  1.48   1   6     5 -1.12
A3           3 2774  4.60  1.30      5    4.79  1.48   1   6     5 -1.00
A4           4 2781  4.70  1.48      5    4.93  1.48   1   6     5 -1.03
A5           5 2784  4.56  1.26      5    4.71  1.48   1   6     5 -0.85
C1           6 2779  4.50  1.24      5    4.64  1.48   1   6     5 -0.85
C2           7 2776  4.37  1.32      5    4.50  1.48   1   6     5 -0.74
C3           8 2780  4.30  1.29      5    4.42  1.48   1   6     5 -0.69
C4           9 2774  2.55  1.38      2    2.41  1.48   1   6     5  0.60
C5          10 2784  3.30  1.63      3    3.25  1.48   1   6     5  0.07
E1          11 2777  2.97  1.63      3    2.86  1.48   1   6     5  0.37
E2          12 2784  3.14  1.61      3    3.06  1.48   1   6     5  0.22
E3          13 2775  4.00  1.35      4    4.07  1.48   1   6     5 -0.47
E4          14 2791  4.42  1.46      5    4.59  1.48   1   6     5 -0.82
E5          15 2779  4.42  1.33      5    4.56  1.48   1   6     5 -0.78
N1          16 2778  2.93  1.57      3    2.82  1.48   1   6     5  0.37
N2          17 2779  3.51  1.53      4    3.51  1.48   1   6     5 -0.08
N3          18 2789  3.22  1.60      3    3.16  1.48   1   6     5  0.15
N4          19 2764  3.19  1.57      3    3.12  1.48   1   6     5  0.20
N5          20 2771  2.97  1.62      3    2.85  1.48   1   6     5  0.37
O1          21 2778  4.82  1.13      5    4.96  1.48   1   6     5 -0.90
O2          22 2800  2.71  1.57      2    2.56  1.48   1   6     5  0.59
O3          23 2772  4.44  1.22      5    4.56  1.48   1   6     5 -0.77
O4          24 2786  4.89  1.22      5    5.10  1.48   1   6     5 -1.22
O5          25 2780  2.49  1.33      2    2.34  1.48   1   6     5  0.74
gender      26 2800  1.67  0.47      2    1.71  0.00   1   2     1 -0.73
education   27 2577  3.19  1.11      3    3.22  1.48   1   5     4 -0.05
age         28 2800 28.78 11.13     26   27.43 10.38   3  86    83  1.02
          kurtosis   se
A1           -0.31 0.03
A2            1.05 0.02
A3            0.44 0.02
A4            0.04 0.03
A5            0.16 0.02
C1            0.30 0.02
C2           -0.14 0.03
C3           -0.13 0.02
C4           -0.62 0.03
C5           -1.22 0.03
E1           -1.09 0.03
E2           -1.15 0.03
E3           -0.47 0.03
E4           -0.30 0.03
E5           -0.09 0.03
N1           -1.01 0.03
N2           -1.05 0.03
N3           -1.18 0.03
N4           -1.09 0.03
N5           -1.06 0.03
O1            0.43 0.02
O2           -0.81 0.03
O3            0.30 0.02
O4            1.08 0.02
O5           -0.24 0.03
gender       -1.47 0.01
education    -0.32 0.02
age           0.56 0.21
>  
>  keys.list <-
+   list(agree=c("-A1","A2","A3","A4","A5"),conscientious=c("C1","C2","C3","-C4","-C5"),
+ extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
+ openness = c("O1","-O2","O3","O4","-O5")) 
>  scores <- scoreItems(keys.list,bfi,min=1,max=6) #specify the minimum and maximum values
>  scores
Call: scoreItems(keys = keys.list, items = bfi, min = 1, max = 6)

(Unstandardized) Alpha:
      agree conscientious extraversion neuroticism openness
alpha   0.7          0.72         0.76        0.81      0.6

Standard errors of unstandardized Alpha:
      agree conscientious extraversion neuroticism openness
ASE   0.014         0.014        0.013       0.011    0.017

Average item correlation:
          agree conscientious extraversion neuroticism openness
average.r  0.32          0.34         0.39        0.46     0.23

 Guttman 6* reliability: 
         agree conscientious extraversion neuroticism openness
Lambda.6   0.7          0.72         0.76        0.81      0.6

Signal/Noise based upon av.r : 
             agree conscientious extraversion neuroticism openness
Signal/Noise   2.3           2.6          3.2         4.3      1.5

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.26          0.72         0.35      -0.305     0.30
extraversion   0.46          0.26         0.76      -0.284     0.32
neuroticism   -0.18         -0.23        -0.22       0.812    -0.12
openness       0.15          0.19         0.22      -0.086     0.60

 In order to see the item by scale loadings and frequency counts of the data
 print with the short option = FALSE>  #show the use of the fa.lookup with a dictionary
>  keys.lookup(keys.list,bfi.dictionary[,1:4])
    ItemLabel                                      Item     Giant3
A1-     q_146 Am indifferent to the feelings of others.   Cohesion
A2     q_1162         Inquire about others' well-being.   Cohesion
A3     q_1206               Know how to comfort others.   Cohesion
A4     q_1364                            Love children.   Cohesion
A5     q_1419                 Make people feel at ease.   Cohesion
C1      q_124                   Am exacting in my work.  Stability
C2      q_530     Continue until everything is perfect.  Stability
C3      q_619            Do things according to a plan.  Stability
C4-     q_626           Do things in a half-way manner.  Stability
C5-    q_1949                            Waste my time.  Stability
E1-     q_712                         Don't talk a lot. Plasticity
E2-     q_901     Find it difficult to approach others. Plasticity
E3     q_1205             Know how to captivate people. Plasticity
E4     q_1410                      Make friends easily. Plasticity
E5     q_1768                              Take charge. Plasticity
N1      q_952                         Get angry easily.  Stability
N2      q_974                     Get irritated easily.  Stability
N3     q_1099                Have frequent mood swings.  Stability
N4     q_1479                          Often feel blue.  Stability
N5     q_1505                             Panic easily.  Stability
O1      q_128                         Am full of ideas. Plasticity
O2-     q_316         Avoid difficult reading material. Plasticity
O3      q_492 Carry the conversation to a higher level. Plasticity
O4     q_1738          Spend time reflecting on things. Plasticity
O5-    q_1964     Will not probe deeply into a subject. Plasticity
                   Big6
A1-       Agreeableness
A2        Agreeableness
A3        Agreeableness
A4        Agreeableness
A5        Agreeableness
C1    Conscientiousness
C2    Conscientiousness
C3    Conscientiousness
C4-   Conscientiousness
C5-   Conscientiousness
E1-        Extraversion
E2-        Extraversion
E3         Extraversion
E4         Extraversion
E5         Extraversion
N1  Emotional Stability
N2  Emotional Stability
N3  Emotional Stability
N4  Emotional Stability
N5  Emotional Stability
O1             Openness
O2-            Openness
O3             Openness
O4             Openness
O5-            Openness
>  
> 
> 
> 
> cleanEx()
> nameEx("bi.bars")
> ### * bi.bars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bi.bars
> ### Title: Draw pairs of bargraphs based on two groups
> ### Aliases: bi.bars
> ### Keywords: hplot
> 
> ### ** Examples
> 
> data(bfi)
> with(bfi,{bi.bars(age,gender,ylab="Age",main="Age by males and females")
+        bi.bars(education,gender,xlab="Education",main="Education by gender",horiz=FALSE)})
> 
> 
> 
> cleanEx()
> nameEx("bifactor")
> ### * bifactor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Bechtoldt
> ### Title: Seven data sets showing a bifactor solution.
> ### Aliases: Bechtoldt.1 Bechtoldt.2 Bechtoldt Holzinger Holzinger.9 Reise
> ###   Thurstone Thurstone.33
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
> if(!require(GPArotation)) {message("I am sorry, to run omega requires GPArotation") 
+         } else {
+ #holz <- omega(Holzinger,4, title = "14 ability tests from Holzinger-Swineford")
+ #bf <- omega(Reise,5,title="16 health items from Reise") 
+ #omega(Reise,5,labels=colnames(Reise),title="16 health items from Reise")
+ thur.om <- omega(Thurstone,title="9 variables from Thurstone") #compare with
+ thur.bf   <- fa(Thurstone,3,rotate="biquartimin")
+ factor.congruence(thur.om,thur.bf)
+ }
Loading required package: GPArotation
     MR1   MR2   MR3
g   0.99  0.58  0.46
F1* 0.66  0.97  0.01
F2* 0.55  0.01  0.97
F3* 0.64 -0.15 -0.15
h2  0.98  0.59  0.43
> 
> 
> 
> cleanEx()

detaching ‘package:GPArotation’

> nameEx("biplot.psych")
> ### * biplot.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: biplot.psych
> ### Title: Draw biplots of factor or component scores by factor or
> ###   component loadings
> ### Aliases: biplot.psych
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> #the standard example
> data(USArrests)
> fa2 <- fa(USArrests,2,scores=TRUE)
> biplot(fa2,labels=rownames(USArrests))
> 
> # plot the 3 factor solution
> data(bfi)
> fa3 <- fa(bfi[1:200,1:15],3,scores=TRUE)
> biplot(fa3)
> #just plot factors 1 and 3 from that solution
> biplot(fa3,choose=c(1,3))
> 
> #
> fa2 <- fa(bfi[16:25],2)  #factor analysis
> fa2$scores <- fa2$scores[1:100,]  #just take the first 100
> #now plot with different colors and shapes for males and females
> biplot(fa2,pch=c(24,21)[bfi[1:100,"gender"]],group =bfi[1:100,"gender"],
+    main="Biplot of Conscientiousness and Neuroticism by gender")
> 
> 
> r <- cor(bfi[1:200,1:10], use="pairwise") #find the correlations
> f2 <- fa(r,2) 
> x <- list() 
> x$scores <- factor.scores(bfi[1:200,1:10],f2)
> x$loadings <- f2$loadings
> class(x) <- c('psych','fa')
> biplot(x,main="biplot from correlation matrix and factor scores")
> 
> 
> 
> 
> cleanEx()
> nameEx("block.random")
> ### * block.random
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: block.random
> ### Title: Create a block randomized structure for n independent variables
> ### Aliases: block.random
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> br <- block.random(n=24,c(2,3))
> pairs.panels(br)
> br <- block.random(96,c(time=4,drug=3,sex=2))
> pairs.panels(br)
> 
> 
> 
> cleanEx()
> nameEx("blot")
> ### * blot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: blot
> ### Title: Bond's Logical Operations Test - BLOT
> ### Aliases: blot
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(blot)
> #not run
> #library(ltm)
> #bblot.rasch <- rasch(blot, constraint = cbind(ncol(blot) + 1, 1))  #a 1PL model
> #blot.2pl <- ltm(blot~z1)  #a 2PL model
> #do the same thing with functions in psych
> #blot.fa <- irt.fa(blot)  # a 2PN model
> #plot(blot.fa)
> 
> 
> 
> cleanEx()
> nameEx("bock.table")
> ### * bock.table
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bock
> ### Title: Bock and Liberman (1970) data set of 1000 observations of the
> ###   LSAT
> ### Aliases: bock bock.table lsat6 lsat7 bock.lsat
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(bock)
> responses <- table2df(bock.table[,2:6],count=bock.table[,7],
+         labs= paste("lsat6.",1:5,sep=""))
> describe(responses)
        vars    n mean   sd median trimmed mad min max range  skew kurtosis
lsat6.1    1 1000 0.92 0.27      1    1.00   0   0   1     1 -3.20     8.22
lsat6.2    2 1000 0.71 0.45      1    0.76   0   0   1     1 -0.92    -1.16
lsat6.3    3 1000 0.55 0.50      1    0.57   0   0   1     1 -0.22    -1.95
lsat6.4    4 1000 0.76 0.43      1    0.83   0   0   1     1 -1.24    -0.48
lsat6.5    5 1000 0.87 0.34      1    0.96   0   0   1     1 -2.17     2.72
          se
lsat6.1 0.01
lsat6.2 0.01
lsat6.3 0.02
lsat6.4 0.01
lsat6.5 0.01
> ## maybe str(bock.table) ; plot(bock.table) ...
> 
> 
> 
> cleanEx()
> nameEx("burt")
> ### * burt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: burt
> ### Title: 11 emotional variables from Burt (1915)
> ### Aliases: burt
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(burt)
> eigen(burt)$values  #one is negative!
 [1]  5.16688557  1.79046638  0.96961472  0.77670247  0.69415362  0.62296025
 [7]  0.51271989  0.34702229  0.12987550  0.01413376 -0.02453446
> burt.new <- burt
> burt.new[2,3] <- burt.new[3,2] <- .81
> eigen(burt.new)$values  #all are positive
 [1] 5.151669e+00 1.782279e+00 9.692796e-01 7.792475e-01 6.935441e-01
 [6] 6.177734e-01 5.042254e-01 3.423256e-01 1.319504e-01 2.770323e-02
[11] 2.646875e-06
> bs <- cor.smooth(burt)
Warning in cor.smooth(burt) :
  Matrix was not positive definite, smoothing was done
> round(burt.new - bs,3)
           Sociality Sorrow Tenderness   Joy Wonder Elation Disgust  Anger
Sociality      0.000  0.000      0.010 0.002  0.003   0.001   0.003  0.003
Sorrow         0.000  0.000     -0.044 0.004  0.005   0.002   0.006  0.006
Tenderness     0.010 -0.044      0.000 0.001 -0.001   0.002  -0.002 -0.003
Joy            0.002  0.004      0.001 0.000  0.000   0.000   0.000  0.000
Wonder         0.003  0.005     -0.001 0.000  0.000   0.000  -0.001  0.000
Elation        0.001  0.002      0.002 0.000  0.000   0.000   0.000  0.001
Disgust        0.003  0.006     -0.002 0.000 -0.001   0.000   0.000 -0.001
Anger          0.003  0.006     -0.003 0.000  0.000   0.001  -0.001  0.000
Sex            0.000 -0.001      0.004 0.001  0.001   0.000   0.001  0.002
Fear           0.000  0.002      0.001 0.000  0.000   0.000   0.000  0.000
Subjection     0.000  0.001      0.002 0.000  0.000   0.000   0.000  0.000
              Sex  Fear Subjection
Sociality   0.000 0.000      0.000
Sorrow     -0.001 0.002      0.001
Tenderness  0.004 0.001      0.002
Joy         0.001 0.000      0.000
Wonder      0.001 0.000      0.000
Elation     0.000 0.000      0.000
Disgust     0.001 0.000      0.000
Anger       0.002 0.000      0.000
Sex         0.000 0.000      0.000
Fear        0.000 0.000      0.000
Subjection  0.000 0.000      0.000
> 
> 
> 
> 
> cleanEx()
> nameEx("cattell")
> ### * cattell
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cattell
> ### Title: 12 cognitive variables from Cattell (1963)
> ### Aliases: cattell
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(cattell)
> corPlot(cattell,numbers=TRUE,upper=FALSE,diag=FALSE,
+              main="12 cognitive variables from Cattell (1963)")
> 
> 
> 
> cleanEx()
> nameEx("circ.tests")
> ### * circ.tests
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: circ.tests
> ### Title: Apply four tests of circumplex versus simple structure
> ### Aliases: circ.tests
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> circ.data <- circ.sim(24,500)
> circ.fa <- fa(circ.data,2)
> plot(circ.fa,title="Circumplex Structure")
> ct <- circ.tests(circ.fa)
> #compare with non-circumplex data
> simp.data <- item.sim(24,500)
> simp.fa <- fa(simp.data,2)
> plot(simp.fa,title="Simple Structure")
> st <- circ.tests(simp.fa)
> res <- rbind(ct[1:4],st[1:4])
> rownames(res) <- c("circumplex","Simple")
> print(res,digits=2)
           gaps   fisher RT   VT  
circumplex 0.0056 0.09   0.35 0.35
Simple     0.28   0.11   0.91 0.87
> 
> 
> 
> 
> cleanEx()
> nameEx("cities")
> ### * cities
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cities
> ### Title: Distances between 11 US cities
> ### Aliases: cities city.location
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
> data(cities)
> city.location[,1] <- -city.location[,1]
> #not run
> #an overlay map can be added if the package maps is available
> #
> #
> #libary(maps)
> #map("usa")
> #title("MultiDimensional Scaling of US cities")
> #points(city.location)
> 
> plot(city.location, xlab="Dimension 1", ylab="Dimension 2",
+    main ="Multidimensional scaling of US cities")
> city.loc <- cmdscale(cities, k=2) #ask for a 2 dimensional solution  round(city.loc,0) 
> city.loc <- -city.loc 
>  city.loc <- rescale(city.loc,apply(city.location,2,mean),apply(city.location,2,sd))
> points(city.loc,type="n") 
> text(city.loc,labels=names(cities))
> 
> 
> 
> 
> cleanEx()
> nameEx("cluster.cor")
> ### * cluster.cor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scoreOverlap
> ### Title: Find correlations of composite variables (corrected for overlap)
> ###   from a larger matrix.
> ### Aliases: cluster.cor scoreOverlap
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #use the msq data set that shows the structure of energetic and tense arousal
> small.msq <- msq[ c("active", "energetic", "vigorous", "wakeful", "wide.awake", 
+ "full.of.pep", "lively", "sleepy", "tired", "drowsy","intense", "jittery", "fearful",
+  "tense", "clutched.up", "quiet", "still",    "placid", "calm", "at.rest") ]
> small.R <- cor(small.msq,use="pairwise")
> keys.list <- list(
+ EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
+        "lively", "-sleepy", "-tired", "-drowsy"),
+ TA =c("intense", "jittery", "fearful", "tense", "clutched.up", "-quiet", "-still", 
+        "-placid", "-calm", "-at.rest") ,
+ 
+ high.EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
+        "lively"),
+ low.EA =c("sleepy", "tired", "drowsy"),
+ lowTA= c("quiet", "still", "placid", "calm", "at.rest"),
+ highTA = c("intense", "jittery", "fearful", "tense", "clutched.up")
+    ) 
>    
> keys <- make.keys(small.R,keys.list)
>       
> adjusted.scales <- scoreOverlap(keys.list,small.R)
> #compare with unadjusted
> confounded.scales <- cluster.cor(keys,small.R)
> summary(adjusted.scales)
Call: scoreOverlap(keys = keys.list, r = small.R)

Scale intercorrelations adjusted for item overlap
Scale intercorrelations corrected for attenuation 
 raw correlations (corrected for overlap) below the diagonal, (standardized) alpha on the diagonal 
 corrected (for overlap and reliability) correlations above the diagonal:
           EA    TA high.EA low.EA lowTA highTA
EA       0.93  0.27   0.965 -0.803 -0.18  0.253
TA       0.23  0.75   0.282 -0.167 -0.81  0.821
high.EA  0.90  0.24   0.937 -0.620 -0.12  0.324
low.EA  -0.75 -0.14  -0.579  0.928  0.25 -0.023
lowTA   -0.15 -0.60  -0.098  0.204  0.73 -0.335
highTA   0.21  0.62   0.273 -0.019 -0.25  0.757
> #note that the EA and high and low EA and TA and high and low TA 
> # scale correlations are confounded
> summary(confounded.scales) 
Call: cluster.cor(keys = keys, r.mat = small.R)

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, (standardized) alpha on the diagonal 
 corrected correlations above the diagonal:
           EA    TA high.EA low.EA lowTA highTA
EA       0.93  0.27   1.024 -0.848 -0.18  0.253
TA       0.23  0.75   0.282 -0.167 -1.06  1.056
high.EA  0.96  0.24   0.937 -0.620 -0.12  0.324
low.EA  -0.79 -0.14  -0.579  0.928  0.25 -0.023
lowTA   -0.15 -0.78  -0.098  0.204  0.73 -0.335
highTA   0.21  0.80   0.273 -0.019 -0.25  0.757
> 
> 
> 
> cleanEx()
> nameEx("cluster.fit")
> ### * cluster.fit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster.fit
> ### Title: cluster Fit: fit of the cluster model to a correlation matrix
> ### Aliases: cluster.fit
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
>  r.mat<- Harman74.cor$cov
>  iq.clus <- ICLUST(r.mat,nclusters =2)
>  fit <- cluster.fit(r.mat,iq.clus$loadings,iq.clus$clusters)
>  fit
$clusterfit
[1] 0.7943439

$structurefit
[1] 0.6711022

$patternfit
[1] 0.9249676

$clusterrmse
[1] 0.1753907

$patternrmse
[1] 0.1059401

>  
> 
> 
> 
> 
> cleanEx()
> nameEx("cluster.loadings")
> ### * cluster.loadings
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster.loadings
> ### Title: Find item by cluster correlations, corrected for overlap and
> ###   reliability
> ### Aliases: cluster.loadings
> ### Keywords: multivariate cluster
> 
> ### ** Examples
> 
> 
>  r.mat<- Harman74.cor$cov
>  clusters <- matrix(c(1,1,1,rep(0,24),1,1,1,1,rep(0,17)),ncol=2)
>  cluster.loadings(clusters,r.mat)
Call: cluster.loadings(keys = clusters, r.mat = r.mat)

(Standardized) Alpha:
[1] 0.61 0.79

(Standardized) G6*:
[1] 0.66 0.84

Average item correlation:
[1] 0.35 0.48

Number of items:
[1] 3 4

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
     [,1] [,2]
[1,] 0.61 0.69
[2,] 0.48 0.79

Item by scale intercorrelations
 corrected for item overlap and scale reliability
                        [,1] [,2]
VisualPerception       0.675 0.50
Cubes                  0.512 0.32
PaperFormBoard         0.636 0.36
Flags                  0.550 0.45
GeneralInformation     0.468 0.76
PargraphComprehension  0.459 0.82
SentenceCompletion     0.375 0.84
WordClassification     0.478 0.74
WordMeaning            0.387 0.85
Addition               0.054 0.30
Code                   0.301 0.36
CountingDots           0.328 0.23
StraightCurvedCapitals 0.575 0.46
WordRecognition        0.222 0.30
NumberRecognition      0.238 0.27
FigureRecognition      0.520 0.34
ObjectNumber           0.196 0.31
NumberFigure           0.457 0.29
FigureWord             0.380 0.28
Deduction              0.523 0.57
NumericalPuzzles       0.461 0.43
ProblemReasoning       0.491 0.56
SeriesCompletion       0.661 0.56
ArithmeticProblems     0.382 0.54
> 
>  
> 
> 
> 
> 
> cleanEx()
> nameEx("cluster.plot")
> ### * cluster.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster.plot
> ### Title: Plot factor/cluster loadings and assign items to clusters by
> ###   their highest loading.
> ### Aliases: cluster.plot fa.plot factor.plot
> ### Keywords: multivariate hplot cluster
> 
> ### ** Examples
> 
> circ.data <- circ.sim(24,500)
> circ.fa <- fa(circ.data,2)
> plot(circ.fa,cut=.5)
> f5 <- fa(bfi[1:25],5) 
> plot(f5,labels=colnames(bfi)[1:25],show.points=FALSE)
> plot(f5,labels=colnames(bfi)[1:25],show.points=FALSE,choose=c(1,2,4))
> 
> 
> 
> cleanEx()
> nameEx("cluster2keys")
> ### * cluster2keys
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cluster2keys
> ### Title: Convert a cluster vector (from e.g., kmeans) to a keys matrix
> ###   suitable for scoring item clusters.
> ### Aliases: cluster2keys
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> test.data <- Harman74.cor$cov
> kc <- kmeans(test.data,4)
> keys <- cluster2keys(kc)
> keys  #these match those found by ICLUST
      [,1] [,2] [,3] [,4]
 [1,]    1    0    0    0
 [2,]    1    0    0    0
 [3,]    1    0    0    0
 [4,]    1    0    0    0
 [5,]    0    1    0    0
 [6,]    0    1    0    0
 [7,]    0    1    0    0
 [8,]    0    1    0    0
 [9,]    0    1    0    0
[10,]    0    0    1    0
[11,]    0    0    1    0
[12,]    0    0    1    0
[13,]    0    0    1    0
[14,]    0    0    0    1
[15,]    0    0    0    1
[16,]    0    0    0    1
[17,]    0    0    0    1
[18,]    0    0    0    1
[19,]    0    0    0    1
[20,]    1    0    0    0
[21,]    0    0    1    0
[22,]    1    0    0    0
[23,]    1    0    0    0
[24,]    0    0    1    0
> cluster.cor(keys,test.data)
Call: cluster.cor(keys = keys, r.mat = test.data)

(Standardized) Alpha:
[1] 0.80 0.90 0.83 0.74

(Standardized) G6*:
[1] 0.83 0.90 0.86 0.76

Average item correlation:
[1] 0.36 0.64 0.45 0.32

Number of items:
[1] 7 5 6 6

Signal to Noise ratio based upon average r and n 
[1] 4.0 8.8 4.8 2.8

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
     [,1] [,2] [,3] [,4]
[1,] 0.80 0.72 0.63 0.69
[2,] 0.61 0.90 0.56 0.52
[3,] 0.51 0.48 0.83 0.67
[4,] 0.53 0.43 0.53 0.74
> 
> 
> 
> cleanEx()
> nameEx("comorbidity")
> ### * comorbidity
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: comorbidity
> ### Title: Convert base rates of two diagnoses and their comorbidity into
> ###   phi, Yule, and tetrachorics
> ### Aliases: comorbidity
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> comorbidity(.2,.15,.1,c("Anxiety","Depression")) 
Call: comorbidity(d1 = 0.2, d2 = 0.15, com = 0.1, labels = c("Anxiety", 
    "Depression"))
Comorbidity table 
            Anxiety -Anxiety
Depression      0.1     0.05
-Depression     0.1     0.75

implies phi =  0.49  with Yule =  0.87  and tetrachoric correlation of  0.75
and normal thresholds of  1.04 0.84> 
> 
> 
> cleanEx()
> nameEx("cor.ci")
> ### * cor.ci
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cor.ci
> ### Title: Bootstrapped confidence intervals for raw and composite
> ###   correlations
> ### Aliases: cor.ci corCi
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> cor.ci(bfi[1:200,1:10])  # just the first 10 variables
Call:corCi(x = x, keys = keys, n.iter = n.iter, p = p, overlap = overlap, 
    poly = poly, method = method, plot = plot)

 Coefficients and bootstrapped confidence intervals 
   A1    A2    A3    A4    A5    C1    C2    C3    C4    C5   
A1  1.00                                                      
A2 -0.40  1.00                                                
A3 -0.27  0.55  1.00                                          
A4 -0.06  0.24  0.33  1.00                                    
A5 -0.26  0.43  0.61  0.31  1.00                              
C1 -0.10  0.19  0.15  0.10  0.06  1.00                        
C2 -0.03  0.15  0.12  0.20  0.04  0.49  1.00                  
C3 -0.04  0.17  0.08  0.11  0.07  0.44  0.48  1.00            
C4  0.14 -0.24 -0.15 -0.10 -0.11 -0.34 -0.34 -0.33  1.00      
C5  0.08 -0.10 -0.14 -0.12 -0.12 -0.26 -0.22 -0.27  0.41  1.00

 scale correlations and bootstrapped confidence intervals 
      lower.emp lower.norm estimate upper.norm upper.emp    p
A1-A2     -0.54      -0.53    -0.40      -0.27     -0.28 0.00
A1-A3     -0.39      -0.41    -0.27      -0.12     -0.12 0.00
A1-A4     -0.19      -0.19    -0.06       0.09      0.11 0.46
A1-A5     -0.40      -0.40    -0.26      -0.11     -0.11 0.00
A1-C1     -0.20      -0.22    -0.10       0.04      0.06 0.19
A1-C2     -0.17      -0.18    -0.03       0.12      0.10 0.68
A1-C3     -0.22      -0.20    -0.04       0.13      0.11 0.66
A1-C4      0.01      -0.01     0.14       0.29      0.31 0.08
A1-C5     -0.06      -0.04     0.08       0.22      0.21 0.18
A2-A3      0.43       0.42     0.55       0.65      0.64 0.00
A2-A4      0.11       0.09     0.24       0.36      0.35 0.00
A2-A5      0.31       0.29     0.43       0.55      0.55 0.00
A2-C1      0.05       0.05     0.19       0.33      0.34 0.01
A2-C2     -0.03      -0.03     0.15       0.31      0.30 0.11
A2-C3      0.00       0.00     0.17       0.33      0.33 0.05
A2-C4     -0.37      -0.39    -0.24      -0.09     -0.08 0.00
A2-C5     -0.23      -0.24    -0.10       0.02      0.01 0.09
A3-A4      0.17       0.17     0.33       0.47      0.48 0.00
A3-A5      0.48       0.47     0.61       0.69      0.70 0.00
A3-C1      0.01       0.00     0.15       0.30      0.28 0.05
A3-C2     -0.02      -0.05     0.12       0.28      0.26 0.16
A3-C3     -0.07      -0.08     0.08       0.23      0.22 0.35
A3-C4     -0.28      -0.28    -0.15      -0.02     -0.02 0.03
A3-C5     -0.30      -0.29    -0.14      -0.02     -0.03 0.02
A4-A5      0.19       0.17     0.31       0.43      0.42 0.00
A4-C1     -0.03      -0.02     0.10       0.23      0.22 0.11
A4-C2      0.02       0.06     0.20       0.34      0.32 0.01
A4-C3     -0.03      -0.02     0.11       0.24      0.22 0.09
A4-C4     -0.24      -0.23    -0.10       0.03      0.04 0.14
A4-C5     -0.27      -0.26    -0.12       0.01     -0.01 0.08
A5-C1     -0.08      -0.11     0.06       0.22      0.21 0.50
A5-C2     -0.11      -0.11     0.04       0.18      0.18 0.63
A5-C3     -0.09      -0.07     0.07       0.22      0.19 0.33
A5-C4     -0.24      -0.25    -0.11       0.02      0.03 0.10
A5-C5     -0.27      -0.28    -0.12       0.01      0.00 0.07
C1-C2      0.38       0.38     0.49       0.59      0.60 0.00
C1-C3      0.31       0.29     0.44       0.54      0.53 0.00
C1-C4     -0.48      -0.46    -0.34      -0.19     -0.20 0.00
C1-C5     -0.38      -0.41    -0.26      -0.10     -0.11 0.00
C2-C3      0.34       0.35     0.48       0.58      0.56 0.00
C2-C4     -0.47      -0.47    -0.34      -0.20     -0.22 0.00
C2-C5     -0.34      -0.35    -0.22      -0.07     -0.10 0.00
C3-C4     -0.47      -0.47    -0.33      -0.18     -0.21 0.00
C3-C5     -0.38      -0.38    -0.27      -0.14     -0.14 0.00
C4-C5      0.29       0.27     0.41       0.53      0.54 0.00
> #The keys have overlapping scales
>   keys.list <- list(agree=c("-A1","A2","A3","A4","A5"), conscientious= c("C1", 
+   "C2","C3","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"), neuroticism= 
+   c("N1", "N2", "N3","N4","N5"), openness = c("O1","-O2","O3","O4","-O5"), 
+   alpha=c("-A1","A2","A3","A4","A5","C1","C2","C3","-C4","-C5","N1","N2","N3","N4","N5"),
+ beta = c("-E1","-E2","E3","E4","E5","O1","-O2","O3","O4","-O5") )
>   keys <- make.keys(bfi,keys.list)
>   
> #do not correct for item overlap
> rci <-  cor.ci(bfi[1:200,],keys,n.iter=10,main="correlation with overlapping scales") 
> #also shows the graphic -note the overlap
> #correct for overlap
> rci <-  cor.ci(bfi[1:200,],keys,overlap=TRUE, n.iter=10,main="Correct for overlap") 
> #show the confidence intervals
> ci <- cor.plot.upperLowerCi(rci)  #to show the upper and lower confidence intervals
> ci   #print the confidence intervals in matrix form

 High and low confidence intervals 
               agre cnsc  extr  nrtc  opnn alph  beta
agree          1.00 0.35  0.56 -0.28  0.39 0.50  0.56
conscientious  0.17 1.00  0.41 -0.17  0.42 0.56  0.47
extraversion   0.28 0.14  1.00 -0.36  0.37 0.35  0.70
neuroticism   -0.08 0.02 -0.18  1.00 -0.27 0.39 -0.37
openness       0.03 0.13  0.14  0.01  1.00 0.41  0.63
alpha          0.31 0.38  0.10  0.20 -0.03 1.00  0.43
beta           0.24 0.23  0.59 -0.14  0.54 0.10  1.00
> 
> 
> 
> cleanEx()
> nameEx("cor.plot")
> ### * cor.plot
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cor.plot
> ### Title: Create an image plot for a correlation or factor matrix
> ### Aliases: cor.plot corPlot cor.plot.upperLowerCi
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> cor.plot(Thurstone,main="9 cognitive variables from Thurstone") 
> #just blue implies positive manifold
> #select just some variables to plot
> cor.plot(Thurstone, zlim=c(0,1),main="9 cognitive variables from Thurstone",select=c(1:3,7:9))
> #now show a non-symmetric plot
> cor.plot(Thurstone[4:9,1:3], zlim=c(0,1),main="9 cognitive variables
+  from Thurstone",numbers=TRUE,symmetric=FALSE)
> 
> #Two ways of including stars to show significance
> #From the raw data
> corPlot(sat.act,numbers=TRUE,stars=TRUE)
> #from a correlation matrix with pvals
> cp <- corr.test(sat.act)  #find the correlations and pvals
> r<- cp$r
> p <- cp$p
> corPlot(r,numbers=TRUE,diag=FALSE,stars=TRUE, pval = p,main="Correlation plot
+  with Holm corrected 'significance'")
> 
> #now red means less than .5
> corPlot(mat.sort(Thurstone),TRUE,zlim=c(0,1), 
+        main="9 cognitive variables from Thurstone (sorted by factor loading) ")
> simp <- sim.circ(24)
> corPlot(cor(simp),main="24 variables in a circumplex")
> 
> #scale by raw and adjusted probabilities
> rs <- corr.test(sat.act[1:200,] ) #find the probabilities of the correlations
> corPlot(r=rs$r,numbers=TRUE,pval=rs$p,main="Correlations scaled by probability values") 
>  #Show the upper and lower confidence intervals
> cor.plot.upperLowerCi(R=rs,numbers=TRUE) 
> 
> #do multiple plots 
> #Also show the xaxis option
> op <- par(mfrow=c(2,2))
> corPlot(ability,show.legend=FALSE,keep.par=FALSE,upper=FALSE)
> f4 <- fa(ability,4)
> corPlot(f4,show.legend=FALSE,keep.par=FALSE,numbers=TRUE,xlas=3)
> om <- omega(ability,4)
> corPlot(om,show.legend=FALSE,keep.par=FALSE,numbers=TRUE,xaxis=3)
> par(op)
> 
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("cor.smooth")
> ### * cor.smooth
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cor.smooth
> ### Title: Smooth a non-positive definite correlation matrix to make it
> ###   positive definite
> ### Aliases: cor.smooth cor.smoother
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> bs <- cor.smooth(burt)  #burt data set is not positive definite
Warning in cor.smooth(burt) :
  Matrix was not positive definite, smoothing was done
> plot(burt[lower.tri(burt)],bs[lower.tri(bs)],ylab="smoothed values",xlab="original values")
> abline(0,1,lty="dashed")
> 
> round(burt - bs,3) 
           Sociality Sorrow Tenderness   Joy Wonder Elation Disgust  Anger
Sociality      0.000  0.000      0.010 0.002  0.003   0.001   0.003  0.003
Sorrow         0.000  0.000      0.016 0.004  0.005   0.002   0.006  0.006
Tenderness     0.010  0.016      0.000 0.001 -0.001   0.002  -0.002 -0.003
Joy            0.002  0.004      0.001 0.000  0.000   0.000   0.000  0.000
Wonder         0.003  0.005     -0.001 0.000  0.000   0.000  -0.001  0.000
Elation        0.001  0.002      0.002 0.000  0.000   0.000   0.000  0.001
Disgust        0.003  0.006     -0.002 0.000 -0.001   0.000   0.000 -0.001
Anger          0.003  0.006     -0.003 0.000  0.000   0.001  -0.001  0.000
Sex            0.000 -0.001      0.004 0.001  0.001   0.000   0.001  0.002
Fear           0.000  0.002      0.001 0.000  0.000   0.000   0.000  0.000
Subjection     0.000  0.001      0.002 0.000  0.000   0.000   0.000  0.000
              Sex  Fear Subjection
Sociality   0.000 0.000      0.000
Sorrow     -0.001 0.002      0.001
Tenderness  0.004 0.001      0.002
Joy         0.001 0.000      0.000
Wonder      0.001 0.000      0.000
Elation     0.000 0.000      0.000
Disgust     0.001 0.000      0.000
Anger       0.002 0.000      0.000
Sex         0.000 0.000      0.000
Fear        0.000 0.000      0.000
Subjection  0.000 0.000      0.000
> fa(burt,2) #this throws a warning that the matrix yields an improper solution
Warning in cor.smooth(R) :
  Matrix was not positive definite, smoothing was done
Warning in cor.smooth(R) :
  Matrix was not positive definite, smoothing was done
Warning in cor.smooth(R) :
  Matrix was not positive definite, smoothing was done
Warning in cor.smooth(r) :
  Matrix was not positive definite, smoothing was done
The estimated weights for the factor scores are probably incorrect.  Try a different factor extraction method.
Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate,  :
  An ultra-Heywood case was detected.  Examine the results carefully
Factor Analysis using method =  minres
Call: fa(r = burt, nfactors = 2)
Standardized loadings (pattern matrix) based upon correlation matrix
             MR1   MR2   h2     u2 com
Sociality   0.67  0.51 1.02 -0.021 1.9
Sorrow      0.26  0.79 0.88  0.119 1.2
Tenderness  0.03  0.88 0.79  0.205 1.0
Joy         0.45  0.41 0.54  0.463 2.0
Wonder      0.67  0.13 0.55  0.450 1.1
Elation     0.61  0.16 0.48  0.523 1.1
Disgust     0.39  0.26 0.31  0.690 1.7
Anger       0.79 -0.15 0.54  0.462 1.1
Sex         0.66 -0.08 0.40  0.605 1.0
Fear       -0.19  0.59 0.29  0.715 1.2
Subjection -0.43  0.60 0.31  0.689 1.8

                       MR1  MR2
SS loadings           3.21 2.90
Proportion Var        0.29 0.26
Cumulative Var        0.29 0.55
Proportion Explained  0.53 0.47
Cumulative Proportion 0.53 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.46
MR2 0.46 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  55  and the objective function was  29.97
The degrees of freedom for the model are 34  and the objective function was  23.27 

The root mean square of the residuals (RMSR) is  0.07 
The df corrected root mean square of the residuals is  0.1 

Fit based upon off diagonal values = 0.97> #Smoothing first throws a warning that the matrix was improper, 
> #but produces a better solution 
> fa(cor.smooth(burt),2)  
Warning in cor.smooth(burt) :
  Matrix was not positive definite, smoothing was done
The estimated weights for the factor scores are probably incorrect.  Try a different factor extraction method.
Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate,  :
  An ultra-Heywood case was detected.  Examine the results carefully
Factor Analysis using method =  minres
Call: fa(r = cor.smooth(burt), nfactors = 2)
Standardized loadings (pattern matrix) based upon correlation matrix
             MR1   MR2   h2     u2 com
Sociality   0.68  0.50 1.02 -0.019 1.8
Sorrow      0.28  0.77 0.87  0.130 1.3
Tenderness  0.05  0.86 0.78  0.223 1.0
Joy         0.46  0.40 0.54  0.462 2.0
Wonder      0.68  0.12 0.55  0.451 1.1
Elation     0.61  0.15 0.48  0.523 1.1
Disgust     0.40  0.25 0.31  0.690 1.7
Anger       0.79 -0.16 0.53  0.465 1.1
Sex         0.67 -0.09 0.40  0.604 1.0
Fear       -0.19  0.60 0.29  0.711 1.2
Subjection -0.42  0.61 0.31  0.685 1.8

                       MR1  MR2
SS loadings           3.25 2.82
Proportion Var        0.30 0.26
Cumulative Var        0.30 0.55
Proportion Explained  0.54 0.46
Cumulative Proportion 0.54 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.45
MR2 0.45 1.00

Mean item complexity =  1.4
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  55  and the objective function was  29.97
The degrees of freedom for the model are 34  and the objective function was  23.22 

The root mean square of the residuals (RMSR) is  0.07 
The df corrected root mean square of the residuals is  0.09 

Fit based upon off diagonal values = 0.97> 
> #this next example is a correlation matrix from DeLeuw used as an example 
> #in Knol and ten Berge.  
> #the example is also used in the nearcor documentation
>  cat("pr is the example matrix used in Knol DL, ten Berge (1989)\n")
pr is the example matrix used in Knol DL, ten Berge (1989)
>  pr <- matrix(c(1,     0.477, 0.644, 0.478, 0.651, 0.826,
+ 		0.477, 1,     0.516, 0.233, 0.682, 0.75,
+ 		0.644, 0.516, 1,     0.599, 0.581, 0.742,
+ 		0.478, 0.233, 0.599, 1,     0.741, 0.8,
+ 		0.651, 0.682, 0.581, 0.741, 1,     0.798,
+ 		0.826, 0.75,  0.742, 0.8,   0.798, 1),
+ 	      nrow = 6, ncol = 6)
> 	      
> sm <- cor.smooth(pr)
Warning in cor.smooth(pr) :
  Matrix was not positive definite, smoothing was done
> resid <- pr - sm
> # several goodness of fit tests
> # from Knol and ten Berge
> tr(resid %*% t(resid)) /2
[1] 0.003520413
> 
> # from nearPD
> sum(resid^2)/2
[1] 0.003520413
> 
> 
> 
> 
> cleanEx()
> nameEx("cor.wt")
> ### * cor.wt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cor.wt
> ### Title: The sample size weighted correlation may be used in correlating
> ###   aggregated data
> ### Aliases: cor.wt
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> means.by.age <- statsBy(sat.act,"age")
> wt.cors <- cor.wt(means.by.age)
> lowerMat(wt.cors$r)  #show the weighted correlations
          gendr edctn age   ACT   SATV  SATQ 
gender     1.00                              
education  0.12  1.00                        
age       -0.09  0.67  1.00                  
ACT       -0.24  0.32  0.39  1.00            
SATV      -0.15 -0.23 -0.16  0.39  1.00      
SATQ      -0.33 -0.23 -0.12  0.35  0.66  1.00
> unwt <- lowerCor(means.by.age$mean)
          gendr edctn age   ACT   SATV  SATQ 
gender     1.00                              
education  0.19  1.00                        
age        0.14  0.69  1.00                  
ACT       -0.22  0.07  0.21  1.00            
SATV       0.09 -0.14 -0.03  0.37  1.00      
SATQ      -0.01 -0.10  0.00  0.44  0.73  1.00
> mixed <- lowerUpper(unwt,wt.cors$r)  #combine both results
> cor.plot(mixed,TRUE,main="weighted versus unweighted correlations")
> diff <- lowerUpper(unwt,wt.cors$r,TRUE)
> cor.plot(diff,TRUE,main="differences of weighted versus unweighted correlations")
> 
> 
> 
> cleanEx()
> nameEx("corFiml")
> ### * corFiml
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: corFiml
> ### Title: Find a Full Information Maximum Likelihood (FIML) correlation or
> ###   covariance matrix from a data matrix with missing data
> ### Aliases: corFiml
> ### Keywords: multivariate models
> 
> ### ** Examples
>  
> rML <- corFiml(bfi[20:27])
> rpw <- cor(bfi[20:27],use="pairwise") 
> round(rML - rpw,3)
              N5     O1     O2     O3     O4     O5 gender education
N5         0.000  0.000 -0.001 -0.001  0.000 -0.001 -0.002     0.001
O1         0.000  0.000  0.001  0.000 -0.001  0.001  0.000     0.001
O2        -0.001  0.001  0.000  0.001  0.000 -0.002  0.000     0.001
O3        -0.001  0.000  0.001  0.000 -0.001  0.000 -0.002     0.001
O4         0.000 -0.001  0.000 -0.001  0.000  0.000  0.000     0.001
O5        -0.001  0.001 -0.002  0.000  0.000  0.000  0.000     0.001
gender    -0.002  0.000  0.000 -0.002  0.000  0.000  0.000     0.001
education  0.001  0.001  0.001  0.001  0.001  0.001  0.001     0.000
> mp <- corFiml(bfi[20:27],show=TRUE)
> mp
        N5    O1   O2    O3    O4    O5 gender education
2489  TRUE  TRUE TRUE  TRUE  TRUE  TRUE   TRUE      TRUE
214   TRUE  TRUE TRUE  TRUE  TRUE  TRUE   TRUE     FALSE
22   FALSE  TRUE TRUE  TRUE  TRUE  TRUE   TRUE      TRUE
19    TRUE  TRUE TRUE FALSE  TRUE  TRUE   TRUE      TRUE
18    TRUE FALSE TRUE  TRUE  TRUE  TRUE   TRUE      TRUE
13    TRUE  TRUE TRUE  TRUE  TRUE FALSE   TRUE      TRUE
13    TRUE  TRUE TRUE  TRUE FALSE  TRUE   TRUE      TRUE
3     TRUE  TRUE TRUE FALSE  TRUE  TRUE   TRUE     FALSE
2    FALSE FALSE TRUE FALSE  TRUE FALSE   TRUE      TRUE
2    FALSE FALSE TRUE FALSE  TRUE FALSE   TRUE     FALSE
1     TRUE  TRUE TRUE  TRUE  TRUE FALSE   TRUE     FALSE
1     TRUE  TRUE TRUE  TRUE FALSE  TRUE   TRUE     FALSE
1    FALSE  TRUE TRUE  TRUE  TRUE  TRUE   TRUE     FALSE
1    FALSE  TRUE TRUE FALSE  TRUE FALSE   TRUE      TRUE
1    FALSE  TRUE TRUE FALSE  TRUE FALSE   TRUE     FALSE
> 
> 
> 
> cleanEx()
> nameEx("corr.test")
> ### * corr.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: corr.test
> ### Title: Find the correlations, sample sizes, and probability values
> ###   between elements of a matrix or data.frame.
> ### Aliases: corr.test corr.p
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> ct <- corr.test(attitude)  #find the correlations and give the probabilities
> ct #show the results
Call:corr.test(x = attitude)
Correlation matrix 
           rating complaints privileges learning raises critical advance
rating       1.00       0.83       0.43     0.62   0.59     0.16    0.16
complaints   0.83       1.00       0.56     0.60   0.67     0.19    0.22
privileges   0.43       0.56       1.00     0.49   0.45     0.15    0.34
learning     0.62       0.60       0.49     1.00   0.64     0.12    0.53
raises       0.59       0.67       0.45     0.64   1.00     0.38    0.57
critical     0.16       0.19       0.15     0.12   0.38     1.00    0.28
advance      0.16       0.22       0.34     0.53   0.57     0.28    1.00
Sample Size 
[1] 30
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
           rating complaints privileges learning raises critical advance
rating       0.00       0.00       0.19     0.00   0.01     1.00    1.00
complaints   0.00       0.00       0.02     0.01   0.00     1.00    1.00
privileges   0.02       0.00       0.00     0.07   0.15     1.00    0.51
learning     0.00       0.00       0.01     0.00   0.00     1.00    0.03
raises       0.00       0.00       0.01     0.00   0.00     0.36    0.01
critical     0.41       0.32       0.44     0.54   0.04     0.00    0.90
advance      0.41       0.23       0.06     0.00   0.00     0.13    0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
> corr.test(attitude[1:3],attitude[4:6]) #reports all values corrected for multiple tests
Call:corr.test(x = attitude[1:3], y = attitude[4:6])
Correlation matrix 
           learning raises critical
rating         0.62   0.59     0.16
complaints     0.60   0.67     0.19
privileges     0.49   0.45     0.15
Sample Size 
[1] 30
Probability values  adjusted for multiple tests. 
           learning raises critical
rating         0.00   0.00     0.96
complaints     0.00   0.00     0.96
privileges     0.03   0.05     0.96

 To see confidence intervals of the correlations, print with the short=FALSE option
> 
> #corr.test(sat.act[1:3],sat.act[4:6],adjust="none")  #don't adjust the probabilities
> 
> #take correlations and show the probabilities as well as the confidence intervals
> print(corr.p(cor(attitude[1:4]),30),short=FALSE)  
Call:corr.p(r = cor(attitude[1:4]), n = 30)
Correlation matrix 
           rating complaints privileges learning
rating       1.00       0.83       0.43     0.62
complaints   0.83       1.00       0.56     0.60
privileges   0.43       0.56       1.00     0.49
learning     0.62       0.60       0.49     1.00
Sample Size 
[1] 30
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
           rating complaints privileges learning
rating       0.00          0       0.02     0.00
complaints   0.00          0       0.00     0.00
privileges   0.02          0       0.00     0.01
learning     0.00          0       0.01     0.00

 To see confidence intervals of the correlations, print with the short=FALSE option

 Confidence intervals based upon normal theory.  To get bootstrapped values, try cor.ci
            lower    r upper    p
ratng-cmpln  0.66 0.83  0.91 0.00
ratng-prvlg  0.08 0.43  0.68 0.02
cmpln-prvlg  0.34 0.62  0.80 0.00
ratng-lrnng  0.25 0.56  0.76 0.00
cmpln-lrnng  0.30 0.60  0.79 0.00
prvlg-lrnng  0.16 0.49  0.72 0.01
> 
> #don't adjust the probabilities
> print(corr.test(sat.act[1:3],sat.act[4:6],adjust="none"),short=FALSE)  
Call:corr.test(x = sat.act[1:3], y = sat.act[4:6], adjust = "none")
Correlation matrix 
            ACT  SATV  SATQ
gender    -0.04 -0.02 -0.17
education  0.15  0.05  0.03
age        0.11 -0.04 -0.03
Sample Size 
          ACT SATV SATQ
gender    700  700  687
education 700  700  687
age       700  700  687
           ACT SATV SATQ
gender    0.33 0.62 0.00
education 0.00 0.22 0.36
age       0.00 0.26 0.37

 To see confidence intervals of the correlations, print with the short=FALSE option

 Confidence intervals based upon normal theory.  To get bootstrapped values, try cor.ci
           lower     r upper    p
gendr-ACT  -0.11 -0.04  0.04 0.33
edctn-ACT   0.08  0.15  0.23 0.00
age-ACT     0.04  0.11  0.18 0.00
gendr-SATV -0.09 -0.02  0.06 0.62
edctn-SATV -0.03  0.05  0.12 0.22
age-SATV   -0.12 -0.04  0.03 0.26
gendr-SATQ -0.24 -0.17 -0.09 0.00
edctn-SATQ -0.04  0.03  0.11 0.36
age-SATQ   -0.11 -0.03  0.04 0.37
> 
> 
> 
> 
> cleanEx()
> nameEx("correct.cor")
> ### * correct.cor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: correct.cor
> ### Title: Find dis-attenuated correlations given correlations and
> ###   reliabilities
> ### Aliases: correct.cor
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> 
> # attitude from the datasets package
> #example 1 is a rather clunky way of doing things
> 
> a1 <- attitude[,c(1:3)]
> a2 <- attitude[,c(4:7)]
> x1 <- rowSums(a1)  #find the sum of the first 3 attitudes
> x2 <- rowSums(a2)   #find the sum of the last 4 attitudes
> alpha1 <- alpha(a1)
> alpha2 <- alpha(a2)
> x <- matrix(c(x1,x2),ncol=2)
> x.cor <- cor(x)
> alpha <- c(alpha1$total$raw_alpha,alpha2$total$raw_alpha)
> round(correct.cor(x.cor,alpha),2)
     [,1] [,2]
[1,] 0.82 0.78
[2,] 0.61 0.75
> #
> #much better - although uses standardized alpha 
> clusters <- matrix(c(rep(1,3),rep(0,7),rep(1,4)),ncol=2)
> cluster.loadings(clusters,cor(attitude))
Call: cluster.loadings(keys = clusters, r.mat = cor(attitude))

(Standardized) Alpha:
[1] 0.82 0.74

(Standardized) G6*:
[1] 0.83 0.78

Average item correlation:
[1] 0.60 0.42

Number of items:
[1] 3 4

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
     [,1] [,2]
[1,] 0.82 0.77
[2,] 0.60 0.74

Item by scale intercorrelations
 corrected for item overlap and scale reliability
           [,1] [,2]
rating     0.85 0.57
complaints 0.92 0.63
privileges 0.58 0.54
learning   0.73 0.72
raises     0.73 0.85
critical   0.21 0.36
advance    0.31 0.72
> # or 
> clusters <- matrix(c(rep(1,3),rep(0,7),rep(1,4)),ncol=2)
> cluster.cor(clusters,cor(attitude))
Call: cluster.cor(keys = clusters, r.mat = cor(attitude))

(Standardized) Alpha:
[1] 0.82 0.74

(Standardized) G6*:
[1] 0.83 0.78

Average item correlation:
[1] 0.60 0.42

Number of items:
[1] 3 4

Signal to Noise ratio based upon average r and n 
[1] 4.6 2.9

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
     [,1] [,2]
[1,] 0.82 0.77
[2,] 0.60 0.74
> #
> #best
> scores <- scoreItems(matrix(c(rep(1,3),rep(0,7),rep(1,4)),ncol=2),attitude)
> scores$corrected
          [,1]      [,2]
[1,] 0.8222013 0.7798835
[2,] 0.6104550 0.7451947
> 
> #However, to do the more general case of correcting correlations for reliabilty
> #corrected <- cor2cov(x.cor,1/alpha)
> #diag(corrected) <- 1
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("cortest.bartlett")
> ### * cortest.bartlett
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cortest.bartlett
> ### Title: Bartlett's test that a correlation matrix is an identity matrix
> ### Aliases: cortest.bartlett
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> set.seed(42)   
> x <- matrix(rnorm(1000),ncol=10)
> r <- cor(x)
> cortest.bartlett(r)      #random data don't differ from an identity matrix
Warning in cortest.bartlett(r) : n not specified, 100 used
$chisq
[1] 33.78894

$p.value
[1] 0.8897656

$df
[1] 45

> data(bfi)
> cortest.bartlett(bfi[1:200,1:10])    #not an identity matrix
R was not square, finding R from data
$chisq
[1] 457.457

$p.value
[1] 1.136106e-69

$df
[1] 45

> f3 <- fa(Thurstone,3)
> f3r <- f3$resid
> cortest.bartlett(f3r,n=213,diag=FALSE)  #incorrect
$chisq
[1] 2203.356

$p.value
[1] 0

$df
[1] 36

> 
> cortest.bartlett(f3r,n=213,diag=TRUE)  #correct (by default)
$chisq
[1] 0.256498

$p.value
[1] 1

$df
[1] 36

> 
> 
> 
> 
> cleanEx()
> nameEx("cortest.mat")
> ### * cortest.mat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cortest.mat
> ### Title: Chi square tests of whether a single matrix is an identity
> ###   matrix, or a pair of matrices are equal.
> ### Aliases: cortest.normal cortest.mat cortest.jennrich cortest
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> x <- matrix(rnorm(1000),ncol=10)
> cortest.normal(x)  #just test if this matrix is an identity
R1 was not square, finding R from data
Tests of correlation matrices 
Call:cortest.normal(R1 = x)
 Chi Square value 40.03  with df =  45   with probability < 0.68 
> x <- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)
> y <- sim.congeneric(loads =c(.9,.8,.7,.6,.5),N=1000,short=FALSE)
> cortest.normal(x$r,y$r,n1=1000,n2=1000) #The Steiger test
Tests of correlation matrices 
Call:cortest.normal(R1 = x$r, R2 = y$r, n1 = 1000, n2 = 1000)
 Chi Square value 6.55  with df =  10   with probability < 0.77 
> cortest.jennrich(x$r,y$r,n1=100,n2=1000) # The Jennrich test
$chi2
[1] 3.519158

$prob
[1] 0.9664439

> cortest.mat(x$r,y$r,n1=1000,n2=1000)   #twice the degrees of freedom as the Jennrich
Tests of correlation matrices 
Call:cortest.mat(R1 = x$r, R2 = y$r, n1 = 1000, n2 = 1000)
 Chi Square value 31.22  with df =  20   with probability < 0.052 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("cosinor")
> ### * cosinor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cosinor
> ### Title: Functions for analysis of circadian or diurnal data
> ### Aliases: cosinor circadian.phase cosinor.plot cosinor.period
> ###   circadian.mean circadian.sd circadian.cor circadian.linear.cor
> ###   circadian.stats circadian.F circadian.reliability circular.mean
> ###   circular.cor
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> time <- seq(1:24) #create a 24 hour time
> pure <- matrix(time,24,18) 
> colnames(pure) <- paste0("H",1:18)
> pure <- data.frame(time,cos((pure - col(pure))*pi/12)*3 + 3)
>     #18 different phases but scaled to 0-6  match mood data
> matplot(pure[-1],type="l",main="Pure circadian arousal rhythms",
+     xlab="time of day",ylab="Arousal") 
> op <- par(mfrow=c(2,2))
>  cosinor.plot(1,3,pure)
>  cosinor.plot(1,5,pure)
>  cosinor.plot(1,8,pure)
>  cosinor.plot(1,12,pure)
> 
> p <- cosinor(pure) #find the acrophases (should match the input)
> 
> #now, test finding the acrophases for  different subjects on 3 variables
> #They should be the first 3, second 3, etc. acrophases of pure
> pp <- matrix(NA,nrow=6*24,ncol=4)
> pure <- as.matrix(pure)
> pp[,1] <- rep(pure[,1],6)
> pp[1:24,2:4] <- pure[1:24,2:4] 
> pp[25:48,2:4] <- pure[1:24,5:7] *2   #to test different variances
> pp[49:72,2:4] <- pure[1:24,8:10] *3
> pp[73:96,2:4] <- pure[1:24,11:13]
> pp[97:120,2:4] <- pure[1:24,14:16]
> pp[121:144,2:4] <- pure[1:24,17:19]
> pure.df <- data.frame(ID = rep(1:6,each=24),pp)
> colnames(pure.df) <- c("ID","Time",paste0("V",1:3))
> cosinor("Time",3:5,"ID",pure.df)
  V1.phase V2.phase V3.phase V1.fit V2.fit V3.fit V1.amp V2.amp V3.amp    V1.sd
1        1        2        3      1      1      1      1      1      1 2.166945
2        4        5        6      1      1      1      1      1      1 4.333891
3        7        8        9      1      1      1      1      1      1 6.500836
4       10       11       12      1      1      1      1      1      1 2.166945
5       13       14       15      1      1      1      1      1      1 2.166945
6       16       17       18      1      1      1      1      1      1 2.166945
     V2.sd    V3.sd V1.mean V2.mean V3.mean V1.intercept V2.intercept
1 2.166945 2.166945       3       3       3    5.0931085    4.8766297
2 4.333891 4.333891       6       6       6    8.1669454    7.1216935
3 6.500836 6.500836       9       9       9    7.3174598    5.7495820
4 2.166945 2.166945       3       3       3    1.1233703    0.9068915
5 2.166945 2.166945       3       3       3    0.9068915    1.1233703
6 2.166945 2.166945       3       3       3    1.9165273    2.4391533
  V3.intercept
1    4.5322618
2    6.0000000
3    4.4032147
4    0.8330546
5    1.4677382
6    3.0000000
> 
> op <- par(mfrow=c(2,2))
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="1")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="2")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="3")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="4")
>  
>  #now, show those in one panel as spagetti plots
> op <- par(mfrow=c(1,1))
> cosinor.plot(2,3,pure.df,IDloc=1,ID="1",multi=TRUE,ylim=c(0,20),ylab="Modeled")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="2",multi=TRUE,add=TRUE,lty="dotdash")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="3",multi=TRUE,add=TRUE,lty="dashed")
>  cosinor.plot(2,3,pure.df,IDloc=1,ID="4",multi=TRUE,add=TRUE,lty="dotted")
> 
> set.seed(42)   #what else?
> noisy <- pure
> noisy[,2:19]<- noisy[,2:19] + rnorm(24*18,0,.2)
> 
> n <- cosinor(time,noisy) #add a bit of noise
> 
> small.pure <- pure[c(8,11,14,17,20,23),]
> small.noisy <- noisy[c(8,11,14,17,20,23),]
> small.time <- c(8,11,14,17,20,23)
> 
> cosinor.plot(1,3,small.pure,multi=TRUE)
> cosinor.plot(1,3,small.noisy,multi=TRUE,add=TRUE,lty="dashed")
> 
>          
> # sp <- cosinor(small.pure)
> # spo <- cosinor(small.pure,opti=TRUE) #iterative fit
> # sn <- cosinor(small.noisy) #linear
> # sno <- cosinor(small.noisy,opti=TRUE) #iterative
> # sum.df <- data.frame(pure=p,noisy = n, small=sp,small.noise = sn, 
> #         small.opt=spo,small.noise.opt=sno)
> # round(sum.df,2)
> # round(circadian.cor(sum.df[,c(1,3,5,7,9,11)]),2)  #compare alternatives 
> # 
> # #now, lets form three "subjects" and show how the grouping variable works
> # mixed.df <- rbind(small.pure,small.noisy,noisy)
> # mixed.df <- data.frame(ID=c(rep(1,6),rep(2,6),rep(3,24)),
> #           time=c(rep(c(8,11,14,17,20,23),2),1:24),mixed.df)
> # group.df <- cosinor(angle="time",x=2:20,code="ID",data=mixed.df)
> # round(group.df,2)  #compare these values to the sp,sn,and n values done separately
> 
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("count.pairwise")
> ### * count.pairwise
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: count.pairwise
> ### Title: Count number of pairwise cases for a data set with missing (NA)
> ###   data.
> ### Aliases: count.pairwise pairwiseDescribe
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> ## Not run: 
> ##D x <- matrix(rnorm(1000),ncol=6)
> ##D y <- matrix(rnorm(500),ncol=3)
> ##D x[x < 0] <- NA
> ##D y[y > 1] <- NA
> ##D 
> ##D count.pairwise(x)
> ##D count.pairwise(y)
> ##D count.pairwise(x,y)
> ##D count.pairwise(x,diagonal=FALSE)
> ##D pairwiseDescribe(x,quant=c(.1,.25,.5,.75,.9))
> ## End(Not run)
> 
>     
> 
> 
> 
> cleanEx()
> nameEx("cta")
> ### * cta
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cta
> ### Title: Simulate the C(ues) T(endency) A(ction) model of motivation
> ### Aliases: cta cta.15
> ### Keywords: models
> 
> ### ** Examples
> 
> #not run 
> #cta()   #default values, running over time 
> #cta(type="state") #default values, in a state space  of tendency 1 versus tendency 2
> #these next are examples without graphic output
> #not run
> #two introverts
> #c2i <- c(.95,1.05)
> #cta(n=2,t=10000,cues=c2i,type="none")
> #two extraverts
> #c2e <- c(3.95,4.05)
> #cta(n=2,t=10000,cues=c2e,type="none")
> #three introverts
> #c3i <-  c(.95,1,1.05)
> #cta(3,t=10000,cues=c3i,type="none")
> #three extraverts
> #c3i <- c(3.95,4, 4.05)
> #cta(3,10000,c3i,type="none")
> #mixed
> #c3 <- c(1,2.5,4)
> #cta(3,10000,c3,type="none")
> 
> 
> 
> cleanEx()
> nameEx("cubits")
> ### * cubits
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cubits
> ### Title: Galton's example of the relationship between height and 'cubit'
> ###   or forearm length
> ### Aliases: cubits
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(cubits)
> cubits
   16.5 16.75 17.25 17.75 18.25 18.75 19.25 19.75
71    0     0     0     1     3     4    15     7
70    0     0     0     1     5    13    11     0
69    0     1     1     2    25    15     6     0
68    0     1     3     7    14     7     4     2
67    0     1     7    15    28     8     2     0
66    0     1     7    18    15     6     0     0
65    0     4    10    12     8     2     0     0
64    0     5    11     2     3     0     0     0
63    9    12    10     3     1     0     0     0
> heights <- table2df(cubits,labs = c("height","cubit"))
> ellipses(heights,n=1,main="Galton's co-relation data set")
> ellipses(jitter(heights$height,3),jitter(heights$cubit,3),pch=".",
+      main="Galton's co-relation data set",xlab="height",
+      ylab="Forearm (cubit)") #add in some noise to see the points
> pairs.panels(heights,jiggle=TRUE,main="Galton's cubits data set")
> 
> 
> 
> cleanEx()
> nameEx("cushny")
> ### * cushny
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cushny
> ### Title: A data set from Cushny and Peebles (1905) on the effect of three
> ###   drugs on hours of sleep, used by Student (1908)
> ### Aliases: cushny
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(cushny)
> with(cushny, t.test(drug1,drug2L,paired=TRUE)) #within subjects 

	Paired t-test

data:  drug1 and drug2L
t = -4.0621, df = 9, p-value = 0.002833
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.4598858 -0.7001142
sample estimates:
mean of the differences 
                  -1.58 

> 
> error.bars(cushny[1:4],within=TRUE,ylab="Hours of sleep",xlab="Drug condition", 
+        main="95% confidence of within subject effects")
> 
> 
> 
> cleanEx()
> nameEx("densityBy")
> ### * densityBy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: densityBy
> ### Title: Create a 'violin plot' or density plot of the distribution of a
> ###   set of variables
> ### Aliases: densityBy violinBy
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> densityBy(bfi[1:5])
> #not run
> #violinBy(bfi[1:5],grp=bfi$gender,grp.name=c("M","F"))
> #densityBy(sat.act[5:6],sat.act$education,col=rainbow(6))
>   
> 
> 
> 
> cleanEx()
> nameEx("deprecated")
> ### * deprecated
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.poly
> ### Title: Deprecated Exploratory Factor analysis functions.  Please use fa
> ### Aliases: factor.pa factor.minres factor.wls fa.poly
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #using the Harman 24 mental tests, compare a principal factor with a principal components solution
> pc <- principal(Harman74.cor$cov,4,rotate="varimax")   #principal components
> pa <- fa(Harman74.cor$cov,4,fm="pa" ,rotate="varimax")  #principal axis 
> uls <- fa(Harman74.cor$cov,4,rotate="varimax")          #unweighted least squares is minres
> wls <- fa(Harman74.cor$cov,4,fm="wls")       #weighted least squares
> 
> #to show the loadings sorted by absolute value
> print(uls,sort=TRUE)
Factor Analysis using method =  minres
Call: fa(r = Harman74.cor$cov, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       item  MR1   MR3   MR2  MR4   h2   u2 com
SentenceCompletion        7 0.81  0.19  0.15 0.07 0.73 0.27 1.2
WordMeaning               9 0.81  0.20  0.05 0.22 0.74 0.26 1.3
PargraphComprehension     6 0.76  0.21  0.07 0.23 0.68 0.32 1.4
GeneralInformation        5 0.73  0.19  0.22 0.14 0.64 0.36 1.4
WordClassification        8 0.57  0.34  0.23 0.14 0.51 0.49 2.2
VisualPerception          1 0.15  0.68  0.20 0.15 0.55 0.45 1.4
PaperFormBoard            3 0.15  0.55 -0.01 0.11 0.34 0.66 1.2
Flags                     4 0.23  0.53  0.09 0.07 0.35 0.65 1.5
SeriesCompletion         23 0.37  0.52  0.23 0.22 0.51 0.49 2.7
Cubes                     2 0.11  0.45  0.08 0.08 0.23 0.77 1.3
Deduction                20 0.38  0.42  0.10 0.29 0.42 0.58 2.9
ProblemReasoning         22 0.37  0.41  0.13 0.29 0.40 0.60 3.0
Addition                 10 0.17 -0.11  0.82 0.16 0.74 0.26 1.2
CountingDots             12 0.02  0.20  0.71 0.09 0.55 0.45 1.2
StraightCurvedCapitals   13 0.18  0.42  0.54 0.08 0.51 0.49 2.2
Code                     11 0.18  0.11  0.54 0.37 0.47 0.53 2.1
ArithmeticProblems       24 0.36  0.19  0.49 0.29 0.49 0.51 2.9
NumericalPuzzles         21 0.18  0.40  0.43 0.21 0.42 0.58 2.8
ObjectNumber             17 0.14  0.06  0.22 0.58 0.41 0.59 1.4
WordRecognition          14 0.21  0.05  0.08 0.56 0.36 0.64 1.3
NumberRecognition        15 0.12  0.12  0.08 0.52 0.31 0.69 1.3
FigureRecognition        16 0.07  0.42  0.06 0.52 0.45 0.55 2.0
NumberFigure             18 0.02  0.31  0.34 0.45 0.41 0.59 2.7
FigureWord               19 0.15  0.25  0.18 0.35 0.23 0.77 2.8

                       MR1  MR3  MR2  MR4
SS loadings           3.64 2.93 2.67 2.23
Proportion Var        0.15 0.12 0.11 0.09
Cumulative Var        0.15 0.27 0.38 0.48
Proportion Explained  0.32 0.26 0.23 0.19
Cumulative Proportion 0.32 0.57 0.81 1.00

Mean item complexity =  1.9
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  276  and the objective function was  11.44
The degrees of freedom for the model are 186  and the objective function was  1.72 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.05 

Fit based upon off diagonal values = 0.98
Measures of factor score adequacy             
                                                MR1  MR3  MR2  MR4
Correlation of scores with factors             0.93 0.87 0.91 0.82
Multiple R square of scores with factors       0.87 0.76 0.83 0.68
Minimum correlation of possible factor scores  0.74 0.52 0.65 0.36
> 
> #then compare with a maximum likelihood solution using factanal
> mle <- factanal(covmat=Harman74.cor$cov,factors=4)
> factor.congruence(list(mle,pa,pc,uls,wls))
        Factor1 Factor2 Factor3 Factor4  PA1  PA3  PA2  PA4  RC1  RC3  RC2  RC4
Factor1    1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
Factor2    0.61    1.00    0.50    0.61 0.61 1.00 0.50 0.60 0.60 0.99 0.49 0.52
Factor3    0.46    0.50    1.00    0.57 0.46 0.50 1.00 0.56 0.45 0.44 1.00 0.48
Factor4    0.56    0.61    0.57    1.00 0.56 0.62 0.58 1.00 0.55 0.55 0.56 0.99
PA1        1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
PA3        0.61    1.00    0.50    0.62 0.61 1.00 0.50 0.61 0.61 0.99 0.50 0.53
PA2        0.46    0.50    1.00    0.58 0.46 0.50 1.00 0.57 0.46 0.44 1.00 0.49
PA4        0.55    0.60    0.56    1.00 0.55 0.61 0.57 1.00 0.54 0.54 0.55 0.99
RC1        1.00    0.60    0.45    0.55 1.00 0.61 0.46 0.54 1.00 0.53 0.43 0.46
RC3        0.54    0.99    0.44    0.55 0.54 0.99 0.44 0.54 0.53 1.00 0.43 0.47
RC2        0.44    0.49    1.00    0.56 0.44 0.50 1.00 0.55 0.43 0.43 1.00 0.47
RC4        0.47    0.52    0.48    0.99 0.47 0.53 0.49 0.99 0.46 0.47 0.47 1.00
MR1        1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
MR3        0.61    1.00    0.50    0.62 0.61 1.00 0.50 0.61 0.61 0.99 0.50 0.53
MR2        0.46    0.50    1.00    0.58 0.46 0.50 1.00 0.57 0.46 0.44 1.00 0.49
MR4        0.55    0.60    0.56    1.00 0.55 0.61 0.57 1.00 0.54 0.54 0.55 0.99
WLS1       0.98    0.47    0.30    0.40 0.98 0.48 0.30 0.39 0.98 0.41 0.28 0.32
WLS3       0.36    0.95    0.41    0.41 0.36 0.95 0.41 0.39 0.35 0.97 0.41 0.32
WLS2       0.23    0.22    0.95    0.36 0.23 0.22 0.95 0.35 0.22 0.16 0.95 0.28
WLS4       0.28    0.40    0.36    0.94 0.28 0.41 0.37 0.94 0.27 0.36 0.35 0.97
         MR1  MR3  MR2  MR4 WLS1 WLS3 WLS2 WLS4
Factor1 1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
Factor2 0.61 1.00 0.50 0.60 0.47 0.95 0.22 0.40
Factor3 0.46 0.50 1.00 0.56 0.30 0.41 0.95 0.36
Factor4 0.56 0.62 0.58 1.00 0.40 0.41 0.36 0.94
PA1     1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
PA3     0.61 1.00 0.50 0.61 0.48 0.95 0.22 0.41
PA2     0.46 0.50 1.00 0.57 0.30 0.41 0.95 0.37
PA4     0.55 0.61 0.57 1.00 0.39 0.39 0.35 0.94
RC1     1.00 0.61 0.46 0.54 0.98 0.35 0.22 0.27
RC3     0.54 0.99 0.44 0.54 0.41 0.97 0.16 0.36
RC2     0.44 0.50 1.00 0.55 0.28 0.41 0.95 0.35
RC4     0.47 0.53 0.49 0.99 0.32 0.32 0.28 0.97
MR1     1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
MR3     0.61 1.00 0.50 0.61 0.48 0.95 0.22 0.41
MR2     0.46 0.50 1.00 0.57 0.30 0.41 0.95 0.37
MR4     0.55 0.61 0.57 1.00 0.39 0.39 0.35 0.94
WLS1    0.98 0.48 0.30 0.39 1.00 0.22 0.09 0.13
WLS3    0.36 0.95 0.41 0.39 0.22 1.00 0.17 0.23
WLS2    0.23 0.22 0.95 0.35 0.09 0.17 1.00 0.20
WLS4    0.28 0.41 0.37 0.94 0.13 0.23 0.20 1.00
> #note that the order of factors and the sign of some of factors may differ 
> 
> #finally, compare the unrotated factor, ml, uls, and  wls solutions
> wls <- fa(Harman74.cor$cov,4,rotate="none",fm="wls")
> pa <- fa(Harman74.cor$cov,4,rotate="none",fm="pa")
> minres <-  factanal(factors=4,covmat=Harman74.cor$cov,rotation="none")
> mle <- fa(Harman74.cor$cov,4,rotate="none",fm="mle")
> uls <- fa(Harman74.cor$cov,4,rotate="none",fm="uls")
> factor.congruence(list(minres,mle,pa,wls,uls))
        Factor1 Factor2 Factor3 Factor4   ML1   ML2  ML3   ML4  PA1   PA2   PA3
Factor1    1.00    0.11    0.25    0.06  1.00  0.11 0.25  0.06 1.00 -0.04 -0.05
Factor2    0.11    1.00    0.06    0.07  0.11  1.00 0.06  0.07 0.14  0.98 -0.08
Factor3    0.25    0.06    1.00    0.01  0.25  0.06 1.00  0.01 0.30  0.10  0.95
Factor4    0.06    0.07    0.01    1.00  0.06  0.07 0.01  1.00 0.07  0.13 -0.04
ML1        1.00    0.11    0.25    0.06  1.00  0.11 0.25  0.06 1.00 -0.04 -0.05
ML2        0.11    1.00    0.06    0.07  0.11  1.00 0.06  0.07 0.14  0.98 -0.08
ML3        0.25    0.06    1.00    0.01  0.25  0.06 1.00  0.01 0.30  0.10  0.95
ML4        0.06    0.07    0.01    1.00  0.06  0.07 0.01  1.00 0.07  0.13 -0.04
PA1        1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
PA2       -0.04    0.98    0.10    0.13 -0.04  0.98 0.10  0.13 0.00  1.00  0.00
PA3       -0.05   -0.08    0.95   -0.04 -0.05 -0.08 0.95 -0.04 0.00  0.00  1.00
PA4       -0.01   -0.08    0.02    0.99 -0.01 -0.08 0.02  0.99 0.00  0.00  0.00
WLS1       1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
WLS2      -0.04    0.98    0.09    0.13 -0.04  0.98 0.09  0.13 0.00  1.00 -0.01
WLS3      -0.05   -0.07    0.95   -0.04 -0.05 -0.07 0.95 -0.04 0.00  0.01  1.00
WLS4      -0.01   -0.07    0.02    0.99 -0.01 -0.07 0.02  0.99 0.00  0.01  0.00
ULS1       1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
ULS2      -0.04    0.98    0.09    0.13 -0.04  0.98 0.09  0.13 0.00  1.00  0.00
ULS3      -0.05   -0.07    0.95   -0.04 -0.05 -0.07 0.95 -0.04 0.00  0.00  1.00
ULS4      -0.01   -0.08    0.02    0.99 -0.01 -0.08 0.02  0.99 0.00  0.00  0.00
          PA4 WLS1  WLS2  WLS3  WLS4 ULS1  ULS2  ULS3  ULS4
Factor1 -0.01 1.00 -0.04 -0.05 -0.01 1.00 -0.04 -0.05 -0.01
Factor2 -0.08 0.14  0.98 -0.07 -0.07 0.14  0.98 -0.07 -0.08
Factor3  0.02 0.30  0.09  0.95  0.02 0.30  0.09  0.95  0.02
Factor4  0.99 0.07  0.13 -0.04  0.99 0.07  0.13 -0.04  0.99
ML1     -0.01 1.00 -0.04 -0.05 -0.01 1.00 -0.04 -0.05 -0.01
ML2     -0.08 0.14  0.98 -0.07 -0.07 0.14  0.98 -0.07 -0.08
ML3      0.02 0.30  0.09  0.95  0.02 0.30  0.09  0.95  0.02
ML4      0.99 0.07  0.13 -0.04  0.99 0.07  0.13 -0.04  0.99
PA1      0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
PA2      0.00 0.00  1.00  0.01  0.01 0.00  1.00  0.00  0.00
PA3      0.00 0.00 -0.01  1.00  0.00 0.00  0.00  1.00  0.00
PA4      1.00 0.00 -0.01  0.00  1.00 0.00  0.00  0.00  1.00
WLS1     0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
WLS2    -0.01 0.00  1.00  0.00  0.00 0.00  1.00 -0.01 -0.01
WLS3     0.00 0.00  0.00  1.00  0.00 0.00  0.01  1.00  0.00
WLS4     1.00 0.00  0.00  0.00  1.00 0.00  0.01  0.00  1.00
ULS1     0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
ULS2     0.00 0.00  1.00  0.01  0.01 0.00  1.00  0.00  0.00
ULS3     0.00 0.00 -0.01  1.00  0.00 0.00  0.00  1.00  0.00
ULS4     1.00 0.00 -0.01  0.00  1.00 0.00  0.00  0.00  1.00
> #in particular, note the similarity of the mle and min res solutions
> #note that the order of factors and the sign of some of factors may differ 
> 
> 
> 
> #an example of where the ML and PA and MR models differ is found in Thurstone.33.
> #compare the first two factors with the 3 factor solution 
> Thurstone.33 <- as.matrix(Thurstone.33)
> mle2 <- fa(Thurstone.33,2,rotate="none",fm="mle")
> mle3 <- fa(Thurstone.33,3 ,rotate="none",fm="mle")
> pa2 <- fa(Thurstone.33,2,rotate="none",fm="pa")
> pa3 <- fa(Thurstone.33,3,rotate="none",fm="pa")
> mr2 <- fa(Thurstone.33,2,rotate="none")
> mr3 <- fa(Thurstone.33,3,rotate="none")
> factor.congruence(list(mle2,mr2,pa2,mle3,pa3,mr3))
      ML1   ML2  MR1   MR2  PA1   PA2   ML1   ML2   ML3   PA1   PA2   PA3   MR1
ML1  1.00  0.16 1.00 -0.03 1.00 -0.03  0.98  0.88  0.16  1.00 -0.02  0.00  1.00
ML2  0.16  1.00 0.19  0.97 0.19  0.98  0.03  0.51 -0.79  0.18  0.97 -0.09  0.18
MR1  1.00  0.19 1.00  0.00 1.00  0.00  0.97  0.89  0.14  1.00  0.01  0.01  1.00
MR2 -0.03  0.97 0.00  1.00 0.00  1.00 -0.14  0.32 -0.87 -0.01  0.97 -0.17 -0.01
PA1  1.00  0.19 1.00  0.00 1.00  0.00  0.97  0.89  0.15  1.00  0.01  0.01  1.00
PA2 -0.03  0.98 0.00  1.00 0.00  1.00 -0.14  0.32 -0.87 -0.01  0.98 -0.17 -0.01
ML1  0.98  0.03 0.97 -0.14 0.97 -0.14  1.00  0.76  0.19  0.98 -0.17 -0.14  0.98
ML2  0.88  0.51 0.89  0.32 0.89  0.32  0.76  1.00 -0.01  0.88  0.39  0.26  0.88
ML3  0.16 -0.79 0.14 -0.87 0.15 -0.87  0.19 -0.01  1.00  0.15 -0.78  0.61  0.15
PA1  1.00  0.18 1.00 -0.01 1.00 -0.01  0.98  0.88  0.15  1.00  0.00  0.00  1.00
PA2 -0.02  0.97 0.01  0.97 0.01  0.98 -0.17  0.39 -0.78  0.00  1.00  0.00  0.00
PA3  0.00 -0.09 0.01 -0.17 0.01 -0.17 -0.14  0.26  0.61  0.00  0.00  1.00  0.00
MR1  1.00  0.18 1.00 -0.01 1.00 -0.01  0.98  0.88  0.15  1.00  0.00  0.00  1.00
MR2 -0.02  0.97 0.01  0.96 0.01  0.97 -0.17  0.40 -0.75  0.00  1.00  0.04  0.00
MR3  0.01 -0.14 0.01 -0.22 0.02 -0.22 -0.13  0.24  0.64  0.00 -0.04  1.00  0.00
      MR2   MR3
ML1 -0.02  0.01
ML2  0.97 -0.14
MR1  0.01  0.01
MR2  0.96 -0.22
PA1  0.01  0.02
PA2  0.97 -0.22
ML1 -0.17 -0.13
ML2  0.40  0.24
ML3 -0.75  0.64
PA1  0.00  0.00
PA2  1.00 -0.04
PA3  0.04  1.00
MR1  0.00  0.00
MR2  1.00  0.00
MR3  0.00  1.00
> 
> #f5 <- fa(bfi[1:25],5)
> #f5  #names are not in ascending numerical order (see note)
> #colnames(f5$loadings) <- paste("F",1:5,sep="")
> #f5
> 
> 
> 
> cleanEx()
> nameEx("describe")
> ### * describe
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: describe
> ### Title: Basic descriptive statistics useful for psychometrics
> ### Aliases: describe describeData
> ### Keywords: multivariate models univar
> 
> ### ** Examples
> 
> data(sat.act)
> describe(sat.act)
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.59   9.50     22   23.86   5.93  13  65    52  1.64
ACT          4 700  28.55   4.82     29   28.84   4.45   3  36    33 -0.66
SATV         5 700 612.23 112.90    620  619.45 118.61 200 800   600 -0.64
SATQ         6 687 610.22 115.64    620  617.25 118.61 200 800   600 -0.59
          kurtosis   se
gender       -1.62 0.02
education    -0.07 0.05
age           2.42 0.36
ACT           0.53 0.18
SATV          0.33 4.27
SATQ         -0.02 4.41
> 
> describe(sat.act,skew=FALSE)
          vars   n   mean     sd min max range   se
gender       1 700   1.65   0.48   1   2     1 0.02
education    2 700   3.16   1.43   0   5     5 0.05
age          3 700  25.59   9.50  13  65    52 0.36
ACT          4 700  28.55   4.82   3  36    33 0.18
SATV         5 700 612.23 112.90 200 800   600 4.27
SATQ         6 687 610.22 115.64 200 800   600 4.41
> describe(sat.act,IQR=TRUE) #show the interquartile Range
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.59   9.50     22   23.86   5.93  13  65    52  1.64
ACT          4 700  28.55   4.82     29   28.84   4.45   3  36    33 -0.66
SATV         5 700 612.23 112.90    620  619.45 118.61 200 800   600 -0.64
SATQ         6 687 610.22 115.64    620  617.25 118.61 200 800   600 -0.59
          kurtosis   se IQR
gender       -1.62 0.02   1
education    -0.07 0.05   1
age           2.42 0.36  10
ACT           0.53 0.18   7
SATV          0.33 4.27 150
SATQ         -0.02 4.41 170
> describe(sat.act,quant=c(.1,.25,.5,.75,.90) ) #find the 10th, 25th, 50th, 
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.59   9.50     22   23.86   5.93  13  65    52  1.64
ACT          4 700  28.55   4.82     29   28.84   4.45   3  36    33 -0.66
SATV         5 700 612.23 112.90    620  619.45 118.61 200 800   600 -0.64
SATQ         6 687 610.22 115.64    620  617.25 118.61 200 800   600 -0.59
          kurtosis   se Q0.1 Q0.25 Q0.5 Q0.75 Q0.9
gender       -1.62 0.02    1     1    2     2    2
education    -0.07 0.05    1     3    3     4    5
age           2.42 0.36   18    19   22    29   39
ACT           0.53 0.18   22    25   29    32   35
SATV          0.33 4.27  450   550  620   700  750
SATQ         -0.02 4.41  450   530  620   700  750
>                    #75th and 90th percentiles
>                    
>                    
>  
> describeData(sat.act) #the fast version 
n.obs =  700 of which  687   are complete cases.   Number of variables =  6  of which all are numeric  TRUE  
          variable # n.obs type  H1  H2  H3  H4  T1  T2  T3  T4
gender             1   700    1   2   2   2   1   1   2   1   1
education          2   700    1   3   3   3   4   4   3   4   5
age                3   700    1  19  23  20  27  40  24  35  25
ACT                4   700    1  24  35  21  26  27  31  32  25
SATV               5   700    1 500 600 480 550 613 700 700 600
SATQ               6   687    1 500 500 470 520 630 630 780 600
>    
> #now show how to adjust the displayed number of digits
>  des <- describe(sat.act)  #find the descriptive statistics.  Keep the original accuracy
>  des  #show the normal output, which is rounded to 2 decimals
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.59   9.50     22   23.86   5.93  13  65    52  1.64
ACT          4 700  28.55   4.82     29   28.84   4.45   3  36    33 -0.66
SATV         5 700 612.23 112.90    620  619.45 118.61 200 800   600 -0.64
SATQ         6 687 610.22 115.64    620  617.25 118.61 200 800   600 -0.59
          kurtosis   se
gender       -1.62 0.02
education    -0.07 0.05
age           2.42 0.36
ACT           0.53 0.18
SATV          0.33 4.27
SATQ         -0.02 4.41
>  print(des,digits=3)  #show the output, but round to 3 (trailing) digits
          vars   n    mean      sd median trimmed     mad min max range   skew
gender       1 700   1.647   0.478      2   1.684   0.000   1   2     1 -0.615
education    2 700   3.164   1.425      3   3.307   1.483   0   5     5 -0.681
age          3 700  25.594   9.499     22  23.863   5.930  13  65    52  1.643
ACT          4 700  28.547   4.824     29  28.843   4.448   3  36    33 -0.656
SATV         5 700 612.234 112.903    620 619.454 118.608 200 800   600 -0.644
SATQ         6 687 610.217 115.639    620 617.254 118.608 200 800   600 -0.593
          kurtosis    se
gender      -1.625 0.018
education   -0.075 0.054
age          2.424 0.359
ACT          0.535 0.182
SATV         0.325 4.267
SATQ        -0.018 4.412
>  print(des, signif=3) #round all numbers to the 3 significant digits 
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.60   9.50     22   23.90   5.93  13  65    52  1.64
ACT          4 700  28.50   4.82     29   28.80   4.45   3  36    33 -0.66
SATV         5 700 612.00 113.00    620  619.00 119.00 200 800   600 -0.64
SATQ         6 687 610.00 116.00    620  617.00 119.00 200 800   600 -0.59
          kurtosis   se
gender       -1.62 0.02
education    -0.08 0.05
age           2.42 0.36
ACT           0.53 0.18
SATV          0.33 4.27
SATQ         -0.02 4.41
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("describe.by")
> ### * describe.by
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: describeBy
> ### Title: Basic summary statistics by group
> ### Aliases: describeBy describe.by
> ### Keywords: models univar
> 
> ### ** Examples
> 
> 
> data(sat.act)
> describeBy(sat.act,sat.act$gender) #just one grouping variable	

 Descriptive statistics by group 
group: 1
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 247   1.00   0.00      1    1.00   0.00   1   1     0   NaN
education    2 247   3.00   1.54      3    3.12   1.48   0   5     5 -0.54
age          3 247  25.86   9.74     22   24.23   5.93  14  58    44  1.43
ACT          4 247  28.79   5.06     30   29.23   4.45   3  36    33 -1.06
SATV         5 247 615.11 114.16    630  622.07 118.61 200 800   600 -0.63
SATQ         6 245 635.87 116.02    660  645.53  94.89 300 800   500 -0.72
          kurtosis   se
gender         NaN 0.00
education    -0.60 0.10
age           1.43 0.62
ACT           1.89 0.32
SATV          0.13 7.26
SATQ         -0.12 7.41
------------------------------------------------------------ 
group: 2
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 453   2.00   0.00      2    2.00   0.00   2   2     0   NaN
education    2 453   3.26   1.35      3    3.40   1.48   0   5     5 -0.74
age          3 453  25.45   9.37     22   23.70   5.93  13  65    52  1.77
ACT          4 453  28.42   4.69     29   28.63   4.45  15  36    21 -0.39
SATV         5 453 610.66 112.31    620  617.91 103.78 200 800   600 -0.65
SATQ         6 442 596.00 113.07    600  602.21 133.43 200 800   600 -0.58
          kurtosis   se
gender         NaN 0.00
education     0.27 0.06
age           3.03 0.44
ACT          -0.42 0.22
SATV          0.42 5.28
SATQ          0.13 5.38
> #describeBy(sat.act,list(sat.act$gender,sat.act$education))  #two grouping variables
> des.mat <- describeBy(sat.act$age,sat.act$education,mat=TRUE) #matrix (data.frame) output 
> des.mat <- describeBy(sat.act$age,list(sat.act$education,sat.act$gender),
+              mat=TRUE,digits=2)  #matrix output
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("df2latex")
> ### * df2latex
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: df2latex
> ### Title: Convert a data frame, correlation matrix, or factor analysis
> ###   output to a LaTeX table
> ### Aliases: df2latex cor2latex fa2latex omega2latex irt2latex ICC2latex
> ### Keywords: utilities
> 
> ### ** Examples
> 
> df2latex(Thurstone,rowlabels=FALSE,apa=FALSE,short.names=FALSE,
+         caption="Thurstone Correlation matrix")
% df2latex % Thurstone % FALSE % FALSE % FALSE % Thurstone Correlation matrix 
\begin{table}[htpb]\caption{Thurstone Correlation matrix}
\begin{center}
\begin{scriptsize} 
\begin{tabular} { r r r r r r r r r r }
Sentences  &  Vocabulary  &  Sent.Completion  &  First.Letters  &  Four.Letter.Words  &  Suffixes  &  Letter.Series  &  Pedigrees  &  Letter.Group \cr 
 \cr 
  &  1.00  &  0.83  &  0.78  &  0.44  &  0.43  &  0.45  &  0.45  &  0.54  &  0.38 \cr 
   &  0.83  &  1.00  &  0.78  &  0.49  &  0.46  &  0.49  &  0.43  &  0.54  &  0.36 \cr 
   &  0.78  &  0.78  &  1.00  &  0.46  &  0.42  &  0.44  &  0.40  &  0.53  &  0.36 \cr 
   &  0.44  &  0.49  &  0.46  &  1.00  &  0.67  &  0.59  &  0.38  &  0.35  &  0.42 \cr 
   &  0.43  &  0.46  &  0.42  &  0.67  &  1.00  &  0.54  &  0.40  &  0.37  &  0.45 \cr 
   &  0.45  &  0.49  &  0.44  &  0.59  &  0.54  &  1.00  &  0.29  &  0.32  &  0.32 \cr 
   &  0.45  &  0.43  &  0.40  &  0.38  &  0.40  &  0.29  &  1.00  &  0.56  &  0.60 \cr 
   &  0.54  &  0.54  &  0.53  &  0.35  &  0.37  &  0.32  &  0.56  &  1.00  &  0.45 \cr 
   &  0.38  &  0.36  &  0.36  &  0.42  &  0.45  &  0.32  &  0.60  &  0.45  &  1.00 \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> df2latex(Thurstone,heading="Thurstone Correlation matrix in APA style")
% df2latex % Thurstone % Thurstone Correlation matrix in APA style 
\begin{table}[htpb]\caption{df2latex}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r r r r }
 \multicolumn{ 9 }{l}{ Thurstone Correlation matrix in APA style } \cr 
 \hline Variable  &   Sntnc  &  Vcblr  &  Snt.C  &  Frs.L  &  F.L.W  &  Sffxs  &  Ltt.S  &  Pdgrs  &  Ltt.G \cr 
  \hline 
Sentences   &  1.00  &  0.83  &  0.78  &  0.44  &  0.43  &  0.45  &  0.45  &  0.54  &  0.38 \cr 
 Vocabulary   &  0.83  &  1.00  &  0.78  &  0.49  &  0.46  &  0.49  &  0.43  &  0.54  &  0.36 \cr 
 Sent.Completion   &  0.78  &  0.78  &  1.00  &  0.46  &  0.42  &  0.44  &  0.40  &  0.53  &  0.36 \cr 
 First.Letters   &  0.44  &  0.49  &  0.46  &  1.00  &  0.67  &  0.59  &  0.38  &  0.35  &  0.42 \cr 
 Four.Letter.Words   &  0.43  &  0.46  &  0.42  &  0.67  &  1.00  &  0.54  &  0.40  &  0.37  &  0.45 \cr 
 Suffixes   &  0.45  &  0.49  &  0.44  &  0.59  &  0.54  &  1.00  &  0.29  &  0.32  &  0.32 \cr 
 Letter.Series   &  0.45  &  0.43  &  0.40  &  0.38  &  0.40  &  0.29  &  1.00  &  0.56  &  0.60 \cr 
 Pedigrees   &  0.54  &  0.54  &  0.53  &  0.35  &  0.37  &  0.32  &  0.56  &  1.00  &  0.45 \cr 
 Letter.Group   &  0.38  &  0.36  &  0.36  &  0.42  &  0.45  &  0.32  &  0.60  &  0.45  &  1.00 \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> 
> df2latex(describe(sat.act)[2:10],short.names=FALSE)
% df2latex % describe(sat.act)[2:10] % FALSE 
\begin{table}[htpb]\caption{df2latex}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r r r r }
 \multicolumn{ 9 }{l}{ A table from the psych package in R } \cr 
 \hline Variable  &   n  &  mean  &  sd  &  median  &  trimmed  &  mad  &  min  &  max  &  range \cr 
  \hline 
gender   &  700  &    1.65  &    0.48  &    2  &    1.68  &    0.00  &    1  &    2  &    1 \cr 
 education   &  700  &    3.16  &    1.43  &    3  &    3.31  &    1.48  &    0  &    5  &    5 \cr 
 age   &  700  &   25.59  &    9.50  &   22  &   23.86  &    5.93  &   13  &   65  &   52 \cr 
 ACT   &  700  &   28.55  &    4.82  &   29  &   28.84  &    4.45  &    3  &   36  &   33 \cr 
 SATV   &  700  &  612.23  &  112.90  &  620  &  619.45  &  118.61  &  200  &  800  &  600 \cr 
 SATQ   &  687  &  610.22  &  115.64  &  620  &  617.25  &  118.61  &  200  &  800  &  600 \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> cor2latex(Thurstone)
% df2latex % R % digits % rowlabels % apa % short.names % font.size % heading % caption % label % TRUE % stars % silent % file % append % cut % big 
\begin{table}[htpb]\caption{cor2latex}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r r r r }
 \multicolumn{ 9 }{l}{ A correlation table from the psych package in R. } \cr 
 \hline Variable  &   Sntnc  &  Vcblr  &  Snt.C  &  Frs.L  &  F.L.W  &  Sffxs  &  Ltt.S  &  Pdgrs  &  Ltt.G \cr 
  \hline 
Sentences   &  1.00   &    &    &    &    &    &    &    &   \cr 
 Vocabulary   &  0.83  &  1.00   &    &    &    &    &    &    &   \cr 
 Sent.Completion   &  0.78  &  0.78  &  1.00   &    &    &    &    &    &   \cr 
 First.Letters   &  0.44  &  0.49  &  0.46  &  1.00   &    &    &    &    &   \cr 
 Four.Letter.Words   &  0.43  &  0.46  &  0.42  &  0.67  &  1.00   &    &    &    &   \cr 
 Suffixes   &  0.45  &  0.49  &  0.44  &  0.59  &  0.54  &  1.00   &    &    &   \cr 
 Letter.Series   &  0.45  &  0.43  &  0.40  &  0.38  &  0.40  &  0.29  &  1.00   &    &   \cr 
 Pedigrees   &  0.54  &  0.54  &  0.53  &  0.35  &  0.37  &  0.32  &  0.56  &  1.00   &   \cr 
 Letter.Group   &  0.38  &  0.36  &  0.36  &  0.42  &  0.45  &  0.32  &  0.60  &  0.45  &  1.00  \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> cor2latex(sat.act,short.names=FALSE)
% df2latex % R % digits % rowlabels % apa % short.names % font.size % heading % caption % label % TRUE % stars % silent % file % append % cut % big 
\begin{table}[htpb]\caption{cor2latex}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r }
 \multicolumn{ 6 }{l}{ A correlation table from the psych package in R. } \cr 
 \hline Variable  &   gender  &  education  &  age  &  ACT  &  SATV  &  SATQ \cr 
  \hline 
gender   &   1.00   &    &    &    &    &   \cr 
 education   &   0.09  &   1.00   &    &    &    &   \cr 
 age   &  -0.02  &   0.55  &   1.00   &    &    &   \cr 
 ACT   &  -0.04  &   0.15  &   0.11  &   1.00   &    &   \cr 
 SATV   &  -0.02  &   0.05  &  -0.04  &   0.56  &   1.00   &   \cr 
 SATQ   &  -0.17  &   0.03  &  -0.03  &   0.59  &   0.64  &   1.00  \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> fa2latex(fa(Thurstone,3),heading="Factor analysis from R in quasi APA style")
% Called in the psych package  fa2latex % Called in the psych package  fa(Thurstone, 3) % Called in the psych package  Factor analysis from R in quasi APA style 
\begin{table}[htpb]\caption{fa2latex}
\begin{center}
\begin{scriptsize} 
\begin{tabular} {l r r r r r r }
 \multicolumn{ 6 }{l}{ Factor analysis from R in quasi APA style } \cr 
 \hline Variable  &   MR1  &  MR2  &  MR3  &  h2  &  u2  &  com \cr 
  \hline 
Sentences   &  \bf{ 0.90}  &  -0.03  &   0.04  &  0.82  &  0.18  &  1.01 \cr 
 Vocabulary   &  \bf{ 0.89}  &   0.06  &  -0.03  &  0.84  &  0.16  &  1.01 \cr 
 Sent.Completion   &  \bf{ 0.84}  &   0.03  &   0.00  &  0.74  &  0.26  &  1.00 \cr 
 First.Letters   &   0.00  &  \bf{ 0.85}  &   0.00  &  0.73  &  0.27  &  1.00 \cr 
 Four.Letter.Words   &  -0.02  &  \bf{ 0.75}  &   0.10  &  0.63  &  0.37  &  1.04 \cr 
 Suffixes   &   0.18  &  \bf{ 0.63}  &  -0.08  &  0.50  &  0.50  &  1.20 \cr 
 Letter.Series   &   0.03  &  -0.01  &  \bf{ 0.84}  &  0.73  &  0.27  &  1.00 \cr 
 Pedigrees   &  \bf{ 0.38}  &  -0.05  &  \bf{ 0.46}  &  0.51  &  0.49  &  1.96 \cr 
 Letter.Group   &  -0.06  &   0.21  &  \bf{ 0.63}  &  0.52  &  0.48  &  1.25 \cr 
\hline \cr SS loadings & 2.65 &  1.87 &  1.49 &  \cr  
\cr 
            \hline \cr 
MR1   & 1.00 & 0.59 & 0.53 \cr 
 MR2   & 0.59 & 1.00 & 0.52 \cr 
 MR3   & 0.53 & 0.52 & 1.00 \cr 
 \hline 
\end{tabular}
\end{scriptsize}
\end{center}
\label{default}
\end{table} 

> 
> #If using Sweave to create a LateX table as a separate file then set silent=TRUE
> #e.g., 
> #LaTex preamble 
> #....
> #<<print=FALSE,echo=FALSE>>= 
> #f3 <- fa(Thurstone,3)
> #fa2latex(f3,silent=TRUE,file='testoutput.tex')
> #@
> #
> #\input{testoutput.tex}
> 
>  
> 
> 
> cleanEx()
> nameEx("dfOrder")
> ### * dfOrder
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dfOrder
> ### Title: Sort (order) a dataframe or matrix by multiple columns
> ### Aliases: dfOrder
> ### Keywords: manip utilities
> 
> ### ** Examples
> 
> x <- data.frame(matrix(sample(1:4,64,replace=TRUE),ncol=4))
> dfOrder(x)  # sort by all columns
   X1 X2 X3 X4
11  1  1  4  3
12  1  2  3  2
10  1  2  3  3
5   1  4  4  2
14  2  2  4  2
16  2  3  2  2
1   2  3  2  3
2   2  4  1  3
8   3  1  2  1
3   3  2  4  2
9   3  2  4  2
13  3  4  3  4
6   4  1  1  1
15  4  2  1  2
7   4  3  3  1
4   4  4  3  4
> dfOrder(x,c(1,4))  #sort by the first and 4th column
   X1 X2 X3 X4
5   1  4  4  2
12  1  2  3  2
10  1  2  3  3
11  1  1  4  3
14  2  2  4  2
16  2  3  2  2
1   2  3  2  3
2   2  4  1  3
8   3  1  2  1
3   3  2  4  2
9   3  2  4  2
13  3  4  3  4
6   4  1  1  1
7   4  3  3  1
15  4  2  1  2
4   4  4  3  4
> dfOrder(x,c(1,-2))  #sort by the first in increaseing order, 
   X1 X2 X3 X4
11  1 -1  4 -3
10  1 -2  3 -3
12  1 -2  3 -2
5   1 -4  4 -2
14  2 -2  4 -2
1   2 -3  2 -3
16  2 -3  2 -2
2   2 -4  1 -3
8   3 -1  2 -1
3   3 -2  4 -2
9   3 -2  4 -2
13  3 -4  3 -4
6   4 -1  1 -1
15  4 -2  1 -2
7   4 -3  3 -1
4   4 -4  3 -4
>    #the second in decreasing order
> 
> 
> 
> 
> cleanEx()
> nameEx("diagram")
> ### * diagram
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: diagram
> ### Title: Helper functions for drawing path model diagrams
> ### Aliases: diagram dia.rect dia.ellipse dia.ellipse1 dia.arrow dia.curve
> ###   dia.curved.arrow dia.self dia.shape dia.triangle dia.cone
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> #first, show the primitives
> xlim=c(-2,10)
> ylim=c(0,10)
> plot(NA,xlim=xlim,ylim=ylim,main="Demonstration of diagram functions",axes=FALSE,xlab="",ylab="")
> ul <- dia.rect(1,9,labels="upper left",xlim=xlim,ylim=ylim)
> ml <- dia.rect(1,6,"middle left",xlim=xlim,ylim=ylim)
> ll <- dia.rect(1,3,labels="lower left",xlim=xlim,ylim=ylim)
> bl <- dia.rect(1,1,"bottom left",xlim=xlim,ylim=ylim)
> lr <- dia.ellipse(7,3,"lower right",xlim=xlim,ylim=ylim,e.size=.07)
> ur <- dia.ellipse(7,9,"upper right",xlim=xlim,ylim=ylim,e.size=.07)
> mr <- dia.ellipse(7,6,"middle right",xlim=xlim,ylim=ylim,e.size=.07)
> lm <- dia.triangle(4,1,"Lower Middle",xlim=xlim,ylim=ylim)
> br <- dia.rect(9,1,"bottom right",xlim=xlim,ylim=ylim) 
> dia.curve(from=ul$left,to=bl$left,"double headed",scale=-1)
> 
> dia.arrow(from=lr,to=ul,labels="right to left")
> dia.arrow(from=ul,to=ur,labels="left to right")
> dia.curved.arrow(from=lr,to=ll,labels ="right to left")
> dia.curved.arrow(to=ur,from=ul,labels ="left to right")
> dia.curve(ll$top,ul$bottom,"right")  #for rectangles, specify where to point 
> 
> dia.curve(ll$top,ul$bottom,"left",scale=-1)  #for rectangles, specify where to point 
> dia.curve(mr,ur,"up")  #but for ellipses, you may just point to it.
> dia.curve(mr,lr,"down")
> dia.curve(mr,ur,"up")
> dia.curved.arrow(mr,ur,"up")  #but for ellipses, you may just point to it.
> dia.curved.arrow(mr,lr,"down")  #but for ellipses, you may just point to it.
> 
> dia.curved.arrow(ur$right,mr$right,"3")
> dia.curve(ml,mr,"across")
> dia.curve(ur,lr,"top down")
> dia.curved.arrow(br$top,lr$bottom,"up")
> dia.curved.arrow(bl,br,"left to right")
> dia.curved.arrow(br,bl,"right to left",scale=-1)
> dia.arrow(bl,ll$bottom)
> dia.curved.arrow(ml,ll$right)
> dia.curved.arrow(mr,lr$top)
> 
> #now, put them together in a factor analysis diagram
> v9 <- sim.hierarchical()
> f3 <- fa(v9,3,rotate="cluster")
> fa.diagram(f3,error=TRUE,side=3) 
> 
> 
> 
> cleanEx()
> nameEx("draw.tetra")
> ### * draw.tetra
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: draw.tetra
> ### Title: Draw a correlation ellipse and two normal curves to demonstrate
> ###   tetrachoric correlation
> ### Aliases: draw.tetra draw.cor
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> #if(require(mvtnorm)) {
> #draw.tetra(.5,1,1)
> #draw.tetra(.8,2,1)} else {print("draw.tetra requires the mvtnorm package")
> #draw.cor(.5,cuts=c(0,0))}
> 
> draw.tetra(.5,1,1)
> draw.tetra(.8,2,1)
> draw.cor(.5,cuts=c(0,0))
> 
> 
> 
> cleanEx()
> nameEx("dummy.code")
> ### * dummy.code
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dummy.code
> ### Title: Create dummy coded variables
> ### Aliases: dummy.code
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> new <- dummy.code(sat.act$education)
> new.sat <- data.frame(new,sat.act)
> round(cor(new.sat,use="pairwise"),2)
             X0    X1    X2    X3    X4    X5 gender education   age   ACT
X0         1.00 -0.08 -0.08 -0.24 -0.15 -0.15  -0.08     -0.66 -0.27 -0.07
X1        -0.08  1.00 -0.07 -0.21 -0.13 -0.13  -0.05     -0.40 -0.17 -0.06
X2        -0.08 -0.07  1.00 -0.21 -0.13 -0.13  -0.09     -0.21  0.05 -0.08
X3        -0.24 -0.21 -0.21  1.00 -0.40 -0.40   0.10     -0.09 -0.39 -0.04
X4        -0.15 -0.13 -0.13 -0.40  1.00 -0.25  -0.02      0.29  0.24  0.07
X5        -0.15 -0.13 -0.13 -0.40 -0.25  1.00   0.03      0.65  0.49  0.11
gender    -0.08 -0.05 -0.09  0.10 -0.02  0.03   1.00      0.09 -0.02 -0.04
education -0.66 -0.40 -0.21 -0.09  0.29  0.65   0.09      1.00  0.55  0.15
age       -0.27 -0.17  0.05 -0.39  0.24  0.49  -0.02      0.55  1.00  0.11
ACT       -0.07 -0.06 -0.08 -0.04  0.07  0.11  -0.04      0.15  0.11  1.00
SATV       0.01 -0.03 -0.08  0.00  0.02  0.04  -0.02      0.05 -0.04  0.56
SATQ       0.03 -0.01 -0.07 -0.03  0.01  0.06  -0.17      0.03 -0.03  0.59
           SATV  SATQ
X0         0.01  0.03
X1        -0.03 -0.01
X2        -0.08 -0.07
X3         0.00 -0.03
X4         0.02  0.01
X5         0.04  0.06
gender    -0.02 -0.17
education  0.05  0.03
age       -0.04 -0.03
ACT        0.56  0.59
SATV       1.00  0.64
SATQ       0.64  1.00
> 
> 
> 
> 
> cleanEx()
> nameEx("dwyer")
> ### * dwyer
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Dwyer
> ### Title: 8 cognitive variables used by Dwyer for an example.
> ### Aliases: Dwyer
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(Dwyer)
> Ro <- Dwyer[1:7,1:7]
> Roe <- Dwyer[1:7,8]
> fo <- fa(Ro,2,rotate="none")
> fa.extension(Roe,fo)

Call: fa.extension(Roe = Roe, fo = fo)
Standardized loadings (pattern matrix) based upon correlation matrix
   MR1   MR2   h2   u2
1 0.37 -0.78 0.75 0.25

                       MR1  MR2
SS loadings           0.14 0.61
Proportion Var        0.14 0.61
Cumulative Var        0.14 0.75
Proportion Explained  0.18 0.82
Cumulative Proportion 0.18 1.00
> 
> 
> 
> cleanEx()
> nameEx("eigen.loadings")
> ### * eigen.loadings
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: eigen.loadings
> ### Title: Convert eigen vectors and eigen values to the more normal (for
> ###   psychologists) component loadings
> ### Aliases: eigen.loadings
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> x <- eigen(Harman74.cor$cov)
> x$vectors[1:8,1:4]  #as they appear from eigen
           [,1]         [,2]       [,3]        [,4]
[1,] -0.2158754 -0.003763753 -0.3287459  0.16684940
[2,] -0.1401069 -0.054849237 -0.3075102  0.16446394
[3,] -0.1558742 -0.131808462 -0.3661446  0.08629700
[4,] -0.1789834 -0.122936419 -0.2572906  0.17620811
[5,] -0.2436443 -0.221950283  0.2577381  0.04309103
[6,] -0.2420622 -0.288461492  0.2037767 -0.06579018
[7,] -0.2372958 -0.293489427  0.2731887  0.05917239
[8,] -0.2433556 -0.167723001  0.1103619  0.09493344
> y <- princomp(covmat=Harman74.cor$cov) 
> y$loadings[1:8,1:4] #as they appear from princomp
                          Comp.1       Comp.2     Comp.3      Comp.4
VisualPerception      -0.2158754 -0.003763753 -0.3287459  0.16684940
Cubes                 -0.1401069 -0.054849237 -0.3075102  0.16446394
PaperFormBoard        -0.1558742 -0.131808462 -0.3661446  0.08629700
Flags                 -0.1789834 -0.122936419 -0.2572906  0.17620811
GeneralInformation    -0.2436443 -0.221950283  0.2577381  0.04309103
PargraphComprehension -0.2420622 -0.288461492  0.2037767 -0.06579018
SentenceCompletion    -0.2372958 -0.293489427  0.2731887  0.05917239
WordClassification    -0.2433556 -0.167723001  0.1103619  0.09493344
> eigen.loadings(x)[1:8,1:4] # rescaled by the eigen values
           [,1]         [,2]       [,3]        [,4]
[1,] -0.6157349 -0.005449052 -0.4276989  0.20447285
[2,] -0.3996226 -0.079409132 -0.4000712  0.20154949
[3,] -0.4445954 -0.190828463 -0.4763546  0.10575641
[4,] -0.5105089 -0.177983777 -0.3347354  0.21594189
[5,] -0.6949395 -0.321333174  0.3353177  0.05280778
[6,] -0.6904266 -0.417626171  0.2651138 -0.08062544
[7,] -0.6768317 -0.424905469  0.3554189  0.07251538
[8,] -0.6941158 -0.242824490  0.1435811  0.11634031
> 
> 
> 
> cleanEx()
> nameEx("ellipses")
> ### * ellipses
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ellipses
> ### Title: Plot data and 1 and 2 sigma correlation ellipses
> ### Aliases: ellipses minkowski
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> data(galton)
> ellipses(galton,lm=TRUE)
> ellipses(galton$parent,galton$child,xlab="Mid Parent Height",
+                   ylab="Child Height") #input are two vectors
> data(sat.act)
> ellipses(sat.act)  #shows the pairs.panels ellipses
> minkowski(2,main="Minkowski circles")
> minkowski(1,TRUE)
> minkowski(4,TRUE)
> 
> 
> 
> 
> cleanEx()
> nameEx("epi")
> ### * epi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: epi
> ### Title: Eysenck Personality Inventory (EPI) data for 3570 participants
> ### Aliases: epi epi.dictionary
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(epi)
> epi.keys <- make.keys(epi,list(E = c(1, 3, -5, 8, 10, 13, -15, 17, -20, 22, 25, 27,
+                 -29, -32, -34, -37, 39, -41, 44, 46, 49, -51, 53, 56),
+    N=c(2, 4, 7, 9, 11, 14, 16, 19, 21, 23, 26, 28, 31, 33, 35, 38, 40,
+      43, 45, 47, 50, 52, 55, 57),
+    L = c(6, -12, -18, 24, -30, 36, -42, -48, -54),
+    I =c(1, 3, -5, 8, 10, 13, 22, 39, -41), 
+    S = c(-11, -15, 17, -20, 25, 27, -29, -32, -37, 44, 46, -51, 53)))
> scores <- scoreItems(epi.keys,epi)
>   N <- epi[abs(epi.keys[,"N"]) >0]
>   E <- epi[abs(epi.keys[,"E"]) >0]
>   fa.lookup(epi.keys[,1:3],epi.dictionary) #show the items and keying information
     E N  L
V1   1 0  0
V3   1 0  0
V5  -1 0  0
V8   1 0  0
V10  1 0  0
V13  1 0  0
V15 -1 0  0
V17  1 0  0
V20 -1 0  0
V22  1 0  0
V25  1 0  0
V27  1 0  0
V29 -1 0  0
V32 -1 0  0
V34 -1 0  0
V37 -1 0  0
V39  1 0  0
V41 -1 0  0
V44  1 0  0
V46  1 0  0
V49  1 0  0
V51 -1 0  0
V53  1 0  0
V56  1 0  0
V2   0 1  0
V4   0 1  0
V7   0 1  0
V9   0 1  0
V11  0 1  0
V14  0 1  0
V16  0 1  0
V19  0 1  0
V21  0 1  0
V23  0 1  0
V26  0 1  0
V28  0 1  0
V31  0 1  0
V33  0 1  0
V35  0 1  0
V38  0 1  0
V40  0 1  0
V43  0 1  0
V45  0 1  0
V47  0 1  0
V50  0 1  0
V52  0 1  0
V55  0 1  0
V57  0 1  0
V6   0 0  1
V12  0 0 -1
V18  0 0 -1
V24  0 0  1
V30  0 0 -1
V36  0 0  1
V42  0 0 -1
V48  0 0 -1
V54  0 0 -1
                                                                                                              Content
V1                                                                                  Do you often long for excitement?
V3                                                                                          Are you usually carefree?
V5                                                           Do you stop and think things over before doing anything?
V8                                              Do you generally do and say things quickly without stopping to think?
V10                                                                          Would you do almost anything for a dare?
V13                                                                 Do you often do things on the spur of the moment?
V15                                                                Generally do you prefer reading to meeting people?
V17                                                                                      Do you like going out a lot?
V20                                                                    Do you prefer to have few but special friends?
V22                                                                       When people shout at you do you shout back?
V25                                       Can you usually let yourself go and enjoy yourself a lot at a lively party?
V27                                                                Do other people think of you as being very lively?
V29                                                              Are you mostly quiet when you are with other people?
V32 If there is something you want to know about, would you rather look it upin a book than talk to someone about it?
V34                                             Do you like the kind of work that you need to pay close attention to?
V37                                                     Do you hate being with a crowd who play jokes on one another?
V39                                                        Do you like doing things in which you have to act quickly?
V41                                                                   Are you slow and unhurried in the way you move?
V44                      Do you like talking to people so much that you never miss a chance of talking to a stranger?
V46                                   Would you be very unhappy if you could not see lots of people most of the time?
V49                                                                Would you say that you were fairly self-confident?
V51                                                   Do you find it hard to really enjoy yourself at a lively party?
V53                                                                   Can you easily get some life into a dull party?
V56                                                                             Do you like playing pranks on others?
V2                                                           Do you often need understanding friends to cheer you up?
V4                                                                 Do you find it very hard to take no for an answer?
V7                                                                                      Do your moods go up and down?
V9                                                                Do you ever feel just miserable for no good reason?
V11                                         Do you suddenly feel shy when you want to talk to an attractive stranger?
V14                                                     Do you often worry about things you should have done or said?
V16                                                                             Are your feelings rather easily hurt?
V19                                          Are you sometimes bubbling over with energy and sometimes very sluggish?
V21                                                                                            Do you daydream a lot?
V23                                                                   Are you often troubled about feelings of guilt?
V26                                                                   Would you call yourself tense or highly strung?
V28                      After you have done something important, do you come away feelingyou could have done better?
V31                                                          Do ideas run through your head so that you cannot sleep?
V33                                                                 Do you get palpitations or thumping in your hear?
V35                                                                       Do you get attacks of shaking or trembling?
V38                                                                                      Are you an irritable person?
V40                                                                Do you worry about awful things that might happen?
V43                                                                                      Do you have many nightmares?
V45                                                                              Are you troubled by aches and pains?
V47                                                                         Would you call yourself a nervous person?
V50                                                 Are you easily hurt when people find fault with you or your work?
V52                                                                      Are you troubled by feelings of inferiority?
V55                                                                                   Do you worry about your health?
V57                                                                                 Do you suffer from sleeplessness?
V6  If you say you will do something do you always keep your promise,no matter how inconvenient it might be to do so?
V12                                                            Once in a while do you lose your temper and get angry?
V18                    Do you occasionally have thoughts and ideas that you would not like otherpeople to know about?
V24                                                                      Are all your habits good and desirable ones?
V30                                                                                          Do you sometimes gossip?
V36                     Would you always declare everything at customs, even if you knewyou could never be found out?
V42                                                               Have you ever been late for an appointment or work?
V48                                       Of all the people you know, are there some whom you definitely do not like?
V54                                                        Do you sometimes talk about things you know nothing about?
> 
> 
> 
> cleanEx()
> nameEx("epi.bfi")
> ### * epi.bfi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: epi.bfi
> ### Title: 13 personality scales from the Eysenck Personality Inventory and
> ###   Big 5 inventory
> ### Aliases: epi.bfi
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(epi.bfi)
> pairs.panels(epi.bfi[,1:5])
> describe(epi.bfi)
         vars   n   mean    sd median trimmed   mad min max range  skew
epiE        1 231  13.33  4.14     14   13.49  4.45   1  22    21 -0.33
epiS        2 231   7.58  2.69      8    7.77  2.97   0  13    13 -0.57
epiImp      3 231   4.37  1.88      4    4.36  1.48   0   9     9  0.06
epilie      4 231   2.38  1.50      2    2.27  1.48   0   7     7  0.66
epiNeur     5 231  10.41  4.90     10   10.39  4.45   0  23    23  0.06
bfagree     6 231 125.00 18.14    126  125.26 17.79  74 167    93 -0.21
bfcon       7 231 113.25 21.88    114  113.42 22.24  53 178   125 -0.02
bfext       8 231 102.18 26.45    104  102.99 22.24   8 168   160 -0.41
bfneur      9 231  87.97 23.34     90   87.70 23.72  34 152   118  0.07
bfopen     10 231 123.43 20.51    125  123.78 20.76  73 173   100 -0.16
bdi        11 231   6.78  5.78      6    5.97  4.45   0  27    27  1.29
traitanx   12 231  39.01  9.52     38   38.36  8.90  22  71    49  0.67
stateanx   13 231  39.85 11.48     38   38.92 10.38  21  79    58  0.72
         kurtosis   se
epiE        -0.06 0.27
epiS        -0.02 0.18
epiImp      -0.62 0.12
epilie       0.24 0.10
epiNeur     -0.50 0.32
bfagree     -0.27 1.19
bfcon        0.23 1.44
bfext        0.51 1.74
bfneur      -0.55 1.54
bfopen      -0.16 1.35
bdi          1.50 0.38
traitanx     0.47 0.63
stateanx    -0.01 0.76
> 
> 
> 
> cleanEx()
> nameEx("error.bars")
> ### * error.bars
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: error.bars
> ### Title: Plot means and confidence intervals
> ### Aliases: error.bars error.bars.tab
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> x <- replicate(20,rnorm(50))
> boxplot(x,notch=TRUE,main="Notched boxplot with error bars")
> error.bars(x,add=TRUE)
> abline(h=0)
> 
> #show 50% confidence regions and color each variable separately
> error.bars(attitude,alpha=.5,
+    main="50 percent confidence limits",col=rainbow(ncol(attitude)) )
>    
> error.bars(attitude,bar=TRUE)  #show the use of bar graphs
> 
> 
> #combine with a strip chart and boxplot
> stripchart(attitude,vertical=TRUE,method="jitter",jitter=.1,pch=19,
+            main="Stripchart with 95 percent confidence limits")
> boxplot(attitude,add=TRUE)
> error.bars(attitude,add=TRUE,arrow.len=.2)
> 
> #use statistics from somewhere else
> #by specifying n, we are using the t distribution for confidences
> #The first example allows the variables to be spaced along the x axis
> my.stats <- data.frame(values=c(1,2,8),mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
>  error.bars(stats=my.stats,type="b",main="data with confidence intervals")
> #don't connect the groups
> my.stats <- data.frame(values=c(1,2,8),mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
>       error.bars(stats=my.stats,main="data with confidence intervals")
> #by not specifying value, the groups are equally spaced
> my.stats <- data.frame(mean=c(10,12,18),se=c(2,3,5),n=c(5,10,20))
> rownames(my.stats) <- c("First", "Second","Third")
> error.bars(stats=my.stats,xlab="Condition",ylab="Score")
> 
> 
> #Consider the case where we get stats from describe
> temp <- describe(attitude)
> error.bars(stats=temp)
> 
> #show these do not differ from the other way by overlaying the two
> error.bars(attitude,add=TRUE,col="red")
> 
> #n is omitted
> #the error distribution is a normal distribution
> my.stats <- data.frame(mean=c(2,4,8),se=c(2,1,2))
> rownames(my.stats) <- c("First", "Second","Third")
> error.bars(stats=my.stats,xlab="Condition",ylab="Score")
> #n is specified
> #compare this with small n which shows larger confidence regions
> my.stats <- data.frame(mean=c(2,4,8),se=c(2,1,2),n=c(10,10,3))
> rownames(my.stats) <- c("First", "Second","Third")
> error.bars(stats=my.stats,xlab="Condition",ylab="Score")
> 
> 
> #example of arrest rates (as percentage of condition)
> arrest <- data.frame(Control=c(14,21),Treated =c(3,23))
> rownames(arrest) <- c("Arrested","Not Arrested")
> error.bars.tab(arrest,ylab="Probability of Arrest",xlab="Control vs Treatment",
+ main="Probability of Arrest varies by treatment")
> 
> 
> #Show the raw  rates 
> error.bars.tab(arrest,raw=TRUE,ylab="Number Arrested",xlab="Control vs Treatment",
+ main="Count of Arrest varies by treatment")
> 
> 
> 
> cleanEx()
> nameEx("error.bars.by")
> ### * error.bars.by
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: error.bars.by
> ### Title: Plot means and confidence intervals for multiple groups
> ### Aliases: error.bars.by
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> 
> data(sat.act)
> #The generic plot of variables by group
> error.bars.by(sat.act[1:4],sat.act$gender,legend=7)
> #a bar plot
> error.bars.by(sat.act[5:6],sat.act$gender,bars=TRUE,labels=c("male","female"),
+     main="SAT V and SAT Q by gender",ylim=c(0,800),colors=c("red","blue"),
+     legend=5,v.labels=c("SATV","SATQ"))  #draw a barplot
> #a bar plot of SAT by age -- not recommended, see the next plot
> error.bars.by(sat.act[5:6],sat.act$education,bars=TRUE,xlab="Education",
+    main="95 percent confidence limits of Sat V and Sat Q", ylim=c(0,800),
+    v.labels=c("SATV","SATQ"),legend=5,colors=c("red","blue") )
> #a better graph uses points not bars
>   #plot SAT V and SAT Q by education
> error.bars.by(sat.act[5:6],sat.act$education,TRUE, xlab="Education",
+     legend=5,labels=colnames(sat.act[5:6]),ylim=c(525,700),
+      main="self reported SAT scores by education")
> #make the cats eyes semi-transparent by specifying a negative density
> error.bars.by(sat.act[5:6],sat.act$education,TRUE, xlab="Education",
+     legend=5,labels=colnames(sat.act[5:6]),ylim=c(525,700),
+      main="self reported SAT scores by education",density=-10)
> 
> #now for a more complicated examples using 25 big 5 items scored into 5 scales
> #and showing age trends by decade 
> #this shows how to convert many levels of a grouping variable (age) into more manageable levels.
> data(bfi)   #The Big 5 data
> #first create the keys 
>  keys.list <- list(Agree=c(-1,2:5),Conscientious=c(6:8,-9,-10),
+         Extraversion=c(-11,-12,13:15),Neuroticism=c(16:20),Openness = c(21,-22,23,24,-25))
>  keys <- make.keys(bfi,keys.list)
>  #then create the scores for those older than 10 and less than 80
>  bfis <- subset(bfi,((bfi$age > 10) & (bfi$age < 80)))
> 
>  scores <- scoreItems(keys,bfis,min=1,max=6) #set the right limits for item reversals
>  #now draw the results by age
>  
>  error.bars.by(scores$scores,round(bfis$age/10)*10,by.var=TRUE,
+       main="BFI age trends",legend=3,labels=colnames(scores$scores),
+         xlab="Age",ylab="Mean item score")
> 
>  error.bars.by(scores$scores,round(bfis$age/10)*10,by.var=TRUE,
+       main="BFI age trends",legend=3,labels=colnames(scores$scores),
+         xlab="Age",ylab="Mean item score",density=-10)
> 
> 
> 
> cleanEx()
> nameEx("error.circles")
> ### * error.circles
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: errorCircles
> ### Title: Two way plots of means, error bars, and sample sizes
> ### Aliases: errorCircles
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> #BFI scores for males and females
> errorCircles(1:25,1:25,data=bfi,group="gender",paired=TRUE,ylab="female scores",
+       xlab="male scores",main="BFI scores by gender")
>  abline(a=0,b=1)
> #drop the circles since all samples are the same sizes
> errorCircles(1:25,1:25,data=bfi,group="gender",paired=TRUE,circles=FALSE,
+      ylab="female scores",xlab="male scores",main="BFI scores by gender")
>  abline(a=0,b=1)
>  
>  data(affect)
> colors <- c("black","red","white","blue")
> films <- c("Sad","Horror","Neutral","Happy")
> affect.stats <- errorCircles("EA2","TA2",data=affect[-c(1,20)],group="Film",labels=films,
+       xlab="Energetic Arousal",ylab="Tense Arousal",ylim=c(10,22),xlim=c(8,20), 
+       pch=16,cex=2,colors=colors, main ="EA and TA pre and post affective movies")
> #now, use the stats from the prior run 
> errorCircles("EA1","TA1",data=affect.stats,labels=films,pch=16,cex=2,colors=colors,add=TRUE)
> 
> #show sample size with the size of the circles
> errorCircles("SATV","SATQ",sat.act,group="education")
> 
> #Can also provide error.bars.by functionality
>  errorCircles(2,5,group=2,data=sat.act,circles=FALSE,pch=16,colors="blue",
+       ylim= c(200,800),main="SATV by education",labels="")
>  #just do the breakdown and then show the points
> # errorCircles(3,5,group=3,data=sat.act,circles=FALSE,pch=16,colors="blue",
> #         ylim= c(200,800),main="SATV by age",labels="",bars=FALSE)
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("error.crosses")
> ### * error.crosses
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: error.crosses
> ### Title: Plot x and y error bars
> ### Aliases: error.crosses
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> 
> #just draw one pair of variables
> desc <- describe(attitude)
> x <- desc[1,]
> y <- desc[2,]
> error.crosses(x,y,xlab=rownames(x),ylab=rownames(y))   
> 
> #now for a bit more complicated plotting 
> data(bfi)
> desc <- describeBy(bfi[1:25],bfi$gender) #select a high and low group
> error.crosses(desc$'1',desc$'2',ylab="female scores",xlab="male scores",main="BFI scores by gender")
>  abline(a=0,b=1)
>  
> #do it from summary statistics  (using standard errors) 
> g1.stats <- data.frame(n=c(10,20,30),mean=c(10,12,18),se=c(2,3,5))
> g2.stats <- data.frame(n=c(15,20,25),mean=c(6,14,15),se =c(1,2,3))
> error.crosses(g1.stats,g2.stats)
> 
> #Or, if you prefer to draw +/- 1 sd. instead of 95% confidence
> g1.stats <- data.frame(n=c(10,20,30),mean=c(10,12,18),sd=c(2,3,5))
> g2.stats <- data.frame(n=c(15,20,25),mean=c(6,14,15),sd =c(1,2,3))
> error.crosses(g1.stats,g2.stats,sd=TRUE)
> 
> #and seem even fancy plotting: This is taken from a study of mood
> #four films were given (sad, horror, neutral, happy)
> #with a pre and post test
> data(affect)
> colors <- c("black","red","green","blue")
> films <- c("Sad","Horror","Neutral","Happy")
> affect.mat <- describeBy(affect[10:17],affect$Film,mat=TRUE)
>  error.crosses(affect.mat[c(1:4,17:20),],affect.mat[c(5:8,21:24),],
+     labels=films[affect.mat$group1],xlab="Energetic Arousal",
+      ylab="Tense Arousal",colors = 
+      colors[affect.mat$group1],pch=16,cex=2)
> 
> 
> 
> 
> cleanEx()
> nameEx("error.dots")
> ### * error.dots
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: error.dots
> ### Title: Show a dot.chart with error bars for different groups or
> ###   variables
> ### Aliases: error.dots
> ### Keywords: ~kwd1 ~kwd2
> 
> ### ** Examples
> 
> ##---- Should be DIRECTLY executable !! ----
> ##-- ==>  Define data, use random,
> ##--	or do  help(data=index)  for the standard data sets.
> 
> ## The function is currently defined as
> function (x, var = NULL, se = NULL, group = NULL, sd = FALSE, 
+     head = 12, tail = 12, sort = TRUE, decreasing = FALSE, main = NULL, 
+     alpha = 0.05, eyes = FALSE, min.n = NULL, max.labels = 40, 
+     labels = NULL, groups = NULL, gdata = NULL, cex = par("cex"), 
+     pt.cex = cex, pch = 21, gpch = 21, bg = par("bg"), color = par("fg"), 
+     gcolor = par("fg"), lcolor = "gray", xlab = NULL, ylab = NULL, 
+     xlim = NULL, ...) 
+ {
+     opar <- par("mai", "mar", "cex", "yaxs")
+     on.exit(par(opar))
+     par(cex = cex, yaxs = "i")
+     if (length(class(x)) > 1) {
+         if (class(x)[1] == "psych") {
+             obj <- class(x)[2]
+             switch(obj, statsBy = {
+                 if (is.null(min.n)) {
+                   se <- x$sd[, var]/sqrt(x$n[, var])
+                   x <- x$mean[, var]
+                 } else {
+                   se <- x$sd[, var]
+                   n.obs <- x$n[, var]
+                   x <- x$mean[, var]
+                   if (sd) {
+                     se <- x$sd[, var]
+                   } else {
+                     se <- se/sqrt(n.obs)
+                   }
+                   x <- subset(x, n.obs > min.n)
+                   se <- subset(se, n.obs > min.n)
+                   n.obs <- subset(n.obs, n.obs > min.n)
+                 }
+             }, describe = {
+                 if (sd) {
+                   se <- x$sd
+                 } else {
+                   se <- x$se
+                 }
+                 labels <- rownames(x)
+                 x <- x$mean
+                 names(x) <- labels
+             }, describeBy = {
+                 des <- x
+                 if (is.null(xlab)) xlab <- var
+                 var <- which(rownames(des[[1]]) == var)
+                 x <- se <- rep(NA, length(des))
+                 for (grp in 1:length(x)) {
+                   x[grp] <- des[[grp]][["mean"]][var]
+                   if (sd) {
+                     se[grp] <- des[[grp]][["sd"]][var]
+                   } else {
+                     se[grp] <- des[[grp]][["se"]][var]
+                   }
+                 }
+                 names(x) <- names(des)
+                 if (is.null(xlab)) xlab <- var
+             })
+         }
+     }
+     else {
+         if (is.null(group)) {
+             des <- describe(x)
+             x <- des$mean
+             if (sd) {
+                 se <- des$sd
+             }
+             else {
+                 se <- des$se
+             }
+             names(x) <- rownames(des)
+         }
+         else {
+             if (is.null(xlab)) 
+                 xlab <- var
+             des <- describeBy(x, group = group)
+             x <- se <- rep(NA, length(des))
+             names(x) <- names(des)
+             var <- which(rownames(des[[1]]) == var)
+             for (grp in 1:length(des)) {
+                 x[grp] <- des[[grp]][["mean"]][var]
+                 if (sd) {
+                   se[grp] <- des[[grp]][["sd"]][var]
+                 }
+                 else {
+                   se[grp] <- des[[grp]][["se"]][var]
+                 }
+             }
+         }
+     }
+     n.var <- length(x)
+     if (sort) {
+         ord <- order(x, decreasing = decreasing)
+     }
+     else {
+         ord <- n.var:1
+     }
+     x <- x[ord]
+     se <- se[ord]
+     temp <- temp.se <- rep(NA, min(head + tail, n.var))
+     if ((head + tail) < n.var) {
+         if (head > 0) {
+             temp[1:head] <- x[1:head]
+             temp.se[1:head] <- se[1:head]
+             names(temp) <- names(x)[1:head]
+         }
+         if (tail > 0) {
+             temp[(head + 1):(head + tail)] <- x[(length(x) - 
+                 tail + 1):length(x)]
+             temp.se[(head + 1):(head + tail)] <- se[(length(x) - 
+                 tail + 1):length(x)]
+             names(temp)[(head + 1):(head + tail)] <- names(x)[(length(x) - 
+                 tail + 1):length(x)]
+         }
+         x <- temp
+         se <- temp.se
+     }
+     if (missing(main)) {
+         if (sd) {
+             main <- "means + standard deviation"
+         }
+         else {
+             main = "Confidence Intervals around the mean"
+         }
+     }
+     labels <- names(x)
+     if (sd) {
+         ci <- se
+     }
+     else {
+         ci <- qnorm((1 - alpha/2)) * se
+     }
+     if (!is.null(ci) && is.null(xlim)) 
+         xlim <- c(min(x - ci), max(x + ci))
+     labels <- substr(labels, 1, max.labels)
+     if (eyes) {
+         ln <- seq(-3, 3, 0.1)
+         rev <- (length(ln):1)
+     }
+     if (!is.numeric(x)) 
+         stop("'x' must be a numeric vector or matrix")
+     n <- length(x)
+     if (is.matrix(x)) {
+         if (is.null(labels)) 
+             labels <- rownames(x)
+         if (is.null(labels)) 
+             labels <- as.character(1L:nrow(x))
+         labels <- rep_len(labels, n)
+         if (is.null(groups)) 
+             groups <- col(x, as.factor = TRUE)
+         glabels <- levels(groups)
+     }
+     else {
+         if (is.null(labels)) 
+             labels <- names(x)
+         glabels <- if (!is.null(groups)) 
+             levels(groups)
+         if (!is.vector(x)) {
+             warning("'x' is neither a vector nor a matrix: using as.numeric(x)")
+             x <- as.numeric(x)
+         }
+     }
+     plot.new()
+     linch <- if (!is.null(labels)) 
+         max(strwidth(labels, "inch"), na.rm = TRUE)
+     else 0
+     if (is.null(glabels)) {
+         ginch <- 0
+         goffset <- 0
+     }
+     else {
+         ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
+         goffset <- 0.4
+     }
+     if (!(is.null(labels) && is.null(glabels))) {
+         nmai <- par("mai")
+         nmai[2L] <- nmai[4L] + max(linch + goffset, ginch) + 
+             0.1
+         par(mai = nmai)
+     }
+     if (is.null(groups)) {
+         o <- 1L:n
+         y <- o
+         ylim <- c(0, n + 1)
+     }
+     else {
+         o <- sort.list(as.numeric(groups), decreasing = TRUE)
+         x <- x[o]
+         groups <- groups[o]
+         color <- rep_len(color, length(groups))[o]
+         lcolor <- rep_len(lcolor, length(groups))[o]
+         offset <- cumsum(c(0, diff(as.numeric(groups)) != 0))
+         y <- 1L:n + 2 * offset
+         ylim <- range(0, y + 2)
+     }
+     plot.window(xlim = xlim, ylim = ylim, log = "")
+     lheight <- par("csi")
+     if (!is.null(labels)) {
+         linch <- max(strwidth(labels, "inch"), na.rm = TRUE)
+         loffset <- (linch + 0.1)/lheight
+         labs <- labels[o]
+         mtext(labs, side = 2, line = loffset, at = y, adj = 0, 
+             col = color, las = 2, cex = cex, ...)
+     }
+     abline(h = y, lty = "dotted", col = lcolor)
+     points(x, y, pch = pch, col = color, bg = bg, cex = pt.cex/cex)
+     if (!is.null(ci)) {
+         if (!eyes) {
+             segments(x - ci, y, x + ci, y, col = par("fg"), lty = par("lty"), 
+                 lwd = par("lwd"))
+         }
+         else {
+             catseyes(x, y, se = se, n = NULL, alpha = alpha, 
+                 density = -10)
+         }
+     }
+     if (!is.null(groups)) {
+         gpos <- rev(cumsum(rev(tapply(groups, groups, length)) + 
+             2) - 1)
+         ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
+         goffset <- (max(linch + 0.2, ginch, na.rm = TRUE) + 0.1)/lheight
+         mtext(glabels, side = 2, line = goffset, at = gpos, adj = 0, 
+             col = gcolor, las = 2, cex = cex, ...)
+         if (!is.null(gdata)) {
+             abline(h = gpos, lty = "dotted")
+             points(gdata, gpos, pch = gpch, col = gcolor, bg = bg, 
+                 cex = pt.cex/cex, ...)
+         }
+     }
+     axis(1)
+     box()
+     invisible()
+     if (!is.null(group)) 
+         result <- des
+   }
function (x, var = NULL, se = NULL, group = NULL, sd = FALSE, 
    head = 12, tail = 12, sort = TRUE, decreasing = FALSE, main = NULL, 
    alpha = 0.05, eyes = FALSE, min.n = NULL, max.labels = 40, 
    labels = NULL, groups = NULL, gdata = NULL, cex = par("cex"), 
    pt.cex = cex, pch = 21, gpch = 21, bg = par("bg"), color = par("fg"), 
    gcolor = par("fg"), lcolor = "gray", xlab = NULL, ylab = NULL, 
    xlim = NULL, ...) 
{
    opar <- par("mai", "mar", "cex", "yaxs")
    on.exit(par(opar))
    par(cex = cex, yaxs = "i")
    if (length(class(x)) > 1) {
        if (class(x)[1] == "psych") {
            obj <- class(x)[2]
            switch(obj, statsBy = {
                if (is.null(min.n)) {
                  se <- x$sd[, var]/sqrt(x$n[, var])
                  x <- x$mean[, var]
                } else {
                  se <- x$sd[, var]
                  n.obs <- x$n[, var]
                  x <- x$mean[, var]
                  if (sd) {
                    se <- x$sd[, var]
                  } else {
                    se <- se/sqrt(n.obs)
                  }
                  x <- subset(x, n.obs > min.n)
                  se <- subset(se, n.obs > min.n)
                  n.obs <- subset(n.obs, n.obs > min.n)
                }
            }, describe = {
                if (sd) {
                  se <- x$sd
                } else {
                  se <- x$se
                }
                labels <- rownames(x)
                x <- x$mean
                names(x) <- labels
            }, describeBy = {
                des <- x
                if (is.null(xlab)) xlab <- var
                var <- which(rownames(des[[1]]) == var)
                x <- se <- rep(NA, length(des))
                for (grp in 1:length(x)) {
                  x[grp] <- des[[grp]][["mean"]][var]
                  if (sd) {
                    se[grp] <- des[[grp]][["sd"]][var]
                  } else {
                    se[grp] <- des[[grp]][["se"]][var]
                  }
                }
                names(x) <- names(des)
                if (is.null(xlab)) xlab <- var
            })
        }
    }
    else {
        if (is.null(group)) {
            des <- describe(x)
            x <- des$mean
            if (sd) {
                se <- des$sd
            }
            else {
                se <- des$se
            }
            names(x) <- rownames(des)
        }
        else {
            if (is.null(xlab)) 
                xlab <- var
            des <- describeBy(x, group = group)
            x <- se <- rep(NA, length(des))
            names(x) <- names(des)
            var <- which(rownames(des[[1]]) == var)
            for (grp in 1:length(des)) {
                x[grp] <- des[[grp]][["mean"]][var]
                if (sd) {
                  se[grp] <- des[[grp]][["sd"]][var]
                }
                else {
                  se[grp] <- des[[grp]][["se"]][var]
                }
            }
        }
    }
    n.var <- length(x)
    if (sort) {
        ord <- order(x, decreasing = decreasing)
    }
    else {
        ord <- n.var:1
    }
    x <- x[ord]
    se <- se[ord]
    temp <- temp.se <- rep(NA, min(head + tail, n.var))
    if ((head + tail) < n.var) {
        if (head > 0) {
            temp[1:head] <- x[1:head]
            temp.se[1:head] <- se[1:head]
            names(temp) <- names(x)[1:head]
        }
        if (tail > 0) {
            temp[(head + 1):(head + tail)] <- x[(length(x) - 
                tail + 1):length(x)]
            temp.se[(head + 1):(head + tail)] <- se[(length(x) - 
                tail + 1):length(x)]
            names(temp)[(head + 1):(head + tail)] <- names(x)[(length(x) - 
                tail + 1):length(x)]
        }
        x <- temp
        se <- temp.se
    }
    if (missing(main)) {
        if (sd) {
            main <- "means + standard deviation"
        }
        else {
            main = "Confidence Intervals around the mean"
        }
    }
    labels <- names(x)
    if (sd) {
        ci <- se
    }
    else {
        ci <- qnorm((1 - alpha/2)) * se
    }
    if (!is.null(ci) && is.null(xlim)) 
        xlim <- c(min(x - ci), max(x + ci))
    labels <- substr(labels, 1, max.labels)
    if (eyes) {
        ln <- seq(-3, 3, 0.1)
        rev <- (length(ln):1)
    }
    if (!is.numeric(x)) 
        stop("'x' must be a numeric vector or matrix")
    n <- length(x)
    if (is.matrix(x)) {
        if (is.null(labels)) 
            labels <- rownames(x)
        if (is.null(labels)) 
            labels <- as.character(1L:nrow(x))
        labels <- rep_len(labels, n)
        if (is.null(groups)) 
            groups <- col(x, as.factor = TRUE)
        glabels <- levels(groups)
    }
    else {
        if (is.null(labels)) 
            labels <- names(x)
        glabels <- if (!is.null(groups)) 
            levels(groups)
        if (!is.vector(x)) {
            warning("'x' is neither a vector nor a matrix: using as.numeric(x)")
            x <- as.numeric(x)
        }
    }
    plot.new()
    linch <- if (!is.null(labels)) 
        max(strwidth(labels, "inch"), na.rm = TRUE)
    else 0
    if (is.null(glabels)) {
        ginch <- 0
        goffset <- 0
    }
    else {
        ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
        goffset <- 0.4
    }
    if (!(is.null(labels) && is.null(glabels))) {
        nmai <- par("mai")
        nmai[2L] <- nmai[4L] + max(linch + goffset, ginch) + 
            0.1
        par(mai = nmai)
    }
    if (is.null(groups)) {
        o <- 1L:n
        y <- o
        ylim <- c(0, n + 1)
    }
    else {
        o <- sort.list(as.numeric(groups), decreasing = TRUE)
        x <- x[o]
        groups <- groups[o]
        color <- rep_len(color, length(groups))[o]
        lcolor <- rep_len(lcolor, length(groups))[o]
        offset <- cumsum(c(0, diff(as.numeric(groups)) != 0))
        y <- 1L:n + 2 * offset
        ylim <- range(0, y + 2)
    }
    plot.window(xlim = xlim, ylim = ylim, log = "")
    lheight <- par("csi")
    if (!is.null(labels)) {
        linch <- max(strwidth(labels, "inch"), na.rm = TRUE)
        loffset <- (linch + 0.1)/lheight
        labs <- labels[o]
        mtext(labs, side = 2, line = loffset, at = y, adj = 0, 
            col = color, las = 2, cex = cex, ...)
    }
    abline(h = y, lty = "dotted", col = lcolor)
    points(x, y, pch = pch, col = color, bg = bg, cex = pt.cex/cex)
    if (!is.null(ci)) {
        if (!eyes) {
            segments(x - ci, y, x + ci, y, col = par("fg"), lty = par("lty"), 
                lwd = par("lwd"))
        }
        else {
            catseyes(x, y, se = se, n = NULL, alpha = alpha, 
                density = -10)
        }
    }
    if (!is.null(groups)) {
        gpos <- rev(cumsum(rev(tapply(groups, groups, length)) + 
            2) - 1)
        ginch <- max(strwidth(glabels, "inch"), na.rm = TRUE)
        goffset <- (max(linch + 0.2, ginch, na.rm = TRUE) + 0.1)/lheight
        mtext(glabels, side = 2, line = goffset, at = gpos, adj = 0, 
            col = gcolor, las = 2, cex = cex, ...)
        if (!is.null(gdata)) {
            abline(h = gpos, lty = "dotted")
            points(gdata, gpos, pch = gpch, col = gcolor, bg = bg, 
                cex = pt.cex/cex, ...)
        }
    }
    axis(1)
    box()
    invisible()
    if (!is.null(group)) 
        result <- des
}
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("esem")
> ### * esem
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: esem
> ### Title: Perform and Exploratory Structural Equation Model (ESEM) by
> ###   using factor extension techniques
> ### Aliases: esem esem.diagram interbattery
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #make up a sem like problem using sim.structure
> fx <-matrix(c( .9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)  
> fy <- matrix(c(.6,.5,.4),ncol=1)
> rownames(fx) <- c("V","Q","A","nach","Anx")
> rownames(fy)<- c("gpa","Pre","MA")
> Phi <-matrix( c(1,0,.7,.0,1,.7,.7,.7,1),ncol=3)
> gre.gpa <- sim.structural(fx,Phi,fy)
> print(gre.gpa)
Call: sim.structural(fx = fx, Phi = Phi, fy = fy)

 $model (Population correlation matrix) 
        V    Q     A  nach   Anx   gpa   Pre    MA
V    1.00 0.72  0.54  0.00  0.00  0.38  0.32  0.25
Q    0.72 1.00  0.48  0.00  0.00  0.34  0.28  0.22
A    0.54 0.48  1.00  0.48 -0.42  0.50  0.42  0.34
nach 0.00 0.00  0.48  1.00 -0.56  0.34  0.28  0.22
Anx  0.00 0.00 -0.42 -0.56  1.00 -0.29 -0.24 -0.20
gpa  0.38 0.34  0.50  0.34 -0.29  1.00  0.30  0.24
Pre  0.32 0.28  0.42  0.28 -0.24  0.30  1.00  0.20
MA   0.25 0.22  0.34  0.22 -0.20  0.24  0.20  1.00

$reliability (population reliability) 
   V    Q    A nach  Anx  gpa  Pre   MA 
0.81 0.64 0.72 0.64 0.49 0.36 0.25 0.16 
> 
> #now esem it:
> example <- esem(gre.gpa$model,varsX=1:5,varsY=6:8,nfX=2,nfY=1,n.obs=1000,plot=FALSE)
> example
Exploratory Structural Equation Modeling  Analysis using method =  minres
Call: esem(r = gre.gpa$model, varsX = 1:5, varsY = 6:8, nfX = 2, nfY = 1, 
    n.obs = 1000, plot = FALSE)

For the 'X' set:
       MR1   MR2
V     0.91 -0.06
Q     0.81 -0.05
A     0.53  0.57
nach -0.10  0.81
Anx   0.08 -0.71

For the 'Y' set:
    MR1
gpa 0.6
Pre 0.5
MA  0.4

Correlations between the X and Y sets.
     X1   X2   Y1
X1 1.00 0.19 0.68
X2 0.19 1.00 0.67
Y1 0.68 0.67 1.00

The degrees of freedom for the null model are  56  and the empirical chi square  function was  6930.29
The degrees of freedom for the model are 7  and the empirical chi square function was  21.83 
  with prob <  0.0027 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.04 
 with the empirical chi square  21.83  with prob <  0.0027 
The total number of observations was  1000  with fitted Chi Square =  2175.06  with prob <  0 

Empirical BIC =  -26.53
ESABIC =  -4.29
Fit based upon off diagonal values = 1
To see the item loadings for the X and Y sets combined, and the associated fa output, print with  short=FALSE.
> esem.diagram(example,simple=FALSE)
> 
> #compare two alternative solutions to the first 2 factors of the neo.
> #solution 1 is the normal 2 factor solution.
> #solution 2 is an esem with 1 factor for the first 6 variables, and 1 for the second 6.
> f2 <- fa(neo[1:12,1:12],2)
> es2 <- esem(neo,1:6,7:12,1,1)
> summary(f2)

Factor analysis with Call: fa(r = neo[1:12, 1:12], nfactors = 2)

Test of the hypothesis that 2 factors are sufficient.
The degrees of freedom for the model is 43  and the objective function was  0.63 

The root mean square of the residuals (RMSA) is  0.06 
The df corrected root mean square of the residuals is  0.07 

 With factor correlations of 
      MR1   MR2
MR1  1.00 -0.22
MR2 -0.22  1.00
> summary(es2)

Exploratory Structural Equation Modeling  with Call: esem(r = neo, varsX = 1:6, varsY = 7:12, nfX = 1, nfY = 1)

Test of the hypothesis that 2 factors are sufficient.
The degrees of freedom for the model is 43  and the objective function was  4.44 

The root mean square of the residuals (RMSA) is  0.06 
The df corrected root mean square of the residuals is  0.08 
      X1    Y1
X1  1.00 -0.28
Y1 -0.28  1.00
> fa.congruence(f2,es2)
       X1    Y1
MR1  0.98 -0.36
MR2 -0.21  0.94
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("fa")
> ### * fa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa
> ### Title: Exploratory Factor analysis using MinRes (minimum residual) as
> ###   well as EFA by Principal Axis, Weighted Least Squares or Maximum
> ###   Likelihood
> ### Aliases: fa fac
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #using the Harman 24 mental tests, compare a principal factor with a principal components solution
> pc <- principal(Harman74.cor$cov,4,rotate="varimax")   #principal components
> pa <- fa(Harman74.cor$cov,4,fm="pa" ,rotate="varimax")  #principal axis 
> uls <- fa(Harman74.cor$cov,4,rotate="varimax")          #unweighted least squares is minres
> wls <- fa(Harman74.cor$cov,4,fm="wls")       #weighted least squares
> 
> #to show the loadings sorted by absolute value
> print(uls,sort=TRUE)
Factor Analysis using method =  minres
Call: fa(r = Harman74.cor$cov, nfactors = 4, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
                       item  MR1   MR3   MR2  MR4   h2   u2 com
SentenceCompletion        7 0.81  0.19  0.15 0.07 0.73 0.27 1.2
WordMeaning               9 0.81  0.20  0.05 0.22 0.74 0.26 1.3
PargraphComprehension     6 0.76  0.21  0.07 0.23 0.68 0.32 1.4
GeneralInformation        5 0.73  0.19  0.22 0.14 0.64 0.36 1.4
WordClassification        8 0.57  0.34  0.23 0.14 0.51 0.49 2.2
VisualPerception          1 0.15  0.68  0.20 0.15 0.55 0.45 1.4
PaperFormBoard            3 0.15  0.55 -0.01 0.11 0.34 0.66 1.2
Flags                     4 0.23  0.53  0.09 0.07 0.35 0.65 1.5
SeriesCompletion         23 0.37  0.52  0.23 0.22 0.51 0.49 2.7
Cubes                     2 0.11  0.45  0.08 0.08 0.23 0.77 1.3
Deduction                20 0.38  0.42  0.10 0.29 0.42 0.58 2.9
ProblemReasoning         22 0.37  0.41  0.13 0.29 0.40 0.60 3.0
Addition                 10 0.17 -0.11  0.82 0.16 0.74 0.26 1.2
CountingDots             12 0.02  0.20  0.71 0.09 0.55 0.45 1.2
StraightCurvedCapitals   13 0.18  0.42  0.54 0.08 0.51 0.49 2.2
Code                     11 0.18  0.11  0.54 0.37 0.47 0.53 2.1
ArithmeticProblems       24 0.36  0.19  0.49 0.29 0.49 0.51 2.9
NumericalPuzzles         21 0.18  0.40  0.43 0.21 0.42 0.58 2.8
ObjectNumber             17 0.14  0.06  0.22 0.58 0.41 0.59 1.4
WordRecognition          14 0.21  0.05  0.08 0.56 0.36 0.64 1.3
NumberRecognition        15 0.12  0.12  0.08 0.52 0.31 0.69 1.3
FigureRecognition        16 0.07  0.42  0.06 0.52 0.45 0.55 2.0
NumberFigure             18 0.02  0.31  0.34 0.45 0.41 0.59 2.7
FigureWord               19 0.15  0.25  0.18 0.35 0.23 0.77 2.8

                       MR1  MR3  MR2  MR4
SS loadings           3.64 2.93 2.67 2.23
Proportion Var        0.15 0.12 0.11 0.09
Cumulative Var        0.15 0.27 0.38 0.48
Proportion Explained  0.32 0.26 0.23 0.19
Cumulative Proportion 0.32 0.57 0.81 1.00

Mean item complexity =  1.9
Test of the hypothesis that 4 factors are sufficient.

The degrees of freedom for the null model are  276  and the objective function was  11.44
The degrees of freedom for the model are 186  and the objective function was  1.72 

The root mean square of the residuals (RMSR) is  0.04 
The df corrected root mean square of the residuals is  0.05 

Fit based upon off diagonal values = 0.98
Measures of factor score adequacy             
                                                MR1  MR3  MR2  MR4
Correlation of scores with factors             0.93 0.87 0.91 0.82
Multiple R square of scores with factors       0.87 0.76 0.83 0.68
Minimum correlation of possible factor scores  0.74 0.52 0.65 0.36
> 
> #then compare with a maximum likelihood solution using factanal
> mle <- factanal(covmat=Harman74.cor$cov,factors=4)
> factor.congruence(list(mle,pa,pc,uls,wls))
        Factor1 Factor2 Factor3 Factor4  PA1  PA3  PA2  PA4  RC1  RC3  RC2  RC4
Factor1    1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
Factor2    0.61    1.00    0.50    0.61 0.61 1.00 0.50 0.60 0.60 0.99 0.49 0.52
Factor3    0.46    0.50    1.00    0.57 0.46 0.50 1.00 0.56 0.45 0.44 1.00 0.48
Factor4    0.56    0.61    0.57    1.00 0.56 0.62 0.58 1.00 0.55 0.55 0.56 0.99
PA1        1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
PA3        0.61    1.00    0.50    0.62 0.61 1.00 0.50 0.61 0.61 0.99 0.50 0.53
PA2        0.46    0.50    1.00    0.58 0.46 0.50 1.00 0.57 0.46 0.44 1.00 0.49
PA4        0.55    0.60    0.56    1.00 0.55 0.61 0.57 1.00 0.54 0.54 0.55 0.99
RC1        1.00    0.60    0.45    0.55 1.00 0.61 0.46 0.54 1.00 0.53 0.43 0.46
RC3        0.54    0.99    0.44    0.55 0.54 0.99 0.44 0.54 0.53 1.00 0.43 0.47
RC2        0.44    0.49    1.00    0.56 0.44 0.50 1.00 0.55 0.43 0.43 1.00 0.47
RC4        0.47    0.52    0.48    0.99 0.47 0.53 0.49 0.99 0.46 0.47 0.47 1.00
MR1        1.00    0.61    0.46    0.56 1.00 0.61 0.46 0.55 1.00 0.54 0.44 0.47
MR3        0.61    1.00    0.50    0.62 0.61 1.00 0.50 0.61 0.61 0.99 0.50 0.53
MR2        0.46    0.50    1.00    0.58 0.46 0.50 1.00 0.57 0.46 0.44 1.00 0.49
MR4        0.55    0.60    0.56    1.00 0.55 0.61 0.57 1.00 0.54 0.54 0.55 0.99
WLS1       0.98    0.47    0.30    0.40 0.98 0.48 0.30 0.39 0.98 0.41 0.28 0.32
WLS3       0.36    0.95    0.41    0.41 0.36 0.95 0.41 0.39 0.35 0.97 0.41 0.32
WLS2       0.23    0.22    0.95    0.36 0.23 0.22 0.95 0.35 0.22 0.16 0.95 0.28
WLS4       0.28    0.40    0.36    0.94 0.28 0.41 0.37 0.94 0.27 0.36 0.35 0.97
         MR1  MR3  MR2  MR4 WLS1 WLS3 WLS2 WLS4
Factor1 1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
Factor2 0.61 1.00 0.50 0.60 0.47 0.95 0.22 0.40
Factor3 0.46 0.50 1.00 0.56 0.30 0.41 0.95 0.36
Factor4 0.56 0.62 0.58 1.00 0.40 0.41 0.36 0.94
PA1     1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
PA3     0.61 1.00 0.50 0.61 0.48 0.95 0.22 0.41
PA2     0.46 0.50 1.00 0.57 0.30 0.41 0.95 0.37
PA4     0.55 0.61 0.57 1.00 0.39 0.39 0.35 0.94
RC1     1.00 0.61 0.46 0.54 0.98 0.35 0.22 0.27
RC3     0.54 0.99 0.44 0.54 0.41 0.97 0.16 0.36
RC2     0.44 0.50 1.00 0.55 0.28 0.41 0.95 0.35
RC4     0.47 0.53 0.49 0.99 0.32 0.32 0.28 0.97
MR1     1.00 0.61 0.46 0.55 0.98 0.36 0.23 0.28
MR3     0.61 1.00 0.50 0.61 0.48 0.95 0.22 0.41
MR2     0.46 0.50 1.00 0.57 0.30 0.41 0.95 0.37
MR4     0.55 0.61 0.57 1.00 0.39 0.39 0.35 0.94
WLS1    0.98 0.48 0.30 0.39 1.00 0.22 0.09 0.13
WLS3    0.36 0.95 0.41 0.39 0.22 1.00 0.17 0.23
WLS2    0.23 0.22 0.95 0.35 0.09 0.17 1.00 0.20
WLS4    0.28 0.41 0.37 0.94 0.13 0.23 0.20 1.00
> #note that the order of factors and the sign of some of factors may differ 
> 
> #finally, compare the unrotated factor, ml, uls, and  wls solutions
> wls <- fa(Harman74.cor$cov,4,rotate="none",fm="wls")
> pa <- fa(Harman74.cor$cov,4,rotate="none",fm="pa")
> minres <-  factanal(factors=4,covmat=Harman74.cor$cov,rotation="none")
> mle <- fa(Harman74.cor$cov,4,rotate="none",fm="mle")
> uls <- fa(Harman74.cor$cov,4,rotate="none",fm="uls")
> factor.congruence(list(minres,mle,pa,wls,uls))
        Factor1 Factor2 Factor3 Factor4   ML1   ML2  ML3   ML4  PA1   PA2   PA3
Factor1    1.00    0.11    0.25    0.06  1.00  0.11 0.25  0.06 1.00 -0.04 -0.05
Factor2    0.11    1.00    0.06    0.07  0.11  1.00 0.06  0.07 0.14  0.98 -0.08
Factor3    0.25    0.06    1.00    0.01  0.25  0.06 1.00  0.01 0.30  0.10  0.95
Factor4    0.06    0.07    0.01    1.00  0.06  0.07 0.01  1.00 0.07  0.13 -0.04
ML1        1.00    0.11    0.25    0.06  1.00  0.11 0.25  0.06 1.00 -0.04 -0.05
ML2        0.11    1.00    0.06    0.07  0.11  1.00 0.06  0.07 0.14  0.98 -0.08
ML3        0.25    0.06    1.00    0.01  0.25  0.06 1.00  0.01 0.30  0.10  0.95
ML4        0.06    0.07    0.01    1.00  0.06  0.07 0.01  1.00 0.07  0.13 -0.04
PA1        1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
PA2       -0.04    0.98    0.10    0.13 -0.04  0.98 0.10  0.13 0.00  1.00  0.00
PA3       -0.05   -0.08    0.95   -0.04 -0.05 -0.08 0.95 -0.04 0.00  0.00  1.00
PA4       -0.01   -0.08    0.02    0.99 -0.01 -0.08 0.02  0.99 0.00  0.00  0.00
WLS1       1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
WLS2      -0.04    0.98    0.09    0.13 -0.04  0.98 0.09  0.13 0.00  1.00 -0.01
WLS3      -0.05   -0.07    0.95   -0.04 -0.05 -0.07 0.95 -0.04 0.00  0.01  1.00
WLS4      -0.01   -0.07    0.02    0.99 -0.01 -0.07 0.02  0.99 0.00  0.01  0.00
ULS1       1.00    0.14    0.30    0.07  1.00  0.14 0.30  0.07 1.00  0.00  0.00
ULS2      -0.04    0.98    0.09    0.13 -0.04  0.98 0.09  0.13 0.00  1.00  0.00
ULS3      -0.05   -0.07    0.95   -0.04 -0.05 -0.07 0.95 -0.04 0.00  0.00  1.00
ULS4      -0.01   -0.08    0.02    0.99 -0.01 -0.08 0.02  0.99 0.00  0.00  0.00
          PA4 WLS1  WLS2  WLS3  WLS4 ULS1  ULS2  ULS3  ULS4
Factor1 -0.01 1.00 -0.04 -0.05 -0.01 1.00 -0.04 -0.05 -0.01
Factor2 -0.08 0.14  0.98 -0.07 -0.07 0.14  0.98 -0.07 -0.08
Factor3  0.02 0.30  0.09  0.95  0.02 0.30  0.09  0.95  0.02
Factor4  0.99 0.07  0.13 -0.04  0.99 0.07  0.13 -0.04  0.99
ML1     -0.01 1.00 -0.04 -0.05 -0.01 1.00 -0.04 -0.05 -0.01
ML2     -0.08 0.14  0.98 -0.07 -0.07 0.14  0.98 -0.07 -0.08
ML3      0.02 0.30  0.09  0.95  0.02 0.30  0.09  0.95  0.02
ML4      0.99 0.07  0.13 -0.04  0.99 0.07  0.13 -0.04  0.99
PA1      0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
PA2      0.00 0.00  1.00  0.01  0.01 0.00  1.00  0.00  0.00
PA3      0.00 0.00 -0.01  1.00  0.00 0.00  0.00  1.00  0.00
PA4      1.00 0.00 -0.01  0.00  1.00 0.00  0.00  0.00  1.00
WLS1     0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
WLS2    -0.01 0.00  1.00  0.00  0.00 0.00  1.00 -0.01 -0.01
WLS3     0.00 0.00  0.00  1.00  0.00 0.00  0.01  1.00  0.00
WLS4     1.00 0.00  0.00  0.00  1.00 0.00  0.01  0.00  1.00
ULS1     0.00 1.00  0.00  0.00  0.00 1.00  0.00  0.00  0.00
ULS2     0.00 0.00  1.00  0.01  0.01 0.00  1.00  0.00  0.00
ULS3     0.00 0.00 -0.01  1.00  0.00 0.00  0.00  1.00  0.00
ULS4     1.00 0.00 -0.01  0.00  1.00 0.00  0.00  0.00  1.00
> #in particular, note the similarity of the mle and min res solutions
> #note that the order of factors and the sign of some of factors may differ 
> 
> 
> 
> #an example of where the ML and PA and MR models differ is found in Thurstone.33.
> #compare the first two factors with the 3 factor solution 
> Thurstone.33 <- as.matrix(Thurstone.33)
> mle2 <- fa(Thurstone.33,2,rotate="none",fm="mle")
> mle3 <- fa(Thurstone.33,3 ,rotate="none",fm="mle")
> pa2 <- fa(Thurstone.33,2,rotate="none",fm="pa")
> pa3 <- fa(Thurstone.33,3,rotate="none",fm="pa")
> mr2 <- fa(Thurstone.33,2,rotate="none")
> mr3 <- fa(Thurstone.33,3,rotate="none")
> factor.congruence(list(mle2,mr2,pa2,mle3,pa3,mr3))
      ML1   ML2  MR1   MR2  PA1   PA2   ML1   ML2   ML3   PA1   PA2   PA3   MR1
ML1  1.00  0.16 1.00 -0.03 1.00 -0.03  0.98  0.88  0.16  1.00 -0.02  0.00  1.00
ML2  0.16  1.00 0.19  0.97 0.19  0.98  0.03  0.51 -0.79  0.18  0.97 -0.09  0.18
MR1  1.00  0.19 1.00  0.00 1.00  0.00  0.97  0.89  0.14  1.00  0.01  0.01  1.00
MR2 -0.03  0.97 0.00  1.00 0.00  1.00 -0.14  0.32 -0.87 -0.01  0.97 -0.17 -0.01
PA1  1.00  0.19 1.00  0.00 1.00  0.00  0.97  0.89  0.15  1.00  0.01  0.01  1.00
PA2 -0.03  0.98 0.00  1.00 0.00  1.00 -0.14  0.32 -0.87 -0.01  0.98 -0.17 -0.01
ML1  0.98  0.03 0.97 -0.14 0.97 -0.14  1.00  0.76  0.19  0.98 -0.17 -0.14  0.98
ML2  0.88  0.51 0.89  0.32 0.89  0.32  0.76  1.00 -0.01  0.88  0.39  0.26  0.88
ML3  0.16 -0.79 0.14 -0.87 0.15 -0.87  0.19 -0.01  1.00  0.15 -0.78  0.61  0.15
PA1  1.00  0.18 1.00 -0.01 1.00 -0.01  0.98  0.88  0.15  1.00  0.00  0.00  1.00
PA2 -0.02  0.97 0.01  0.97 0.01  0.98 -0.17  0.39 -0.78  0.00  1.00  0.00  0.00
PA3  0.00 -0.09 0.01 -0.17 0.01 -0.17 -0.14  0.26  0.61  0.00  0.00  1.00  0.00
MR1  1.00  0.18 1.00 -0.01 1.00 -0.01  0.98  0.88  0.15  1.00  0.00  0.00  1.00
MR2 -0.02  0.97 0.01  0.96 0.01  0.97 -0.17  0.40 -0.75  0.00  1.00  0.04  0.00
MR3  0.01 -0.14 0.01 -0.22 0.02 -0.22 -0.13  0.24  0.64  0.00 -0.04  1.00  0.00
      MR2   MR3
ML1 -0.02  0.01
ML2  0.97 -0.14
MR1  0.01  0.01
MR2  0.96 -0.22
PA1  0.01  0.02
PA2  0.97 -0.22
ML1 -0.17 -0.13
ML2  0.40  0.24
ML3 -0.75  0.64
PA1  0.00  0.00
PA2  1.00 -0.04
PA3  0.04  1.00
MR1  0.00  0.00
MR2  1.00  0.00
MR3  0.00  1.00
> 
> #f5 <- fa(bfi[1:25],5)
> #f5  #names are not in ascending numerical order (see note)
> #colnames(f5$loadings) <- paste("F",1:5,sep="")
> #f5
> 
> #Get the variance accounted for object from the print function
> p <- print(mr3)
Factor Analysis using method =  minres
Call: fa(r = Thurstone.33, nfactors = 3, rotate = "none")
Standardized loadings (pattern matrix) based upon correlation matrix
                          MR1   MR2   MR3   h2    u2 com
Definitions              0.77  0.00  0.11 0.61 0.392 1.0
Arithmetical_Problems    0.64  0.39 -0.20 0.59 0.406 1.9
Classification           0.73 -0.09  0.18 0.57 0.431 1.2
Artificial_Languange     0.65  0.13  0.20 0.48 0.519 1.3
Antonyms                 0.81 -0.26  0.02 0.72 0.278 1.2
Number_Series_Completion 0.58  0.52 -0.13 0.62 0.381 2.1
Analogies                0.74 -0.04  0.25 0.62 0.381 1.2
Logical_Inference        0.75  0.01 -0.04 0.56 0.438 1.0
Paragraph_Reading        0.82 -0.39 -0.39 0.98 0.019 1.9

                       MR1  MR2  MR3
SS loadings           4.73 0.67 0.37
Proportion Var        0.53 0.07 0.04
Cumulative Var        0.53 0.60 0.64
Proportion Explained  0.82 0.12 0.06
Cumulative Proportion 0.82 0.94 1.00

Mean item complexity =  1.4
Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  4.85
The degrees of freedom for the model are 12  and the objective function was  0.07 

The root mean square of the residuals (RMSR) is  0.02 
The df corrected root mean square of the residuals is  0.03 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR2  MR3
Correlation of scores with factors             0.97 0.86 0.83
Multiple R square of scores with factors       0.94 0.73 0.68
Minimum correlation of possible factor scores  0.88 0.46 0.37
> round(p$Vaccounted,2)
                       MR1  MR2  MR3
SS loadings           4.73 0.67 0.37
Proportion Var        0.53 0.07 0.04
Cumulative Var        0.53 0.60 0.64
Proportion Explained  0.82 0.12 0.06
Cumulative Proportion 0.82 0.94 1.00
> 
> 
> 
> cleanEx()
> nameEx("fa.diagram")
> ### * fa.diagram
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.diagram
> ### Title: Graph factor loading matrices
> ### Aliases: fa.graph fa.rgraph fa.diagram het.diagram
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> 
> test.simple <- fa(item.sim(16),2,rotate="oblimin")
> #if(require(Rgraphviz)) {fa.graph(test.simple) } 
> fa.diagram(test.simple)
> f3 <- fa(Thurstone,3,rotate="cluster")
> fa.diagram(f3,cut=.4,digits=2)
> f3l <- f3$loadings
> fa.diagram(f3l,main="input from a matrix")
> Phi <- f3$Phi
> fa.diagram(f3l,Phi=Phi,main="Input from a matrix")
> fa.diagram(ICLUST(Thurstone,2,title="Two cluster solution of Thurstone"),main="Input from ICLUST")
> het.diagram(Thurstone,levels=list(1:4,5:8,3:7))
> 
> 
> 
> cleanEx()
> nameEx("fa.extension")
> ### * fa.extension
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.extension
> ### Title: Apply Dwyer's factor extension to find factor loadings for
> ###   extended variables
> ### Aliases: fa.extension fa.extend
> ### Keywords: multivariate
> 
> ### ** Examples
>  #The Dwyer Example
> Ro <- Dwyer[1:7,1:7]
> Roe <- Dwyer[1:7,8]
> fo <- fa(Ro,2,rotate="none")
> fe <- fa.extension(Roe,fo)
> 
> #an example from simulated data
> set.seed(42) 
>  d <- sim.item(12)    #two orthogonal factors 
>  R <- cor(d)
>  Ro <- R[c(1,2,4,5,7,8,10,11),c(1,2,4,5,7,8,10,11)]
>  Roe <- R[c(1,2,4,5,7,8,10,11),c(3,6,9,12)]
>  fo <- fa(Ro,2)
>  fe <- fa.extension(Roe,fo)
>  fa.diagram(fo,fe=fe)
>  
>  #create two correlated factors
>  fx <- matrix(c(.9,.8,.7,.85,.75,.65,rep(0,12),.9,.8,.7,.85,.75,.65),ncol=2)
>  Phi <- matrix(c(1,.6,.6,1),2)
>  sim.data <- sim.structure(fx,Phi,n=1000,raw=TRUE)
>  R <- cor(sim.data$observed)
>  Ro <- R[c(1,2,4,5,7,8,10,11),c(1,2,4,5,7,8,10,11)]
>  Roe <- R[c(1,2,4,5,7,8,10,11),c(3,6,9,12)]
>  fo <- fa(Ro,2)
>  fe <- fa.extension(Roe,fo)
>  fa.diagram(fo,fe=fe)
>  
>  #now show how fa.extend works with the same data set
>  #note that we have to make sure that the variables are in the order to do the factor congruence
>  fe2 <- fa.extend(R,2,ov=c(1,2,4,5,7,8,10,11),ev=c(3,6,9,12),n.obs=1000)
>  fa.diagram(fe2,main="factor analysis with extension variables")
>  fa2 <- fa(sim.data$observed[,c(1,2,4,5,7,8,10,11,3,6,9,12)],2)
>  factor.congruence(fe2,fa2)
     MR1 MR2
MR1 1.00   0
MR2 0.01   1
>  summary(fe2)
>  
>  #an example of extending an omega analysis
>  
>  
> fload <- matrix(c(c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),
+         rep(0,20)),c(c(c(.9,.8,.7,.6),rep(0,20)),c(.9,.8,.7,.6))),ncol=5)
>  gload <- matrix(rep(.7,5))
>  five.factor <- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
>  ss <- c(1,2,3,5,6,7,9,10,11,13,14,15,17,18,19)
>  Ro <- cor(five.factor$observed[,ss])
>  Re <- cor(five.factor$observed[,ss],five.factor$observed[,-ss])
>  om5 <-omega(Ro,5)   #the omega analysis
>  fa.extension(Re,om5) #the extension analysis

Call: fa.extension(Roe = Re, fo = om5)
Standardized loadings (pattern matrix) based upon correlation matrix
       g   F1*   F2*   F3*   F4*   F5*   h2   u2
V4  0.38  0.07  0.02  0.34  0.03 -0.01 0.27 0.73
V8  0.43  0.00  0.07 -0.03  0.36  0.05 0.32 0.68
V12 0.38  0.41  0.04 -0.02  0.01  0.03 0.31 0.69
V16 0.39 -0.02 -0.01  0.02 -0.01  0.36 0.28 0.72
V20 0.38 -0.03  0.34  0.07  0.01  0.01 0.27 0.73

                         g  F1*  F2*  F3*  F4*  F5*
SS loadings           0.77 0.17 0.12 0.12 0.13 0.13
Proportion Var        0.15 0.03 0.02 0.02 0.03 0.03
Cumulative Var        0.15 0.19 0.21 0.24 0.26 0.29
Proportion Explained  0.53 0.12 0.09 0.09 0.09 0.09
Cumulative Proportion 0.53 0.65 0.73 0.82 0.91 1.00
>  
> 
> 
> 
> cleanEx()
> nameEx("fa.parallel")
> ### * fa.parallel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.parallel
> ### Title: Scree plots of data or correlation matrix compared to random
> ###   "parallel" matrices
> ### Aliases: fa.parallel fa.parallel.poly plot.poly.parallel
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> #test.data <- Harman74.cor$cov   #The 24 variable Holzinger - Harman problem
> #fa.parallel(test.data,n.obs=145)
> fa.parallel(Thurstone,n.obs=213)   #the 9 variable Thurstone problem
Parallel analysis suggests that the number of factors =  3  and the number of components =  1 
> 
> #set.seed(123)
> #minor <- sim.minor(24,4,400) #4 large and 12 minor factors
> #ffa.parallel(minor$observed) #shows 5 factors and 4 components -- compare with 
> #fa.parallel(minor$observed,SMC=FALSE) #which shows 6  and 4 components factors
> #a demonstration of parallel analysis of a dichotomous variable
> #fp <- fa.parallel(ability)    #use the default Pearson correlation
> #fpt <- fa.parallel(ability,cor="tet")  #do a tetrachoric correlation
> #fpt <- fa.parallel(ability,cor="tet",quant=.95)  #do a tetrachoric correlation and 
> #use the 95th percentile of the simulated results
> #apply(fp$values,2,function(x) quantile(x,.95))  #look at the 95th percentile of values
> #apply(fpt$values,2,function(x) quantile(x,.95))  #look at the 95th percentile of values
> #describe(fpt$values)  #look at all the statistics of the simulated values
> 
> 
> 
> 
> cleanEx()
> nameEx("fa.sort")
> ### * fa.sort
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.sort
> ### Title: Sort factor analysis or principal components analysis loadings
> ### Aliases: fa.sort fa.organize
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> test.simple <- fa(sim.item(16),2)
> fa.sort(test.simple)
Factor Analysis using method =  minres
Call: fa(r = sim.item(16), nfactors = 2)
Standardized loadings (pattern matrix) based upon correlation matrix
      MR1   MR2   h2   u2 com
V3  -0.67  0.06 0.45 0.55   1
V12  0.66 -0.01 0.44 0.56   1
V11  0.65  0.06 0.43 0.57   1
V9   0.62 -0.01 0.39 0.61   1
V4  -0.62  0.01 0.38 0.62   1
V1  -0.61 -0.03 0.38 0.62   1
V2  -0.60  0.02 0.36 0.64   1
V10  0.57  0.04 0.33 0.67   1
V8   0.00  0.65 0.43 0.57   1
V6  -0.01  0.65 0.42 0.58   1
V7   0.04  0.62 0.39 0.61   1
V14  0.03 -0.61 0.37 0.63   1
V15 -0.03 -0.60 0.37 0.63   1
V16  0.01 -0.59 0.34 0.66   1
V5  -0.03  0.57 0.33 0.67   1
V13 -0.01 -0.53 0.28 0.72   1

                       MR1  MR2
SS loadings           3.15 2.92
Proportion Var        0.20 0.18
Cumulative Var        0.20 0.38
Proportion Explained  0.52 0.48
Cumulative Proportion 0.52 1.00

 With factor correlations of 
     MR1  MR2
MR1 1.00 0.08
MR2 0.08 1.00

Mean item complexity =  1
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  120  and the objective function was  4.36 with Chi Square of  2146.44
The degrees of freedom for the model are 89  and the objective function was  0.24 

The root mean square of the residuals (RMSR) is  0.03 
The df corrected root mean square of the residuals is  0.03 

The harmonic number of observations is  500 with the empirical chi square  91.95  with prob <  0.39 
The total number of observations was  500  with Likelihood Chi Square =  116.25  with prob <  0.028 

Tucker Lewis Index of factoring reliability =  0.982
RMSEA index =  0.026  and the 90 % confidence intervals are  0.009 0.037
BIC =  -436.85
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                MR1  MR2
Correlation of scores with factors             0.92 0.91
Multiple R square of scores with factors       0.84 0.82
Minimum correlation of possible factor scores  0.68 0.65
> fa.organize(test.simple,c(2,1))  #the factors but not the items have been rearranged
Factor Analysis using method =  minres
Call: fa(r = sim.item(16), nfactors = 2)
Standardized loadings (pattern matrix) based upon correlation matrix
      MR2   MR1   h2   u2 com
V1  -0.03 -0.61 0.38 0.62   1
V2   0.02 -0.60 0.36 0.64   1
V3   0.06 -0.67 0.45 0.55   1
V4   0.01 -0.62 0.38 0.62   1
V5   0.57 -0.03 0.33 0.67   1
V6   0.65 -0.01 0.42 0.58   1
V7   0.62  0.04 0.39 0.61   1
V8   0.65  0.00 0.43 0.57   1
V9  -0.01  0.62 0.39 0.61   1
V10  0.04  0.57 0.33 0.67   1
V11  0.06  0.65 0.43 0.57   1
V12 -0.01  0.66 0.44 0.56   1
V13 -0.53 -0.01 0.28 0.72   1
V14 -0.61  0.03 0.37 0.63   1
V15 -0.60 -0.03 0.37 0.63   1
V16 -0.59  0.01 0.34 0.66   1

                       MR2  MR1
SS loadings           2.92 3.15
Proportion Var        0.18 0.20
Cumulative Var        0.18 0.38
Proportion Explained  0.48 0.52
Cumulative Proportion 0.48 1.00

 With factor correlations of 
     MR2  MR1
MR2 1.00 0.08
MR1 0.08 1.00

Mean item complexity =  1
Test of the hypothesis that 2 factors are sufficient.

The degrees of freedom for the null model are  120  and the objective function was  4.36 with Chi Square of  2146.44
The degrees of freedom for the model are 89  and the objective function was  0.24 

The root mean square of the residuals (RMSR) is  0.03 
The df corrected root mean square of the residuals is  0.03 

The harmonic number of observations is  500 with the empirical chi square  91.95  with prob <  0.39 
The total number of observations was  500  with Likelihood Chi Square =  116.25  with prob <  0.028 

Tucker Lewis Index of factoring reliability =  0.982
RMSEA index =  0.026  and the 90 % confidence intervals are  0.009 0.037
BIC =  -436.85
Fit based upon off diagonal values = 0.99
Measures of factor score adequacy             
                                                MR2  MR1
Correlation of scores with factors             0.91 0.92
Multiple R square of scores with factors       0.82 0.84
Minimum correlation of possible factor scores  0.65 0.68
> 
> 
> 
> cleanEx()
> nameEx("faMulti")
> ### * faMulti
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fa.multi
> ### Title: Multi level (hierarchical) factor analysis
> ### Aliases: fa.multi fa.multi.diagram
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> f31 <- fa.multi(Thurstone,3,1) #compare with \code{omega}
> f31
$f1
Factor Analysis using method =  minres
Call: fa(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, 
    scores = scores, residuals = residuals, SMC = SMC, covar = covar, 
    missing = missing, impute = impute, min.err = min.err, max.iter = max.iter, 
    symmetric = symmetric, warnings = warnings, fm = fm, alpha = alpha, 
    oblique.scores = oblique.scores, np.obs = np.obs, use = use, 
    cor = cor)
Standardized loadings (pattern matrix) based upon correlation matrix
                    MR1   MR2   MR3   h2   u2 com
Sentences          0.90 -0.03  0.04 0.82 0.18 1.0
Vocabulary         0.89  0.06 -0.03 0.84 0.16 1.0
Sent.Completion    0.84  0.03  0.00 0.74 0.26 1.0
First.Letters      0.00  0.85  0.00 0.73 0.27 1.0
Four.Letter.Words -0.02  0.75  0.10 0.63 0.37 1.0
Suffixes           0.18  0.63 -0.08 0.50 0.50 1.2
Letter.Series      0.03 -0.01  0.84 0.73 0.27 1.0
Pedigrees          0.38 -0.05  0.46 0.51 0.49 2.0
Letter.Group      -0.06  0.21  0.63 0.52 0.48 1.2

                       MR1  MR2  MR3
SS loadings           2.65 1.87 1.49
Proportion Var        0.29 0.21 0.17
Cumulative Var        0.29 0.50 0.67
Proportion Explained  0.44 0.31 0.25
Cumulative Proportion 0.44 0.75 1.00

 With factor correlations of 
     MR1  MR2  MR3
MR1 1.00 0.59 0.53
MR2 0.59 1.00 0.52
MR3 0.53 0.52 1.00

Mean item complexity =  1.2
Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  36  and the objective function was  5.2
The degrees of freedom for the model are 12  and the objective function was  0.01 

The root mean square of the residuals (RMSR) is  0.01 
The df corrected root mean square of the residuals is  0.01 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1  MR2  MR3
Correlation of scores with factors             0.96 0.92 0.90
Multiple R square of scores with factors       0.93 0.85 0.82
Minimum correlation of possible factor scores  0.86 0.71 0.63

$f2
Factor Analysis using method =  minres
Call: fa(r = f1$Phi, nfactors = nfact2, rotate = rotate)
Standardized loadings (pattern matrix) based upon correlation matrix
     MR1   h2   u2 com
MR1 0.78 0.61 0.39   1
MR2 0.76 0.57 0.43   1
MR3 0.68 0.46 0.54   1

                MR1
SS loadings    1.65
Proportion Var 0.55

Mean item complexity =  1
Test of the hypothesis that 1 factor is sufficient.

The degrees of freedom for the null model are  3  and the objective function was  0.86
The degrees of freedom for the model are 0  and the objective function was  0 

The root mean square of the residuals (RMSR) is  0 
The df corrected root mean square of the residuals is  NA 

Fit based upon off diagonal values = 1
Measures of factor score adequacy             
                                                MR1
Correlation of scores with factors             0.89
Multiple R square of scores with factors       0.79
Minimum correlation of possible factor scores  0.58

> fa.multi.diagram(f31)
> 
> 
> 
> cleanEx()
> nameEx("factor.congruence")
> ### * factor.congruence
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.congruence
> ### Title: Coefficient of factor congruence
> ### Aliases: factor.congruence fa.congruence
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #factor congruence of factors and components, both rotated
> #fa <- fa(Harman74.cor$cov,4)
> #pc <- principal(Harman74.cor$cov,4)
> #factor.congruence(fa,pc)
>  #    RC1  RC3  RC2  RC4
> #MR1 0.98 0.41 0.28 0.32
> #MR3 0.35 0.96 0.41 0.31
> #MR2 0.23 0.16 0.95 0.28
> #MR4 0.28 0.38 0.36 0.98
> 
> 
> 
> #factor congruence without rotation
> #fa <- fa(Harman74.cor$cov,4,rotate="none")
> #pc <- principal(Harman74.cor$cov,4,rotate="none")
> #factor.congruence(fa,pc)   #just show the beween method congruences
> #     PC1   PC2   PC3   PC4
> #MR1 1.00 -0.04 -0.06 -0.01
> #MR2 0.15  0.97 -0.01 -0.15
> #MR3 0.31  0.05  0.94  0.11
> #MR4 0.07  0.21 -0.12  0.96
> 
> #factor.congruence(list(fa,pc))  #this shows the within method congruence as well
> 
>  #     MR1   MR2  MR3   MR4  PC1   PC2   PC3   PC4
> #MR1  1.00  0.11 0.25  0.06 1.00 -0.04 -0.06 -0.01
> #MR2  0.11  1.00 0.06  0.07 0.15  0.97 -0.01 -0.15
> #MR3  0.25  0.06 1.00  0.01 0.31  0.05  0.94  0.11
> #MR4  0.06  0.07 0.01  1.00 0.07  0.21 -0.12  0.96
> #PC1  1.00  0.15 0.31  0.07 1.00  0.00  0.00  0.00
> #PC2 -0.04  0.97 0.05  0.21 0.00  1.00  0.00  0.00
> #PC3 -0.06 -0.01 0.94 -0.12 0.00  0.00  1.00  0.00
> #PC4 -0.01 -0.15 0.11  0.96 0.00  0.00  0.00  1.00
> 
> #pa <- fa(Harman74.cor$cov,4,fm="pa")
> # factor.congruence(fa,pa)
> #         PA1  PA3  PA2  PA4
> #Factor1 1.00 0.61 0.46 0.55
> #Factor2 0.61 1.00 0.50 0.60
> #Factor3 0.46 0.50 1.00 0.57
> #Factor4 0.56 0.62 0.58 1.00
> 
> 
> #compare with 
> #round(cor(fa$loading,pc$loading),2)
> #      RC1   RC3   RC2   RC4
> #MR1  0.99 -0.18 -0.33 -0.34
> #MR3 -0.33  0.96 -0.16 -0.43
> #MR2 -0.29 -0.46  0.98 -0.21
> #MR4 -0.44 -0.30 -0.22  0.98
> 
> 
> 
> 
> cleanEx()
> nameEx("factor.fit")
> ### * factor.fit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.fit
> ### Title: How well does the factor model fit a correlation matrix. Part of
> ###   the VSS package
> ### Aliases: factor.fit
> ### Keywords: models models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D #compare the fit of 4 to 3 factors for the Harman 24 variables
> ##D fa4 <- factanal(x,4,covmat=Harman74.cor$cov)
> ##D round(factor.fit(Harman74.cor$cov,fa4$loading),2)
> ##D #[1] 0.9
> ##D fa3 <- factanal(x,3,covmat=Harman74.cor$cov)
> ##D round(factor.fit(Harman74.cor$cov,fa3$loading),2)
> ##D #[1] 0.88
> ##D 
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("factor.model")
> ### * factor.model
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.model
> ### Title: Find R = F F' + U2 is the basic factor model
> ### Aliases: factor.model
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> f2 <- matrix(c(.9,.8,.7,rep(0,6),.6,.7,.8),ncol=2)
> mod <- factor.model(f2)
> round(mod,2)
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,] 0.81 0.72 0.63 0.00 0.00 0.00
[2,] 0.72 0.64 0.56 0.00 0.00 0.00
[3,] 0.63 0.56 0.49 0.00 0.00 0.00
[4,] 0.00 0.00 0.00 0.36 0.42 0.48
[5,] 0.00 0.00 0.00 0.42 0.49 0.56
[6,] 0.00 0.00 0.00 0.48 0.56 0.64
> 
> 
> 
> cleanEx()
> nameEx("factor.residuals")
> ### * factor.residuals
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.residuals
> ### Title: R* = R- F F'
> ### Aliases: factor.residuals
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> fa2 <- fa(Harman74.cor$cov,2,rotate=TRUE)
Specified rotation not found, rotate='none' used
>  fa2resid <- factor.residuals(Harman74.cor$cov,fa2)
>  fa2resid[1:4,1:4] #residuals with two factors extracted
                 VisualPerception      Cubes PaperFormBoard      Flags
VisualPerception        0.6533350 0.10174203      0.1656341 0.19205836
Cubes                   0.1017420 0.86326270      0.1638187 0.05229164
PaperFormBoard          0.1656341 0.16381874      0.8232249 0.10052094
Flags                   0.1920584 0.05229164      0.1005209 0.76340745
>  fa4 <- fa(Harman74.cor$cov,4,rotate=TRUE)
Specified rotation not found, rotate='none' used
>  fa4resid <- factor.residuals(Harman74.cor$cov,fa4)
>  fa4resid[1:4,1:4] #residuals with 4 factors extracted
                 VisualPerception       Cubes PaperFormBoard       Flags
VisualPerception       0.44982015 -0.03531402    -0.01016952  0.04145577
Cubes                 -0.03531402  0.77015627     0.04339997 -0.04969772
PaperFormBoard        -0.01016952  0.04339997     0.66152929 -0.02945683
Flags                  0.04145577 -0.04969772    -0.02945683  0.65020432
> 
> 
> 
> 
> cleanEx()
> nameEx("factor.rotate")
> ### * factor.rotate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.rotate
> ### Title: "Hand" rotate a factor loading matrix
> ### Aliases: factor.rotate
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #using the Harman 24 mental tests, rotate the 2nd and 3rd factors 45 degrees
> f4<- fa(Harman74.cor$cov,4,rotate="TRUE")
Specified rotation not found, rotate='none' used
> f4r45 <- factor.rotate(f4,45,2,3)
> f4r90 <- factor.rotate(f4r45,45,2,3)
> print(factor.congruence(f4,f4r45),digits=3) #poor congruence with original
    [,1] [,2]  [,3] [,4]
MR1    1 0.00  0.00    0
MR2    0 0.76 -0.76    0
MR3    0 0.65  0.65    0
MR4    0 0.00  0.00    1
> print(factor.congruence(f4,f4r90),digits=3) #factor 2 and 3 have been exchanged and 3 flipped
    [,1] [,2] [,3] [,4]
MR1    1    0    0    0
MR2    0    0   -1    0
MR3    0    1    0    0
MR4    0    0    0    1
> 
> #a graphic example
> data(Harman23.cor)
> f2 <- fa(Harman23.cor$cov,2,rotate="none")
> op <- par(mfrow=c(1,2))
> cluster.plot(f2,xlim=c(-1,1),ylim=c(-1,1),title="Unrotated ")
> f2r <- factor.rotate(f2,-33,plot=TRUE,xlim=c(-1,1),ylim=c(-1,1),title="rotated -33 degrees")
> op <- par(mfrow=c(1,1))
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("factor.scores")
> ### * factor.scores
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.scores
> ### Title: Various ways to estimate factor scores for the factor analysis
> ###   model
> ### Aliases: factor.scores
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> f3 <- fa(Thurstone)
> f3$weights  #just the scoring weights
                         MR1
Sentences         0.18680979
Vocabulary        0.23056715
Sent.Completion   0.15762958
First.Letters     0.13887845
Four.Letter.Words 0.13244727
Suffixes          0.09233885
Letter.Series     0.10932454
Pedigrees         0.11176173
Letter.Group      0.10401145
> f5 <- fa(bfi,5)
> round(cor(f5$scores,use="pairwise"),2)
      MR2   MR1   MR3   MR5   MR4
MR2  1.00 -0.23 -0.23 -0.05 -0.02
MR1 -0.23  1.00  0.33  0.44  0.25
MR3 -0.23  0.33  1.00  0.24  0.25
MR5 -0.05  0.44  0.24  1.00  0.19
MR4 -0.02  0.25  0.25  0.19  1.00
> #compare to the f5 solution
> 
> 
> 
> 
> cleanEx()
> nameEx("factor.stats")
> ### * factor.stats
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor.stats
> ### Title: Find various goodness of fit statistics for factor analysis and
> ###   principal components
> ### Aliases: factor.stats fa.stats
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> v9 <- sim.hierarchical()
> f3 <- fa(v9,3)
> factor.stats(v9,f3,n.obs=500)
The estimated weights for the factor scores are probably incorrect.  Try a different factor extraction method.
Call: fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, 
    alpha = alpha, fm = fm)

Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the model is 12  and the fit was  0.5 
The number of observations was  500  with Chi Square =  244.88  with prob <  1.6e-45 

Measures of factor score adequacy             
 Correlation of scores with factors            1.06 0.95 0.8
Multiple R square of scores with factors       1.13 0.9 0.63
Minimum correlation of factor score estimates  1.25 0.8 0.27 
> f3o <- fa(v9,3,fm="pa",rotate="Promax")
> factor.stats(v9,f3o,n.obs=500)
Call: fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, 
    alpha = alpha, fm = fm)

Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the model is 12  and the fit was  0.44 
The number of observations was  500  with Chi Square =  216.22  with prob <  1.4e-39 

Measures of factor score adequacy             
 Correlation of scores with factors            1 0.91 0.78
Multiple R square of scores with factors       0.99 0.83 0.61
Minimum correlation of factor score estimates  0.98 0.66 0.21 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("factor2cluster")
> ### * factor2cluster
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: factor2cluster
> ### Title: Extract cluster definitions from factor loadings
> ### Aliases: factor2cluster
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> ## Not run: 
> ##D f  <- factanal(x,4,covmat=Harman74.cor$cov)
> ##D factor2cluster(f) 
> ## End(Not run)
> #                       Factor1 Factor2 Factor3 Factor4
> #VisualPerception             0       1       0       0
> #Cubes                        0       1       0       0
> #PaperFormBoard               0       1       0       0
> #Flags                        0       1       0       0
> #GeneralInformation           1       0       0       0
> #PargraphComprehension        1       0       0       0
> #SentenceCompletion           1       0       0       0
> #WordClassification           1       0       0       0
> #WordMeaning                  1       0       0       0
> #Addition                     0       0       1       0
> #Code                         0       0       1       0
> #CountingDots                 0       0       1       0
> #StraightCurvedCapitals       0       0       1       0
> #WordRecognition              0       0       0       1
> #NumberRecognition            0       0       0       1
> #FigureRecognition            0       0       0       1
> #ObjectNumber                 0       0       0       1
> #NumberFigure                 0       0       0       1
> #FigureWord                   0       0       0       1
> #Deduction                    0       1       0       0
> #NumericalPuzzles             0       0       1       0
> #ProblemReasoning             0       1       0       0
> #SeriesCompletion             0       1       0       0
> #ArithmeticProblems           0       0       1       0
> 
> 
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("fisherz")
> ### * fisherz
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: fisherz
> ### Title: Transformations of r including Fisher r to z and z to r and
> ###   confidence intervals
> ### Aliases: fisherz fisherz2r r.con r2c r2t r2d d2r t2r g2r chi2r r2chi
> ###   cor2cov
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
>  n <- 30
>  r <- seq(0,.9,.1)
>  d <- r2d(r)
>  rc <- matrix(r.con(r,n),ncol=2)
>  t <- r*sqrt(n-2)/sqrt(1-r^2)
>  p <- (1-pt(t,n-2))*2
>  r1 <- t2r(t,(n-2))
>  r2 <- d2r(d)
>  chi <- r2chi(r,n)
>  r3 <- chi2r(chi,n)
>  r.rc <- data.frame(r=r,z=fisherz(r),lower=rc[,1],upper=rc[,2],t=t,p=p,d=d,
+      chi2=chi,d2r=r2,t2r=r1,chi2r=r3)
>  round(r.rc,2)
     r    z lower upper     t    p    d chi2 d2r t2r chi2r
1  0.0 0.00 -0.36  0.36  0.00 1.00 0.00  0.0 0.0 0.0   0.0
2  0.1 0.10 -0.27  0.44  0.53 0.60 0.20  0.3 0.1 0.1   0.1
3  0.2 0.20 -0.17  0.52  1.08 0.29 0.41  1.2 0.2 0.2   0.2
4  0.3 0.31 -0.07  0.60  1.66 0.11 0.63  2.7 0.3 0.3   0.3
5  0.4 0.42  0.05  0.66  2.31 0.03 0.87  4.8 0.4 0.4   0.4
6  0.5 0.55  0.17  0.73  3.06 0.00 1.15  7.5 0.5 0.5   0.5
7  0.6 0.69  0.31  0.79  3.97 0.00 1.50 10.8 0.6 0.6   0.6
8  0.7 0.87  0.45  0.85  5.19 0.00 1.96 14.7 0.7 0.7   0.7
9  0.8 1.10  0.62  0.90  7.06 0.00 2.67 19.2 0.8 0.8   0.8
10 0.9 1.47  0.80  0.95 10.93 0.00 4.13 24.3 0.9 0.9   0.9
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("galton")
> ### * galton
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: galton
> ### Title: Galton's Mid parent child height data
> ### Aliases: galton
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(galton)
> describe(galton)
       vars   n  mean   sd median trimmed  mad  min  max range  skew kurtosis
parent    1 928 68.31 1.79   68.5   68.32 1.48 64.0 73.0     9 -0.04     0.05
child     2 928 68.09 2.52   68.2   68.12 2.97 61.7 73.7    12 -0.09    -0.35
         se
parent 0.06
child  0.08
>  #show the scatter plot and the lowess fit 
> pairs.panels(galton,main="Galton's Parent child heights")  
> #but this makes the regression lines look the same
> pairs.panels(galton,lm=TRUE,main="Galton's Parent child heights") 
>  #better is to scale them 
> pairs.panels(galton,lm=TRUE,xlim=c(62,74),ylim=c(62,74),main="Galton's Parent child heights") 
> 
> 
> 
> cleanEx()
> nameEx("geometric.mean")
> ### * geometric.mean
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: geometric.mean
> ### Title: Find the geometric mean of a vector or columns of a data.frame.
> ### Aliases: geometric.mean
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> x <- seq(1,5)
> x2 <- x^2
> x2[2] <- NA
> X <- data.frame(x,x2)
> geometric.mean(x)
[1] 2.605171
> geometric.mean(x2)
[1] 7.745967
> geometric.mean(X)
       x       x2 
2.605171 7.745967 
> geometric.mean(X,na.rm=FALSE)
       x       x2 
2.605171       NA 
> 
> 
> 
> 
> cleanEx()
> nameEx("glb.algebraic")
> ### * glb.algebraic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glb.algebraic
> ### Title: Find the greatest lower bound to reliability.
> ### Aliases: glb.algebraic
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> Cv<-matrix(c(215, 64, 33, 22,
+               64, 97, 57, 25,
+               33, 57,103, 36,
+               22, 25, 36, 77),ncol=4)
> 
> Cv                    # covariance matrix of a test with 4 subtests
     [,1] [,2] [,3] [,4]
[1,]  215   64   33   22
[2,]   64   97   57   25
[3,]   33   57  103   36
[4,]   22   25   36   77
> Cr<-cov2cor(Cv)       # Correlation matrix of tests
> if(!require(Rcsdp)) {print("Rcsdp must be installed to find the glb.algebraic")} else {
+  glb.algebraic(Cv)     # glb of total score
+ glb.algebraic(Cr)      # glb of sum of standardized scores
+ 
+  w<-c(1,2,2,1)         # glb of weighted total score
+  glb.algebraic(diag(w) %*% Cv %*% diag(w))  
+ alphas <- c(0.8,0,0,0) # Internal consistency of first test is known
+ 
+ glb.algebraic(Cv,LoBounds=alphas*diag(Cv))
+ 
+                       # Fix all diagonal elements to 1 but the first:
+ 
+ lb <- glb.algebraic(Cr,LoBounds=c(0,1,1,1),UpBounds=c(1,1,1,1))
+ lb$solution[1]        # should be the same as the squared mult. corr.
+ smc(Cr)[1] 
+ }                        
Loading required package: Rcsdp
Iter:  0 Ap: 0.00e+00 Pobj: -4.3220280e+04 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -6.5339946e+04 Ad: 8.85e-01 Dobj:  6.4422339e+02 
Iter:  2 Ap: 1.00e+00 Pobj: -3.5006850e+04 Ad: 1.00e+00 Dobj:  3.7009700e+02 
Iter:  3 Ap: 9.96e-01 Pobj: -1.5931469e+03 Ad: 1.00e+00 Dobj:  3.6825944e+02 
Iter:  4 Ap: 1.00e+00 Pobj: -1.4818035e+02 Ad: 1.00e+00 Dobj:  3.2971128e+02 
Iter:  5 Ap: 1.00e+00 Pobj:  7.2073312e+01 Ad: 8.95e-01 Dobj:  2.5243291e+02 
Iter:  6 Ap: 9.37e-01 Pobj:  2.0335167e+02 Ad: 1.00e+00 Dobj:  2.4993271e+02 
Iter:  7 Ap: 1.00e+00 Pobj:  2.2938390e+02 Ad: 1.00e+00 Dobj:  2.4319719e+02 
Iter:  8 Ap: 1.00e+00 Pobj:  2.4049679e+02 Ad: 1.00e+00 Dobj:  2.4221143e+02 
Iter:  9 Ap: 1.00e+00 Pobj:  2.4168448e+02 Ad: 1.00e+00 Dobj:  2.4202895e+02 
Iter: 10 Ap: 1.00e+00 Pobj:  2.4194772e+02 Ad: 1.00e+00 Dobj:  2.4200461e+02 
Iter: 11 Ap: 1.00e+00 Pobj:  2.4199268e+02 Ad: 1.00e+00 Dobj:  2.4200026e+02 
Iter: 12 Ap: 1.00e+00 Pobj:  2.4199959e+02 Ad: 1.00e+00 Dobj:  2.4199968e+02 
Iter: 13 Ap: 1.00e+00 Pobj:  2.4199998e+02 Ad: 1.00e+00 Dobj:  2.4199999e+02 
Iter: 14 Ap: 9.60e-01 Pobj:  2.4200000e+02 Ad: 9.60e-01 Dobj:  2.4200000e+02 
Success: SDP solved
Primal objective value: 2.4200000e+02 
Dual objective value: 2.4200000e+02 
Relative primal infeasibility: 2.41e-15 
Relative dual infeasibility: 1.46e-09 
Real Relative Gap: 4.15e-10 
XZ Relative Gap: 1.48e-09 
DIMACS error measures: 3.62e-15 0.00e+00 3.60e-09 0.00e+00 4.15e-10 1.48e-09
Iter:  0 Ap: 0.00e+00 Pobj: -3.5138439e+02 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -5.5512055e+02 Ad: 8.93e-01 Dobj:  6.1816777e+00 
Iter:  2 Ap: 1.00e+00 Pobj: -3.4368039e+02 Ad: 1.00e+00 Dobj:  3.0269284e+00 
Iter:  3 Ap: 9.92e-01 Pobj: -1.5590727e+01 Ad: 1.00e+00 Dobj:  3.0162195e+00 
Iter:  4 Ap: 1.00e+00 Pobj: -4.9252222e-02 Ad: 1.00e+00 Dobj:  2.8319820e+00 
Iter:  5 Ap: 1.00e+00 Pobj:  8.5779647e-01 Ad: 7.47e-01 Dobj:  2.1897644e+00 
Iter:  6 Ap: 1.00e+00 Pobj:  2.0259656e+00 Ad: 1.00e+00 Dobj:  2.1759295e+00 
Iter:  7 Ap: 1.00e+00 Pobj:  2.1325604e+00 Ad: 1.00e+00 Dobj:  2.1570553e+00 
Iter:  8 Ap: 1.00e+00 Pobj:  2.1541861e+00 Ad: 1.00e+00 Dobj:  2.1553504e+00 
Iter:  9 Ap: 9.99e-01 Pobj:  2.1552060e+00 Ad: 1.00e+00 Dobj:  2.1552527e+00 
Iter: 10 Ap: 1.00e+00 Pobj:  2.1552494e+00 Ad: 1.00e+00 Dobj:  2.1552490e+00 
Iter: 11 Ap: 1.00e+00 Pobj:  2.1552512e+00 Ad: 1.00e+00 Dobj:  2.1552513e+00 
Iter: 12 Ap: 9.56e-01 Pobj:  2.1552513e+00 Ad: 9.62e-01 Dobj:  2.1552513e+00 
Success: SDP solved
Primal objective value: 2.1552513e+00 
Dual objective value: 2.1552513e+00 
Relative primal infeasibility: 3.42e-14 
Relative dual infeasibility: 4.70e-10 
Real Relative Gap: 3.92e-10 
XZ Relative Gap: 7.38e-10 
DIMACS error measures: 5.13e-14 0.00e+00 1.38e-09 0.00e+00 3.92e-10 7.38e-10
Iter:  0 Ap: 0.00e+00 Pobj: -9.5927938e+04 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -1.4494089e+05 Ad: 8.66e-01 Dobj:  1.4964170e+03 
Iter:  2 Ap: 1.00e+00 Pobj: -7.9270724e+04 Ad: 1.00e+00 Dobj:  8.1997812e+02 
Iter:  3 Ap: 9.92e-01 Pobj: -3.5142097e+03 Ad: 1.00e+00 Dobj:  8.1871268e+02 
Iter:  4 Ap: 1.00e+00 Pobj:  1.7422906e+02 Ad: 1.00e+00 Dobj:  7.7344263e+02 
Iter:  5 Ap: 1.00e+00 Pobj:  4.6579250e+02 Ad: 8.74e-01 Dobj:  6.7336534e+02 
Iter:  6 Ap: 1.00e+00 Pobj:  6.4721018e+02 Ad: 1.00e+00 Dobj:  6.7048257e+02 
Iter:  7 Ap: 1.00e+00 Pobj:  6.6502669e+02 Ad: 1.00e+00 Dobj:  6.6825147e+02 
Iter:  8 Ap: 1.00e+00 Pobj:  6.6786329e+02 Ad: 1.00e+00 Dobj:  6.6801172e+02 
Iter:  9 Ap: 1.00e+00 Pobj:  6.6799452e+02 Ad: 1.00e+00 Dobj:  6.6799965e+02 
Iter: 10 Ap: 1.00e+00 Pobj:  6.6799978e+02 Ad: 1.00e+00 Dobj:  6.6799977e+02 
Iter: 11 Ap: 1.00e+00 Pobj:  6.6799999e+02 Ad: 1.00e+00 Dobj:  6.6800000e+02 
Success: SDP solved
Primal objective value: 6.6799999e+02 
Dual objective value: 6.6800000e+02 
Relative primal infeasibility: 9.69e-16 
Relative dual infeasibility: 5.15e-09 
Real Relative Gap: 3.79e-09 
XZ Relative Gap: 7.05e-09 
DIMACS error measures: 1.45e-15 0.00e+00 1.58e-08 0.00e+00 3.79e-09 7.05e-09
Iter:  0 Ap: 0.00e+00 Pobj: -2.8110751e+04 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -5.5032462e+04 Ad: 8.82e-01 Dobj:  7.2636232e+02 
Iter:  2 Ap: 1.00e+00 Pobj: -3.3334596e+04 Ad: 1.00e+00 Dobj:  3.9883283e+02 
Iter:  3 Ap: 9.89e-01 Pobj: -1.4687104e+03 Ad: 1.00e+00 Dobj:  3.9845129e+02 
Iter:  4 Ap: 1.00e+00 Pobj:  1.5243207e+02 Ad: 1.00e+00 Dobj:  3.8636150e+02 
Iter:  5 Ap: 1.00e+00 Pobj:  2.3716023e+02 Ad: 8.25e-01 Dobj:  3.3487516e+02 
Iter:  6 Ap: 9.72e-01 Pobj:  3.0917685e+02 Ad: 1.00e+00 Dobj:  3.2865718e+02 
Iter:  7 Ap: 1.00e+00 Pobj:  3.1984847e+02 Ad: 1.00e+00 Dobj:  3.2502325e+02 
Iter:  8 Ap: 1.00e+00 Pobj:  3.2392994e+02 Ad: 1.00e+00 Dobj:  3.2442411e+02 
Iter:  9 Ap: 1.00e+00 Pobj:  3.2430611e+02 Ad: 1.00e+00 Dobj:  3.2433621e+02 
Iter: 10 Ap: 1.00e+00 Pobj:  3.2433035e+02 Ad: 1.00e+00 Dobj:  3.2433120e+02 
Iter: 11 Ap: 1.00e+00 Pobj:  3.2433135e+02 Ad: 1.00e+00 Dobj:  3.2433137e+02 
Iter: 12 Ap: 9.59e-01 Pobj:  3.2433139e+02 Ad: 9.60e-01 Dobj:  3.2433139e+02 
Success: SDP solved
Primal objective value: 3.2433139e+02 
Dual objective value: 3.2433139e+02 
Relative primal infeasibility: 4.53e-15 
Relative dual infeasibility: 4.08e-09 
Real Relative Gap: 6.60e-10 
XZ Relative Gap: 3.22e-09 
DIMACS error measures: 6.80e-15 0.00e+00 1.15e-08 0.00e+00 6.60e-10 3.22e-09
Iter:  0 Ap: 0.00e+00 Pobj: -8.7846097e+01 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -3.4230783e+02 Ad: 8.72e-01 Dobj:  7.3242446e+00 
Iter:  2 Ap: 1.00e+00 Pobj: -2.1121211e+02 Ad: 9.07e-01 Dobj:  3.6963525e+00 
Iter:  3 Ap: 1.00e+00 Pobj: -2.4656600e+01 Ad: 8.88e-01 Dobj:  3.7848085e+00 
Iter:  4 Ap: 9.63e-01 Pobj:  1.1595651e+00 Ad: 9.23e-01 Dobj:  3.7466943e+00 
Iter:  5 Ap: 9.55e-01 Pobj:  2.7900529e+00 Ad: 9.22e-01 Dobj:  3.4224111e+00 
Iter:  6 Ap: 1.00e+00 Pobj:  3.0938546e+00 Ad: 7.99e-01 Dobj:  3.2363902e+00 
Iter:  7 Ap: 1.00e+00 Pobj:  3.1910734e+00 Ad: 9.02e-01 Dobj:  3.2063734e+00 
Iter:  8 Ap: 1.00e+00 Pobj:  3.2001619e+00 Ad: 1.00e+00 Dobj:  3.2015363e+00 
Iter:  9 Ap: 9.97e-01 Pobj:  3.2012470e+00 Ad: 1.00e+00 Dobj:  3.2013111e+00 
Iter: 10 Ap: 9.98e-01 Pobj:  3.2012972e+00 Ad: 1.00e+00 Dobj:  3.2013016e+00 
Iter: 11 Ap: 1.00e+00 Pobj:  3.2013014e+00 Ad: 9.81e-01 Dobj:  3.2013015e+00 
Iter: 12 Ap: 9.59e-01 Pobj:  3.2013016e+00 Ad: 9.59e-01 Dobj:  3.2013016e+00 
Success: SDP solved
Primal objective value: 3.2013016e+00 
Dual objective value: 3.2013016e+00 
Relative primal infeasibility: 1.33e-16 
Relative dual infeasibility: 2.66e-09 
Real Relative Gap: 7.79e-10 
XZ Relative Gap: 3.69e-09 
DIMACS error measures: 2.00e-16 0.00e+00 9.11e-09 0.00e+00 7.79e-10 3.69e-09
       V1 
0.2013016 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:Rcsdp’

> nameEx("guttman")
> ### * guttman
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: splitHalf
> ### Title: Alternative estimates of test reliabiity
> ### Aliases: splitHalf guttman tenberge glb glb.fa
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> data(attitude)
> splitHalf(attitude)
Split half reliabilities  
Call: splitHalf(r = attitude)

Maximum split half reliability (lambda 4) =  0.89
Guttman lambda 6                          =  0.88
Average split half reliability            =  0.81
Guttman lambda 3 (alpha)                  =  0.84
Minimum split half reliability  (beta)    =  0.74> splitHalf(attitude,covar=TRUE) #do it on the covariances
Split half reliabilities  
Call: splitHalf(r = attitude, covar = TRUE)

Maximum split half reliability (lambda 4) =  0.91
Guttman lambda 6                          =  0.72
Average split half reliability            =  0.82
Guttman lambda 3 (alpha)                  =  0.84
Minimum split half reliability  (beta)    =  0.76> glb(attitude)
$beta
[1] 0.7023614

$beta.factor
[1] 0.6005961

$alpha.pc
[1] 0.7309206

$glb.max
[1] 0.9108864

$glb.IC
[1] 0.9108864

$glb.Km
[1] 0.9108864

$glb.Fa
[1] 0.9011208

$r.smc
[1] 0.8752151

$tenberge
$tenberge$mu0
[1] 0.8390838

$tenberge$mu1
[1] 0.8518997

$tenberge$mu2
[1] 0.8544578

$tenberge$mu3
[1] 0.8549927


$keys
           IC1 IC2 ICr1 ICr2 K1 K2 F1 F2 f1 f2
rating       1   0    1    0  0  1  0  1  1  0
complaints   1   0    0    1  1  0  0  1  1  0
privileges   1   0    1    0  0  1  1  0  1  0
learning     1   0    1    0  0  1  1  0  1  0
raises       0   1    1    0  0  1  1  0  1  0
critical     0   1    1    0  0  1  1  0  0  1
advance      0   1    0    1  1  0  0  1  0  1

> glb.fa(attitude)
$glb
[1] 0.9062287

$communality
    rating complaints privileges   learning     raises   critical    advance 
 0.7733609  0.8981735  0.3500094  0.7634485  0.7666944  0.3268903  0.7836959 

$numf
[1] 3

$Call
glb.fa(r = attitude)

> if(require(Rcsdp)) {glb.algebraic(cor(attitude)) }
Loading required package: Rcsdp
Iter:  0 Ap: 0.00e+00 Pobj: -1.0761147e+03 Ad: 0.00e+00 Dobj:  0.0000000e+00 
Iter:  1 Ap: 1.00e+00 Pobj: -1.6448466e+03 Ad: 8.85e-01 Dobj:  1.1921454e+01 
Iter:  2 Ap: 1.00e+00 Pobj: -1.0225918e+03 Ad: 9.31e-01 Dobj:  5.8368797e+00 
Iter:  3 Ap: 1.00e+00 Pobj: -1.7175253e+02 Ad: 1.00e+00 Dobj:  5.8768659e+00 
Iter:  4 Ap: 9.98e-01 Pobj: -5.1724971e+00 Ad: 1.00e+00 Dobj:  5.8558446e+00 
Iter:  5 Ap: 1.00e+00 Pobj:  1.9432167e+00 Ad: 1.00e+00 Dobj:  5.4487022e+00 
Iter:  6 Ap: 1.00e+00 Pobj:  4.1417423e+00 Ad: 9.54e-01 Dobj:  5.0827903e+00 
Iter:  7 Ap: 1.00e+00 Pobj:  4.8316437e+00 Ad: 1.00e+00 Dobj:  5.0471250e+00 
Iter:  8 Ap: 1.00e+00 Pobj:  4.9762636e+00 Ad: 1.00e+00 Dobj:  5.0222998e+00 
Iter:  9 Ap: 1.00e+00 Pobj:  5.0100742e+00 Ad: 1.00e+00 Dobj:  5.0177689e+00 
Iter: 10 Ap: 1.00e+00 Pobj:  5.0156201e+00 Ad: 1.00e+00 Dobj:  5.0169234e+00 
Iter: 11 Ap: 1.00e+00 Pobj:  5.0165759e+00 Ad: 1.00e+00 Dobj:  5.0167658e+00 
Iter: 12 Ap: 1.00e+00 Pobj:  5.0167300e+00 Ad: 1.00e+00 Dobj:  5.0167390e+00 
Iter: 13 Ap: 1.00e+00 Pobj:  5.0167425e+00 Ad: 1.00e+00 Dobj:  5.0167428e+00 
Iter: 14 Ap: 9.60e-01 Pobj:  5.0167430e+00 Ad: 9.60e-01 Dobj:  5.0167430e+00 
Success: SDP solved
Primal objective value: 5.0167430e+00 
Dual objective value: 5.0167430e+00 
Relative primal infeasibility: 2.89e-15 
Relative dual infeasibility: 1.96e-09 
Real Relative Gap: 8.86e-10 
XZ Relative Gap: 2.27e-09 
DIMACS error measures: 5.27e-15 0.00e+00 8.56e-09 0.00e+00 8.86e-10 2.27e-09
$glb
[1] 0.9204472

$solution
    rating complaints privileges   learning     raises   critical    advance 
 0.8381632  1.0000000  0.4528276  0.9011539  0.8384801  0.3085488  0.6775696 

$status
[1] 0

$Call
glb.algebraic(Cov = cor(attitude))

> guttman(attitude)
Warning: Guttman has been deprecated.  The use of the splitHalf function is recommended
Call: guttman(r = attitude)

Alternative estimates of reliability

Guttman bounds 
L1 =  0.72 
L2 =  0.85 
L3 (alpha) =  0.84 
L4 (max) =  0.89 
L5 =  0.83 
L6 (smc) =  0.88 
TenBerge bounds 
mu0 =  0.84 mu1 =  0.85 mu2 =  0.85 mu3 =  0.85 

alpha of first PC =  0.85 
estimated greatest lower bound based upon communalities=  0.91 

beta found by splitHalf  =  0.74 
> 
> #to show the histogram of all possible splits for the ability test
> #sp <- splitHalf(ability,raw=TRUE)  #this saves the results
> #hist(sp$raw,breaks=101,ylab="SplitHalf reliability",main="SplitHalf 
> #    reliabilities of a test with 16 ability items")
> sp <- splitHalf(bfi[1:10],key=c(1,9,10))
> 
> 
> 
> 
> cleanEx()

detaching ‘package:Rcsdp’

> nameEx("harmonic.mean")
> ### * harmonic.mean
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: harmonic.mean
> ### Title: Find the harmonic mean of a vector, matrix, or columns of a
> ###   data.frame
> ### Aliases: harmonic.mean
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> x <- seq(1,5)
> x2 <- x^2
> x2[2] <- NA
> y <- x - 1
> X <- data.frame(x,x2,y)
> harmonic.mean(x)
[1] 2.189781
> harmonic.mean(x2)
[1] 3.295949
> harmonic.mean(X)
       x       x2        y 
2.189781 3.295949 0.000000 
> harmonic.mean(X,na.rm=FALSE)
       x       x2        y 
2.189781       NA 0.000000 
> harmonic.mean(X,zero=FALSE)
       x       x2        y 
2.189781 3.295949 1.920000 
> 
> 
> 
> 
> cleanEx()
> nameEx("headtail")
> ### * headtail
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: headTail
> ### Title: Combine calls to head and tail
> ### Aliases: headtail headTail topBottom quickView
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> headTail(iqitems,4,8,to=6) #the first 4 and last 6 items from 1 to 6
     reason.4 reason.16 reason.17 reason.19 letter.7 letter.33
5           3         3         6         3        5         3
6           3         3         4         4        6         2
7           3         4         4         2        6         5
8           4         0         6         1        5         1
...       ...       ...       ...       ...      ...       ...
1835        4         4         4         4        6         3
1836        4         4         4         6        6         3
1837        3         2         4         5        6         3
1838        4         4         4         6        6         3
1839        4         4         4         6        6         3
1840        2         4         4         3        2         5
1841        4         4         4         6        6         0
1843        4         4         4         4        6         3
> topBottom(ability,from =2, to = 6) #the first and last 4 items from 2 to 6
     reason.16 reason.17 reason.19 letter.7 letter.33
5            0         0         0        0         1
6            0         1         0        1         0
7            1         1         0        1         0
8           NA         0         0        0         0
1839         1         1         1        1         1
1840         1         1         0        0         0
1841         1         1         1        1        NA
1843         1         1         0        1         1
> headTail(bfi,top=4, bottom=4,from =6, to=10) #the first and last 4 from 6 to 10
       C1  C2  C3  C4  C5
61617   2   3   3   4   4
61618   5   4   4   3   4
61620   4   5   4   2   5
61621   4   4   3   5   5
...   ... ... ... ... ...
67552   2   3   4   4   3
67556   5   5   5   1   1
67559   5   5   5   2   6
67560   5   5   3   3   3
> #not shown
> #quickView(ability,hlength=10,tlength=10)  #this loads a spreadsheet like table  
> 
> 
> 
> cleanEx()
> nameEx("heights")
> ### * heights
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: heights
> ### Title: A data.frame of the Galton (1888) height and cubit data set.
> ### Aliases: heights
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(heights)
> ellipses(heights,n=1,main="Galton's co-relation data set")
> 
> 
> 
> 
> cleanEx()
> nameEx("iclust.diagram")
> ### * iclust.diagram
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: iclust.diagram
> ### Title: Draw an ICLUST hierarchical cluster structure diagram
> ### Aliases: iclust.diagram ICLUST.diagram
> ### Keywords: multivariate cluster hplot
> 
> ### ** Examples
> 
> v9 <- sim.hierarchical()
> v9c <- ICLUST(v9)
> test.data <- Harman74.cor$cov
> ic.out <- ICLUST(test.data)
> #now show how to relabel clusters
> ic.bfi <- iclust(bfi[1:25],beta=3) #find the clusters
> cluster.names <- rownames(ic.bfi$results) #get the old names
> #change the names to the desired ones
> cluster.names[c(16,19,18,15,20)] <- c("Neuroticism","Extra-Open","Agreeableness",
+       "Conscientiousness","Open")
> #now show the new names
> iclust.diagram(ic.bfi,cluster.names=cluster.names,min.size=4,e.size=1.75)
> 
> 
> 
> 
> cleanEx()
> nameEx("income")
> ### * income
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: income
> ### Title: US family income from US census 2008
> ### Aliases: income all.income
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(income)
> with(income[1:40,], plot(mean,prop, main="US family income for 2008",xlab="income", 
+         ylab="Proportion of families",xlim=c(0,100000)))
> with (income[1:40,], points(lowess(mean,prop,f=.3),typ="l"))
> describe(income)
      vars  n     mean       sd   median  trimmed      mad    min       max
value    1 44 60227.27 49125.55 53750.00 53750.00 40771.50   0.00 250000.00
count    2 44  2663.25  2033.63  2280.50  2385.19  1161.62 889.00  14286.00
mean     3 44 66270.18 68666.23 54841.50 54824.56 40803.38 298.00 415517.00
prop     4 44     0.02     0.02     0.02     0.02     0.01   0.01      0.12
          range skew kurtosis       se
value 250000.00 1.75     4.11  7405.95
count  13397.00 4.21    21.39   306.58
mean  415219.00 3.20    12.99 10351.82
prop       0.11 4.21    21.39     0.00
> 
> 
> with(all.income, plot(mean,prop, main="US family income for 2008",xlab="income", 
+                 ylab="Proportion of families",xlim=c(0,250000)))
> with (all.income[1:50,], points(lowess(mean,prop,f=.25),typ="l"))
> #curve(100000* dlnorm(x, 10.8, .8), x = c(0,250000),ylab="Proportion")
> 
> 
> 
> cleanEx()
> nameEx("interp.median")
> ### * interp.median
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: interp.median
> ### Title: Find the interpolated sample median, quartiles, or specific
> ###   quantiles for a vector, matrix, or data frame
> ### Aliases: interp.median interp.quantiles interp.quartiles interp.boxplot
> ###   interp.values interp.qplot.by interp.q interp.quart
> ### Keywords: univar
> 
> ### ** Examples
> 
> interp.median(c(1,2,3,3,3))  # compare with median = 3
[1] 2.666667
> interp.median(c(1,2,2,5))
[1] 2
> interp.quantiles(c(1,2,2,5),.25)
> x <- sample(10,100,TRUE)
> interp.quartiles(x)
      Q1   Median       Q3 
3.571429 5.357143 7.966667 
> #
> x <-  c(1,1,2,2,2,3,3,3,3,4,5,1,1,1,2,2,3,3,3,3,4,5,1,1,1,2,2,3,3,3,3,4,2)
> y <-  c(1,2,3,3,3,3,4,4,4,4,4,1,2,3,3,3,3,4,4,4,4,5,1,5,3,3,3,3,4,4,4,4,4)
> x <-  x[order(x)]   #sort the data by ascending order to make it clearer
> y <- y[order(y)]
> xv <- interp.values(x)
> yv <- interp.values(y)
> barplot(x,space=0,xlab="ordinal position",ylab="value")
> lines(1:length(x)-.5,xv)
> points(c(length(x)/4,length(x)/2,3*length(x)/4),interp.quartiles(x))
> barplot(y,space=0,xlab="ordinal position",ylab="value")
> lines(1:length(y)-.5,yv)
> points(c(length(y)/4,length(y)/2,3*length(y)/4),interp.quartiles(y))
> data(galton)
> interp.median(galton)
       parent    child
[1,] 68.32877 68.18333
> interp.qplot.by(galton$child,galton$parent,ylab="child height"
+ ,xlab="Mid parent height") 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("iqitems")
> ### * iqitems
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: iqitems
> ### Title: 16 multiple choice IQ items
> ### Aliases: iqitems
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D data(iqitems)
> ##D iq.keys <- c(4,4,4, 6,  6,3,4,4,   5,2,2,4,   3,2,6,7)
> ##D score.multiple.choice(iq.keys,iqitems)   #this just gives summary statisics
> ##D #convert them to true false 
> ##D iq.scrub <- scrub(iqitems,isvalue=0)  #first get rid of the zero responses
> ##D iq.tf <-  score.multiple.choice(iq.keys,iq.scrub,score=FALSE) 
> ##D               #convert to wrong (0) and correct (1) for analysis
> ##D describe(iq.tf) 
> ##D #see the ability data set for these analyses
> ##D #now, for some item analysis
> ##D #iq.irt <- irt.fa(iq.tf)  #do a basic irt
> ##D #iq.sc <-score.irt(iq.irt,iq.tf)  #find the scores
> ##D #op <- par(mfrow=c(4,4))
> ##D #irt.responses(iq.sc[,1], iq.tf)  
> ##D #op <- par(mfrow=c(1,1))
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("irt.fa")
> ### * irt.fa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: irt.fa
> ### Title: Item Response Analysis by Exploratory Factor Analysis of
> ###   tetrachoric/polychoric correlations
> ### Aliases: irt.fa irt.select fa2irt
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D set.seed(17)
> ##D d9 <- sim.irt(9,1000,-2.5,2.5,mod="normal") #dichotomous items
> ##D test <- irt.fa(d9$items)
> ##D test 
> ##D op <- par(mfrow=c(3,1))
> ##D plot(test,type="ICC")
> ##D plot(test,type="IIC")
> ##D plot(test,type="test")
> ##D par(op)
> ##D set.seed(17)
> ##D items <- sim.congeneric(N=500,short=FALSE,categorical=TRUE) #500 responses to 4 discrete items
> ##D d4 <- irt.fa(items$observed)  #item response analysis of congeneric measures
> ##D d4    #show just the irt output
> ##D d4$fa  #show just the factor analysis output
> ##D 
> ##D 
> ##D op <- par(mfrow=c(2,2))
> ##D plot(d4,type="ICC")
> ##D par(op)
> ##D 
> ##D 
> ##D #using the iq data set for an example of real items
> ##D #first need to convert the responses to tf
> ##D data(iqitems)
> ##D iq.keys <- c(4,4,4, 6, 6,3,4,4,  5,2,2,4,  3,2,6,7)
> ##D 
> ##D iq.tf <- score.multiple.choice(iq.keys,iqitems,score=FALSE)  #just the responses
> ##D iq.irt <- irt.fa(iq.tf)
> ##D print(iq.irt,short=FALSE) #show the IRT as well as factor analysis output
> ##D p.iq <- plot(iq.irt)  #save the invisible summary table
> ##D p.iq  #show the summary table of information by ability level
> ##D #select a subset of these variables
> ##D small.iq.irt <- irt.select(iq.irt,c(1,5,9,10,11,13))
> ##D small.irt <- irt.fa(small.iq.irt)
> ##D plot(small.irt)
> ##D #find the information for three subset of iq items
> ##D keys <- make.keys(16,list(all=1:16,some=c(1,5,9,10,11,13),others=c(1:5)))
> ##D plot(iq.irt,keys=keys)
> ## End(Not run)
> #compare output to the ltm package or Kamata and Bauer   -- these are in logistic units 
> ls <- irt.fa(lsat6)

> #library(ltm)
> # lsat.ltm <- ltm(lsat6~z1)
> #  round(coefficients(lsat.ltm)/1.702,3)  #convert to normal (approximation)
> #
> #   Dffclt Dscrmn
> #Q1 -1.974  0.485
> #Q2 -0.805  0.425
> #Q3 -0.164  0.523
> #Q4 -1.096  0.405
> #Q5 -1.835  0.386
> 
> 
> #Normal results  ("Standardized and Marginal")(from Akihito Kamata )       
> #Item       discrim             tau 
> #  1       0.4169             -1.5520   
> #  2       0.4333             -0.5999 
> #  3       0.5373             -0.1512 
> #  4       0.4044             -0.7723  
> #  5       0.3587             -1.1966
> #compare to ls 
> 
>   #Normal results  ("Standardized and conditional") (from Akihito Kamata )   
> #item            discrim   tau
> #  1           0.3848    -1.4325  
> #  2           0.3976    -0.5505 
> #  3           0.4733    -0.1332 
> #  4           0.3749    -0.7159 
> #  5           0.3377    -1.1264 
> #compare to ls$fa and ls$tau 
> 
> #Kamata and Bauer (2008) logistic estimates
> #1   0.826    2.773
> #2   0.723    0.990
> #3   0.891    0.249  
> #4   0.688    1.285
> #5   0.657    2.053
>  
>  
> 
>  
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("irt.responses")
> ### * irt.responses
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: irt.responses
> ### Title: Plot probability of multiple choice responses as a function of a
> ###   latent trait
> ### Aliases: irt.responses
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> data(iqitems)
> iq.keys <- c(4,4,4, 6,6,3,4,4,  5,2,2,4,  3,2,6,7)
> scores <- score.multiple.choice(iq.keys,iqitems,score=TRUE,short=FALSE)
> #note that for speed we can just do this on simple item counts rather
> # than IRT based scores.
> op <- par(mfrow=c(2,2))  #set this to see the output for multiple items
> irt.responses(scores$scores,iqitems[1:4],breaks=11)
> op <-  par(op)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("kaiser")
> ### * kaiser
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kaiser
> ### Title: Apply the Kaiser normalization when rotating factors
> ### Aliases: kaiser
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> f3 <- fa(Thurstone,3)
> f3n <- kaiser(fa(Thurstone,3,rotate="none"))
> f3p <- kaiser(fa(Thurstone,3,rotate="none"),rotate="Promax",m=3)
> factor.congruence(list(f3,f3n,f3p))
     MR1  MR2  MR3  MR1  MR2  MR3  MR1  MR2  MR3
MR1 1.00 0.06 0.09 1.00 0.10 0.10 1.00 0.07 0.06
MR2 0.06 1.00 0.08 0.04 1.00 0.11 0.03 1.00 0.09
MR3 0.09 0.08 1.00 0.10 0.07 1.00 0.05 0.03 1.00
MR1 1.00 0.04 0.10 1.00 0.08 0.10 1.00 0.06 0.06
MR2 0.10 1.00 0.07 0.08 1.00 0.09 0.07 1.00 0.08
MR3 0.10 0.11 1.00 0.10 0.09 1.00 0.06 0.05 1.00
MR1 1.00 0.03 0.05 1.00 0.07 0.06 1.00 0.05 0.02
MR2 0.07 1.00 0.03 0.06 1.00 0.05 0.05 1.00 0.04
MR3 0.06 0.09 1.00 0.06 0.08 1.00 0.02 0.04 1.00
> 
> 
> 
> cleanEx()
> nameEx("kappa")
> ### * kappa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cohen.kappa
> ### Title: Find Cohen's kappa and weighted kappa coefficients for
> ###   correlation of two raters
> ### Aliases: wkappa cohen.kappa
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> #rating data (with thanks to Tim Bates)
> rater1 = c(1,2,3,4,5,6,7,8,9) # rater one's ratings
> rater2 = c(1,3,1,6,1,5,5,6,7) # rater one's ratings
> cohen.kappa(x=cbind(rater1,rater2))
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa -0.18     0.00  0.18
weighted kappa    0.43     0.68  0.93

 Number of subjects = 9 
> 
> #data matrix taken from Cohen
> cohen <- matrix(c(
+ 0.44, 0.07, 0.09,
+ 0.05, 0.20, 0.05,
+ 0.01, 0.03, 0.06),ncol=3,byrow=TRUE)
> 
> #cohen.weights  weight differences
> cohen.weights <- matrix(c(
+ 0,1,3,
+ 1,0,6,
+ 3,6,0),ncol=3)
> 
> 
> cohen.kappa(cohen,cohen.weights,n.obs=200)
Warning in cohen.kappa1(x, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.39     0.49  0.59
weighted kappa   -0.39     0.35  1.00

 Number of subjects = 200 
> #cohen reports .492 and .348 
> 
> #another set of weights
> #what if the weights are non-symmetric
> wc <- matrix(c(
+ 0,1,4,
+ 1,0,6,
+ 2,2,0),ncol=3,byrow=TRUE)
> cohen.kappa(cohen,wc)  
Warning in cohen.kappa1(x, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa -0.92     0.49     1
weighted kappa   -1.00     0.35     1

 Number of subjects = 1 
> #Cohen reports kw = .353
> 
> cohen.kappa(cohen,n.obs=200)  #this uses the squared weights
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.39     0.49  0.59
weighted kappa    0.32     0.45  0.58

 Number of subjects = 200 
> 
> fleiss.cohen <- 1 - cohen.weights/9
> cohen.kappa(cohen,fleiss.cohen,n.obs=200)
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.39     0.49  0.59
weighted kappa    0.20     0.35  0.50

 Number of subjects = 200 
> 
> #however, Fleiss, Cohen and Everitt weight similarities
> fleiss <- matrix(c(
+ 106, 10,4,
+ 22,28, 10,
+ 2, 12,  6),ncol=3,byrow=TRUE)
> 
> #Fleiss weights the similarities
> weights <- matrix(c(
+  1.0000, 0.0000, 0.4444,
+  0.0000, 1.0000, 0.6667,
+  0.4444, 0.6667, 1.0000),ncol=3)
>  
>  cohen.kappa(fleiss,weights,n.obs=200)
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.32     0.43  0.53
weighted kappa    0.40     0.51  0.62

 Number of subjects = 200 
>  
>  #another example is comparing the scores of two sets of twins
>  #data may be a 2 column matrix
>  #compare weighted and unweighted
>  #also look at the ICC for this data set.
>  twins <- matrix(c(
+     1, 2, 
+     2, 3,
+     3, 4,
+     5, 6,
+     6, 7), ncol=2,byrow=TRUE)
>   cohen.kappa(twins)
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate  upper
unweighted kappa -0.23    -0.14 -0.046
weighted kappa    0.80     0.87  0.942

 Number of subjects = 5 
>   
> #data may be explicitly categorical
> x <- c("red","yellow","blue","red")
> y <- c("red",  "blue", "blue" ,"red") 
> xy.df <- data.frame(x,y)
> ck <- cohen.kappa(xy.df)
Warning in cohen.kappa1(x, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
> ck
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                  lower estimate upper
unweighted kappa  0.098      0.6  1.00
weighted kappa   -0.693      0.0  0.69

 Number of subjects = 4 
> ck$agree
        x2f
x1f      blue  red yellow
  blue   0.25 0.00   0.00
  red    0.00 0.50   0.00
  yellow 0.25 0.00   0.00
> 
> #The problem of missing categories (from Amy Finnegan)
> numbers <- data.frame(rater1=c(6,3,7,8,7),
+                       rater2=c(6,1,8,5,10))
> cohen.kappa(numbers)  #compare with the next analysis
Warning in cohen.kappa1(x, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                  lower estimate upper
unweighted kappa -0.161     0.13  0.42
weighted kappa   -0.037     0.53  1.00

 Number of subjects = 5 
> cohen.kappa(numbers,levels=1:10)  #specify the number of levels 
Warning in cohen.kappa1(x, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)

Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa -0.16     0.13  0.42
weighted kappa    0.23     0.62  1.00

 Number of subjects = 5 
>               #   these leads to slightly higher weighted kappa
>   
> #finally, input can be a data.frame of ratings from more than two raters
> ratings <- matrix(rep(1:5,4),ncol=4)
> ratings[1,2] <- ratings[2,3] <- ratings[3,4] <- NA
> ratings[2,1] <- ratings[3,2] <- ratings[4,3] <- 1
> cohen.kappa(ratings)
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 
Warning in cohen.kappa1(x1, w = w, n.obs = n.obs, alpha = alpha, levels = levels) :
  upper or lower confidence interval exceed  abs(1)  and set to +/- 1. 

Cohen Kappa (below the diagonal) and Weighted Kappa (above the diagonal) 
For confidence intervals and detail print with all=TRUE
     R1   R2   R3   R4
R1 1.00 0.74 0.67 0.92
R2 0.38 1.00 0.48 1.00
R3 0.67 0.14 1.00 0.80
R4 0.67 1.00 0.50 1.00
>  
>  
>  #In the case of confidence intervals being artificially truncated to +/- 1, it is 
>  #helpful to compare the results of a boot strap resample
>  #ck.boot <-function(x,s=1:nrow(x)) {cohen.kappa(x[s,])$kappa}
>  #library(boot)
>  #ckb <- boot(x,ck.boot,R=1000)
>  #hist(ckb$t)
>  
> 
> 
> 
> cleanEx()
> nameEx("logistic")
> ### * logistic
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: logistic
> ### Title: Logistic transform from x to p and logit transform from p to x
> ### Aliases: logistic logit logistic.grm
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> curve(logistic(x,a=1.702),-3,3,ylab="Probability of x",
+  main="Logistic transform of x",xlab="z score units") 
>  #logistic with a=1.702 is almost the same as pnorm 
>  
> curve(pnorm(x),add=TRUE,lty="dashed")  
> curve(logistic(x),add=TRUE)
> text(2,.8, expression(alpha ==1))
> text(2,1.0,expression(alpha==1.7))
> curve(logistic(x),-4,4,ylab="Probability of x",
+             main = "Logistic transform of x in logit units",xlab="logits")
> curve(logistic(x,d=-1),add=TRUE)
> curve(logistic(x,d=1),add=TRUE)
> curve(logistic(x,c=.2),add=TRUE,lty="dashed")
> text(1.3,.5,"d=1")
> text(.3,.5,"d=0")
> text(-1.5,.5,"d=-1")
> text(-3,.3,"c=.2")
> #demo of graded response model
>  curve(logistic.grm(x,r=1),-4,4,ylim=c(0,1),main="Five level response scale",
+            ylab="Probability of endorsement",xlab="Latent attribute on logit scale")
>  curve(logistic.grm(x,r=2),add=TRUE)
>  curve(logistic.grm(x,r=3),add=TRUE)
>  curve(logistic.grm(x,r=4),add=TRUE)
>  curve(logistic.grm(x,r=5),add=TRUE)
>  
>  text(-2.,.5,1)
>  text(-1.,.4,2)
>  text(0,.4,3)
>  text(1.,.4,4)
>   text(2.,.4,5)
> 
> 
> 
> cleanEx()
> nameEx("lowerUpper")
> ### * lowerUpper
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: lowerUpper
> ### Title: Combine two square matrices to have a lower off diagonal for
> ###   one, upper off diagonal for the other
> ### Aliases: lowerUpper
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>  b1 <- Bechtoldt.1
>  b2 <- Bechtoldt.2
>  b12 <- lowerUpper(b1,b2)
>  cor.plot(b12)
>  diff12 <- lowerUpper(b1,b2,diff=TRUE)
> 
>  cor.plot(t(diff12),numbers=TRUE,main="Bechtoldt1 and the differences from Bechtoldt2")
> 
> 
> 
> 
> cleanEx()
> nameEx("make.keys")
> ### * make.keys
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: make.keys
> ### Title: Create a keys matrix for use by score.items or cluster.cor
> ### Aliases: make.keys keys2list
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> data(attitude)  #specify the items by location
>  key.list <- list(all=c(1,2,3,4,-5,6,7),
+                   first=c(1,2,3),
+                   last=c(4,5,6,7))
>  keys <- make.keys(7,key.list,item.labels = colnames(attitude))
>  keys
           all first last
rating       1     1    0
complaints   1     1    0
privileges   1     1    0
learning     1     0    1
raises      -1     0    1
critical     1     0    1
advance      1     0    1
>  #now, undo this 
> new.keys.list <- keys2list(keys)  #note, these are now given the variable names
> 
>  
>  #scores <- score.items(keys,attitude)
>  #scores
>  
>  data(bfi)
>  #first create the keys by location (the conventional way)
>  keys.list <- list(agree=c(-1,2:5),conscientious=c(6:8,-9,-10),
+  extraversion=c(-11,-12,13:15),neuroticism=c(16:20),openness = c(21,-22,23,24,-25))   
>  keys <- make.keys(25,keys.list,item.labels=colnames(bfi)[1:25])
>  new.keys.list <- keys2list(keys)  #these will be in the form of variable names
>  
>  #alternatively, create by a mixture of names and locations 
>  keys.list <- list(agree=c("-A1","A2","A3","A4","A5"),
+ conscientious=c("C1","C2","C2","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"),
+ neuroticism=c(16:20),openness = c(21,-22,23,24,-25)) 
> keys <- make.keys(bfi,keys.list) #specify the data file to be scored (bfi)
> #or
> keys <- make.keys(colnames(bfi),keys.list) #specify the names of the variables to be used
> #or
> #specify the number of variables to be scored and their names in all cases
> keys <- make.keys(28,keys.list,colnames(bfi)) 
> 
> 
>  scores <- scoreItems(keys,bfi)
>  summary(scores)
Call: scoreItems(keys = keys, items = bfi)

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, (unstandardized) alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.25          0.69         0.37      -0.334     0.33
extraversion   0.46          0.27         0.76      -0.284     0.32
neuroticism   -0.18         -0.25        -0.22       0.812    -0.12
openness       0.15          0.21         0.22      -0.086     0.60
> 
> 
> 
> 
> cleanEx()
> nameEx("mat.sort")
> ### * mat.sort
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mat.sort
> ### Title: Sort the elements of a correlation matrix to reflect factor
> ###   loadings
> ### Aliases: mat.sort
> ### Keywords: multivariate models
> 
> ### ** Examples
> data(Bechtoldt.1)
> sorted <- mat.sort(Bechtoldt.1,fa(Bechtoldt.1,5))
> cor.plot(sorted)
> 
> 
> 
> cleanEx()
> nameEx("matrix.addition")
> ### * matrix.addition
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: matrix.addition
> ### Title: A function to add two vectors or matrices
> ### Aliases: matrix.addition %+%
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> x <- seq(1,4)
> z <- x %+% -t(x)
> x
[1] 1 2 3 4
> z
     [,1] [,2] [,3] [,4]
[1,]    0   -1   -2   -3
[2,]    1    0   -1   -2
[3,]    2    1    0   -1
[4,]    3    2    1    0
> #compare with outer(x,-x,FUN="+")
> x <- matrix(seq(1,6),ncol=2)
> y <- matrix(seq(1,10),nrow=2)
> z <- x %+% y
> x
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
> y
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9
[2,]    2    4    6    8   10
> z
     [,1] [,2] [,3] [,4] [,5]
[1,]    8   12   16   20   24
[2,]   10   14   18   22   26
[3,]   12   16   20   24   28
> #but compare this with outer(x ,y,FUN="+") 
> 
> 
> 
> cleanEx()
> nameEx("mediate")
> ### * mediate
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mediate
> ### Title: Estimate and display direct and indirect effects of mediators
> ###   and moderator in path models
> ### Aliases: mediate mediate.diagram moderate.diagram
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #data from Preacher and Hayes (2004)
> sobel <- structure(list(SATIS = c(-0.59, 1.3, 0.02, 0.01, 0.79, -0.35, 
+ -0.03, 1.75, -0.8, -1.2, -1.27, 0.7, -1.59, 0.68, -0.39, 1.33, 
+ -1.59, 1.34, 0.1, 0.05, 0.66, 0.56, 0.85, 0.88, 0.14, -0.72, 
+ 0.84, -1.13, -0.13, 0.2), THERAPY = structure(c(0, 1, 1, 0, 1, 
+ 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 
+ 1, 1, 1, 0), value.labels = structure(c(1, 0), .Names = c("cognitive", 
+ "standard"))), ATTRIB = c(-1.17, 0.04, 0.58, -0.23, 0.62, -0.26, 
+ -0.28, 0.52, 0.34, -0.09, -1.09, 1.05, -1.84, -0.95, 0.15, 0.07, 
+ -0.1, 2.35, 0.75, 0.49, 0.67, 1.21, 0.31, 1.97, -0.94, 0.11, 
+ -0.54, -0.23, 0.05, -1.07)), .Names = c("SATIS", "THERAPY", "ATTRIB"
+ ), row.names = c(NA, -30L), class = "data.frame", variable.labels = structure(c("Satisfaction", 
+ "Therapy", "Attributional Positivity"), .Names = c("SATIS", "THERAPY", 
+ "ATTRIB")))
>  #n.iter set to 50 (instead of default of 5000) for speed of example
> mediate(1,2,3,sobel,n.iter=50)  #The example in Preacher and Hayes
Call: mediate(y = 1, x = 2, m = 3, data = sobel, n.iter = 50)

The DV (Y) was  SATIS . The IV (X) was  THERAPY . The mediating variable(s) =  ATTRIB .

Total Direct effect(c) of  THERAPY  on  SATIS  =  0.76   S.E. =  0.31  t direct =  2.5   with probability =  0.019
Direct effect (c') of  THERAPY  on  SATIS  removing  ATTRIB  =  0.43   S.E. =  0.32  t direct =  1.35   with probability =  0.19
Indirect effect (ab) of  THERAPY  on  SATIS  through  ATTRIB   =  0.33 
Mean bootstrapped indirect effect =  0.34  with standard error =  0.21  Lower CI =  0.05    Upper CI =  0.79
R2 of model =  0.31
 To see the longer output, specify short = FALSE in the print statement

 Full output  

 Total effect estimates (c) 
        SATIS   se   t   Prob
THERAPY  0.76 0.31 2.5 0.0186

Direct effect estimates     (c') 
        SATIS   se    t  Prob
THERAPY  0.43 0.32 1.35 0.190
ATTRIB   0.40 0.18 2.23 0.034

 'a'  effect estimates 
       THERAPY  se    t   Prob
ATTRIB    0.82 0.3 2.74 0.0106

 'b'  effect estimates 
       SATIS   se    t  Prob
ATTRIB   0.4 0.18 2.23 0.034

 'ab'  effect estimates 
        SATIS boot   sd lower upper
THERAPY  0.33 0.34 0.21  0.05  0.79
> 
> #the pmi covariance matrix from Hayes. 2013.
> #data set from Hayes, 2013 has 123 cases instead of the covariance matrix used here 
> 
> C.pmi <- structure(c(0.251232840197254, 0.119718779155005, 0.157470345195255, 
+ 0.124533519925363, 0.03052112488338, 0.0734039717446355, 0.119718779155005, 
+ 1.74573503931761, 0.647207783553245, 0.914575836332134, 0.0133613221378115, 
+ -0.0379181660669066, 0.157470345195255, 0.647207783553245, 3.01572704251633, 
+ 1.25128282020525, -0.0224576835932294, 0.73973743835799, 0.124533519925363, 
+ 0.914575836332134, 1.25128282020525, 2.40342196454751, -0.0106624017059843, 
+ -0.752990470478475, 0.03052112488338, 0.0133613221378115, -0.0224576835932294, 
+ -0.0106624017059843, 0.229241636678662, 0.884479541516727, 0.0734039717446355, 
+ -0.0379181660669066, 0.73973743835799, -0.752990470478475, 0.884479541516727, 
+ 33.6509729441557), .Dim = c(6L, 6L), .Dimnames = list(c("cond", 
+ "pmi", "import", "reaction", "gender", "age"), c("cond", "pmi", 
+ "import", "reaction", "gender", "age")))
> 
>  #n.iter set to 50 (instead of default of 5000) for speed of example
> mediate(y="reaction",x = "cond",m=c("pmi","import"),data=C.pmi,n.obs=123,n.iter=50)
The replication data matrices were simulated based upon the specified number of subjects and the observed correlation matrix.
Call: mediate(y = "reaction", x = "cond", m = c("pmi", "import"), data = C.pmi, 
    n.obs = 123, n.iter = 50)

The DV (Y) was  reaction . The IV (X) was  cond . The mediating variable(s) =  pmi import .

Total Direct effect(c) of  cond  on  reaction  =  0.5   S.E. =  0.28  t direct =  1.79   with probability =  0.077
Direct effect (c') of  cond  on  reaction  removing  pmi import  =  0.1   S.E. =  0.24  t direct =  0.43   with probability =  0.67
Indirect effect (ab) of  cond  on  reaction  through  pmi import   =  0.39 
Mean bootstrapped indirect effect =  0.49  with standard error =  0.15  Lower CI =  0.21    Upper CI =  0.73
R2 of model =  0.33
 To see the longer output, specify short = FALSE in the print statement

 Full output  

 Total effect estimates (c) 
     reaction   se    t   Prob
cond      0.5 0.28 1.79 0.0766

Direct effect estimates     (c') 
       reaction   se    t     Prob
cond       0.10 0.24 0.43 6.66e-01
pmi        0.40 0.09 4.26 4.04e-05
import     0.32 0.07 4.59 1.13e-05

 'a'  effect estimates 
       cond   se    t   Prob
pmi    0.48 0.24 2.02 0.0452
import 0.63 0.31 2.02 0.0452

 'b'  effect estimates 
       reaction   se    t     Prob
pmi        0.40 0.09 4.26 4.04e-05
import     0.32 0.07 4.59 1.13e-05

 'ab'  effect estimates 
     reaction boot   sd lower upper
cond     0.39 0.49 0.15  0.21  0.73
> 
> 
> 
> #Data from sem package taken from Kerckhoff (and in turn, from Lisrel manual)
> R.kerch <- structure(list(Intelligence = c(1, -0.1, 0.277, 0.25, 0.572, 
+ 0.489, 0.335), Siblings = c(-0.1, 1, -0.152, -0.108, -0.105, 
+ -0.213, -0.153), FatherEd = c(0.277, -0.152, 1, 0.611, 0.294, 
+ 0.446, 0.303), FatherOcc = c(0.25, -0.108, 0.611, 1, 0.248, 0.41, 
+ 0.331), Grades = c(0.572, -0.105, 0.294, 0.248, 1, 0.597, 0.478
+ ), EducExp = c(0.489, -0.213, 0.446, 0.41, 0.597, 1, 0.651), 
+     OccupAsp = c(0.335, -0.153, 0.303, 0.331, 0.478, 0.651, 1
+     )), .Names = c("Intelligence", "Siblings", "FatherEd", "FatherOcc", 
+ "Grades", "EducExp", "OccupAsp"), class = "data.frame", row.names = c("Intelligence", 
+ "Siblings", "FatherEd", "FatherOcc", "Grades", "EducExp", "OccupAsp"
+ ))
> 
>  #n.iter set to 50 (instead of default of 5000) for speed of demo
> mod.k <- mediate("OccupAsp","Intelligence",m= c(2:5),data=R.kerch,n.obs=767,n.iter=50)
The replication data matrices were simulated based upon the specified number of subjects and the observed correlation matrix.
> mediate.diagram(mod.k) 
> #print the path values 
> mod.k
Call: mediate(y = "OccupAsp", x = "Intelligence", m = c(2:5), data = R.kerch, 
    n.obs = 767, n.iter = 50)

The DV (Y) was  OccupAsp . The IV (X) was  Intelligence . The mediating variable(s) =  Siblings FatherEd FatherOcc Grades .

Total Direct effect(c) of  Intelligence  on  OccupAsp  =  0.34   S.E. =  0.03  t direct =  9.83   with probability =  0
Direct effect (c') of  Intelligence  on  OccupAsp  removing  Siblings FatherEd FatherOcc Grades  =  0.05   S.E. =  0.04  t direct =  1.29   with probability =  0.2
Indirect effect (ab) of  Intelligence  on  OccupAsp  through  Siblings FatherEd FatherOcc Grades   =  0.29 
Mean bootstrapped indirect effect =  0.3  with standard error =  0.03  Lower CI =  0.26    Upper CI =  0.34
R2 of model =  0.29
 To see the longer output, specify short = FALSE in the print statement

 Full output  

 Total effect estimates (c) 
             OccupAsp   se    t Prob
Intelligence     0.34 0.03 9.83    0

Direct effect estimates     (c') 
             OccupAsp   se     t     Prob
Intelligence     0.05 0.04  1.29 1.98e-01
Siblings        -0.08 0.03 -2.59 9.91e-03
FatherEd         0.05 0.04  1.35 1.77e-01
FatherOcc        0.18 0.04  4.70 3.03e-06
Grades           0.38 0.04 10.03 0.00e+00

 'a'  effect estimates 
          Intelligence   se     t Prob
Siblings         -0.10 0.04 -2.78    0
FatherEd          0.28 0.03  7.97    0
FatherOcc         0.25 0.04  7.14    0
Grades            0.57 0.03 19.29    0

 'b'  effect estimates 
          OccupAsp   se     t     Prob
Siblings     -0.08 0.03 -2.59 9.91e-03
FatherEd      0.05 0.04  1.35 1.77e-01
FatherOcc     0.18 0.04  4.70 3.03e-06
Grades        0.38 0.04 10.03 0.00e+00

 'ab'  effect estimates 
             OccupAsp boot   sd lower upper
Intelligence     0.29  0.3 0.03  0.26  0.34
> 
> #Compare the following solution to the path coefficients found by the sem package
> 
> mod.k2 <- mediate(y="OccupAsp",x=c("Intelligence","Siblings","FatherEd","FatherOcc"),
+      m= c(5:6),data=R.kerch,n.obs=767,n.iter=50)
The replication data matrices were simulated based upon the specified number of subjects and the observed correlation matrix.
> mediate.diagram(mod.k2,show.c=FALSE) #simpler output 
> 
> #print the path values
> mod.k2
Call: mediate(y = "OccupAsp", x = c("Intelligence", "Siblings", "FatherEd", 
    "FatherOcc"), m = c(5:6), data = R.kerch, n.obs = 767, n.iter = 50)

The DV (Y) was  OccupAsp . The IV (X) was  Intelligence Siblings FatherEd FatherOcc . The mediating variable(s) =  Grades EducExp .

Total Direct effect(c) of  Intelligence  on  OccupAsp  =  0.25   S.E. =  0.03  t direct =  7.29   with probability =  7.6e-13
Direct effect (c') of  Intelligence  on  OccupAsp  removing  Grades EducExp  =  -0.04   S.E. =  0.03  t direct =  -1.16   with probability =  0.25
Indirect effect (ab) of  Intelligence  on  OccupAsp  through  Grades EducExp   =  0.29 
Mean bootstrapped indirect effect =  0.27  with standard error =  0.02  Lower CI =  0.23    Upper CI =  0.3

Total Direct effect(c) of  Siblings  on  OccupAsp  =  -0.09   S.E. =  0.03  t direct =  -2.78   with probability =  0.0056
Direct effect (c') of  Siblings  on  NA  removing  Grades EducExp  =  -0.02   S.E. =  0.03  t direct =  -0.68   with probability =  0.5
Indirect effect (ab) of  Siblings  on  OccupAsp  through  Grades EducExp   =  -0.07 
Mean bootstrapped indirect effect =  0.27  with standard error =  0.02  Lower CI =  -0.14    Upper CI =  -0.09

Total Direct effect(c) of  FatherEd  on  OccupAsp  =  0.1   S.E. =  0.04  t direct =  2.36   with probability =  0.018
Direct effect (c') of  FatherEd  on  NA  removing  Grades EducExp  =  -0.04   S.E. =  0.04  t direct =  -1.16   with probability =  0.25
Indirect effect (ab) of  FatherEd  on  OccupAsp  through  Grades EducExp   =  0.14 
Mean bootstrapped indirect effect =  0.27  with standard error =  0.02  Lower CI =  0.09    Upper CI =  0.17

Total Direct effect(c) of  FatherOcc  on  OccupAsp  =  0.2   S.E. =  0.04  t direct =  4.8   with probability =  1.9e-06
Direct effect (c') of  FatherOcc  on  NA  removing  Grades EducExp  =  0.1   S.E. =  0.03  t direct =  2.85   with probability =  0.0044
Indirect effect (ab) of  FatherOcc  on  OccupAsp  through  Grades EducExp   =  0.1 
Mean bootstrapped indirect effect =  0.27  with standard error =  0.02  Lower CI =  0.05    Upper CI =  0.13
R2 of model =  0.44
 To see the longer output, specify short = FALSE in the print statement

 Full output  

 Total effect estimates (c) 
             OccupAsp   se     t     Prob
Intelligence     0.25 0.03  7.29 7.63e-13
Siblings        -0.09 0.03 -2.78 5.60e-03
FatherEd         0.10 0.04  2.36 1.84e-02
FatherOcc        0.20 0.04  4.80 1.90e-06

Direct effect estimates     (c') 
             OccupAsp   se     t    Prob
Intelligence    -0.04 0.03 -1.16 0.24600
Siblings        -0.02 0.03 -0.68 0.49800
FatherEd        -0.04 0.04 -1.16 0.24700
FatherOcc        0.10 0.03  2.85 0.00443

 'a'  effect estimates 
             Grades   se     t    Prob
Intelligence   0.53 0.03 17.16 0.00000
Siblings      -0.03 0.03 -1.01 0.31300
FatherEd       0.12 0.04  3.16 0.00162
FatherOcc      0.04 0.04  1.09 0.27500
             EducExp   se     t     Prob
Intelligence    0.37 0.03 12.45 0.00e+00
Siblings       -0.12 0.03 -4.27 2.17e-05
FatherEd        0.22 0.04  6.00 2.99e-09
FatherOcc       0.17 0.04  4.63 4.28e-06

 'b'  effect estimates 
        OccupAsp   se     t     Prob
Grades      0.16 0.04  4.29 2.06e-05
EducExp     0.55 0.04 14.60 0.00e+00

 'ab'  effect estimates 
             OccupAsp  boot   sd lower upper
Intelligence     0.29  0.27 0.02  0.05  0.13
Siblings        -0.07 -0.12 0.02  0.05  0.13
FatherEd         0.14  0.13 0.02  0.05  0.13
FatherOcc        0.10  0.09 0.02  0.05  0.13
> 
> 
> 
> 
> cleanEx()
> nameEx("misc")
> ### * misc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: psych.misc
> ### Title: Miscellaneous helper functions for the psych package
> ### Aliases: psych.misc misc tableF lowerCor lowerMat progressBar reflect
> ###   shannon test.all cor2 levels2numeric char2numeric isCorrelation
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> lowerMat(Thurstone)
                  Sntnc Vcblr Snt.C Frs.L F.L.W Sffxs Ltt.S Pdgrs Ltt.G
Sentences         1.00                                                 
Vocabulary        0.83  1.00                                           
Sent.Completion   0.78  0.78  1.00                                     
First.Letters     0.44  0.49  0.46  1.00                               
Four.Letter.Words 0.43  0.46  0.42  0.67  1.00                         
Suffixes          0.45  0.49  0.44  0.59  0.54  1.00                   
Letter.Series     0.45  0.43  0.40  0.38  0.40  0.29  1.00             
Pedigrees         0.54  0.54  0.53  0.35  0.37  0.32  0.56  1.00       
Letter.Group      0.38  0.36  0.36  0.42  0.45  0.32  0.60  0.45  1.00 
> lb <- lowerCor(bfi[1:10])  #finds and prints the lower correlation matrix, 
   A1    A2    A3    A4    A5    C1    C2    C3    C4    C5   
A1  1.00                                                      
A2 -0.34  1.00                                                
A3 -0.27  0.49  1.00                                          
A4 -0.15  0.34  0.36  1.00                                    
A5 -0.18  0.39  0.50  0.31  1.00                              
C1  0.03  0.09  0.10  0.09  0.12  1.00                        
C2  0.02  0.14  0.14  0.23  0.11  0.43  1.00                  
C3 -0.02  0.19  0.13  0.13  0.13  0.31  0.36  1.00            
C4  0.13 -0.15 -0.12 -0.15 -0.13 -0.34 -0.38 -0.34  1.00      
C5  0.05 -0.12 -0.16 -0.24 -0.17 -0.25 -0.30 -0.34  0.48  1.00
>   # returns the square matrix.
> #fiml <- corFiml(bfi[1:10])     #FIML correlations require lavaan package
> #lowerMat(fiml)  #to get pretty output
> f3 <- fa(Thurstone,3)
> f3r <- reflect(f3,2)  #reflect the second factor
> #find the complexity of the response patterns of the iqitems.
> round(shannon(iqitems),2) 
 reason.4 reason.16 reason.17 reason.19  letter.7 letter.33 letter.34 letter.58 
     1.78      1.52      1.62      1.86      1.88      2.01      1.89      2.29 
matrix.45 matrix.46 matrix.47 matrix.55  rotate.3  rotate.4  rotate.6  rotate.8 
     2.03      2.10      1.92      2.40      2.89      2.76      2.72      2.93 
> #test.all('BinNor')  #Does the BinNor package work when we are using other packages
> bestItems(lb,3,cut=.1)
      A3
A3  1.00
A5  0.50
A2  0.49
A4  0.36
A1 -0.27
C5 -0.16
C2  0.14
C3  0.13
C4 -0.12
> #to make this a latex table 
> #df2latex(bestItems(lb,2,cut=.2))
> #
> data(bfi.dictionary)
> f2 <- fa(bfi[1:10],2)
> fa.lookup(f2,bfi.dictionary)
     MR1   MR2  com   h2 ItemLabel                                      Item
C4 -0.65  0.00 1.00 0.42     q_626           Do things in a half-way manner.
C2  0.64 -0.01 1.00 0.40     q_530     Continue until everything is perfect.
C1  0.57 -0.06 1.02 0.31     q_124                   Am exacting in my work.
C5 -0.56 -0.06 1.02 0.34    q_1949                            Waste my time.
C3  0.54  0.03 1.01 0.31     q_619            Do things according to a plan.
A3 -0.03  0.76 1.00 0.56    q_1206               Know how to comfort others.
A2  0.01  0.68 1.00 0.46    q_1162         Inquire about others' well-being.
A5  0.03  0.60 1.00 0.37    q_1419                 Make people feel at ease.
A4  0.14  0.44 1.22 0.25    q_1364                            Love children.
A1  0.08 -0.41 1.08 0.15     q_146 Am indifferent to the feelings of others.
      Giant3              Big6        Little12 Keying IPIP100
C4 Stability Conscientiousness Industriousness     -1    B5:C
C2 Stability Conscientiousness     Orderliness      1    B5:C
C1 Stability Conscientiousness     Orderliness      1    B5:C
C5 Stability Conscientiousness Industriousness     -1    B5:C
C3 Stability Conscientiousness     Orderliness      1    B5:C
A3  Cohesion     Agreeableness      Compassion      1    B5:A
A2  Cohesion     Agreeableness      Compassion      1    B5:A
A5  Cohesion     Agreeableness      Compassion      1    B5:A
A4  Cohesion     Agreeableness      Compassion      1    B5:A
A1  Cohesion     Agreeableness      Compassion     -1    B5:A
> 
> sa1 <-sat.act[1:2]
> sa2 <- sat.act[3:4]
> sa3 <- sat.act[5:6]
> cor2(sa1,sa2)
            age   ACT
gender    -0.02 -0.04
education  0.55  0.15
> cor2(list(sa1,sa2))  #show within set and between set cors
          gendr edctn age   ACT  
gender     1.00                  
education  0.09  1.00            
age       -0.02  0.55  1.00      
ACT       -0.04  0.15  0.11  1.00
> cor2(list(sa1,sa2,sa3))
          gendr edctn age   ACT   SATV  SATQ 
gender     1.00                              
education  0.09  1.00                        
age       -0.02  0.55  1.00                  
ACT       -0.04  0.15  0.11  1.00            
SATV      -0.02  0.05 -0.04  0.56  1.00      
SATQ      -0.17  0.03 -0.03  0.59  0.64  1.00
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("mixed.cor")
> ### * mixed.cor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mixedCor
> ### Title: Find correlations for mixtures of continuous, polytomous, and
> ###   dichotomous variables
> ### Aliases: mixedCor mixed.cor
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> data(bfi) 
> r <- mixedCor(data=bfi[,c(1:5,26,28)])
  Biserial |                                                                                                    |   0%
> r
Call: mixedCor(data = bfi[, c(1:5, 26, 28)])
       A1    A2    A3    A4    A5    gendr age  
A1      1.00                                    
A2     -0.41  1.00                              
A3     -0.32  0.56  1.00                        
A4     -0.18  0.39  0.41  1.00                  
A5     -0.23  0.45  0.57  0.36  1.00            
gender -0.23  0.25  0.20  0.20  0.14  1.00      
age    -0.17  0.12  0.07  0.16  0.14  0.06  1.00
> #this is the same as
> r <- mixedCor(data=bfi,p=1:5,c=28,d=26)
  Biserial |                                                                                                    |   0%
> r #note how the variable order reflects the original order in the data
Call: mixedCor(data = bfi, c = 28, p = 1:5, d = 26)
       A1    A2    A3    A4    A5    gendr age  
A1      1.00                                    
A2     -0.41  1.00                              
A3     -0.32  0.56  1.00                        
A4     -0.18  0.39  0.41  1.00                  
A5     -0.23  0.45  0.57  0.36  1.00            
gender -0.23  0.25  0.20  0.20  0.14  1.00      
age    -0.17  0.12  0.07  0.16  0.14  0.06  1.00
> #compare to raw Pearson
> #note that the biserials and polychorics are not attenuated
> rp <- cor(bfi[c(1:5,26,28)],use="pairwise")
> lowerMat(rp)
       A1    A2    A3    A4    A5    gendr age  
A1      1.00                                    
A2     -0.34  1.00                              
A3     -0.27  0.49  1.00                        
A4     -0.15  0.34  0.36  1.00                  
A5     -0.18  0.39  0.50  0.31  1.00            
gender -0.16  0.18  0.14  0.13  0.10  1.00      
age    -0.16  0.11  0.07  0.14  0.13  0.05  1.00
> 
> 
> 
> cleanEx()
> nameEx("msq")
> ### * msq
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: msq
> ### Title: 75 mood items from the Motivational State Questionnaire for 3896
> ###   participants
> ### Aliases: msq
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(msq)
> if(FALSE){ #not run in the interests of time
+ #basic descriptive statistics
+ describe(msq)
+ }
> #score them for 20 short scales -- note that these have item overlap
> #The first 2 are from Thayer
> #The next 2 are classic positive and negative affect
> #The next 9 are circumplex scales
> #the last 7 are msq estimates of PANASX scales (missing some items)
> keys <- make.keys(msq[1:75],list(
+ EA = c("active", "energetic", "vigorous", "wakeful", "wide.awake", "full.of.pep",
+        "lively", "-sleepy", "-tired", "-drowsy"),
+ TA =c("intense", "jittery", "fearful", "tense", "clutched.up", "-quiet", "-still", 
+        "-placid", "-calm", "-at.rest") ,
+ PA =c("active", "excited", "strong", "inspired", "determined", "attentive", 
+           "interested", "enthusiastic", "proud", "alert"),
+ NAf =c("jittery", "nervous", "scared", "afraid", "guilty", "ashamed", "distressed",  
+          "upset", "hostile", "irritable" ),
+ HAct = c("active", "aroused", "surprised", "intense", "astonished"),
+ aPA = c("elated", "excited", "enthusiastic", "lively"),
+ uNA = c("calm", "serene", "relaxed", "at.rest", "content", "at.ease"),
+ pa = c("happy", "warmhearted", "pleased", "cheerful", "delighted" ),
+ LAct = c("quiet", "inactive", "idle", "still", "tranquil"),
+ uPA =c( "dull", "bored", "sluggish", "tired", "drowsy"),
+ naf = c( "sad", "blue", "unhappy", "gloomy", "grouchy"),
+ aNA = c("jittery", "anxious", "nervous", "fearful", "distressed"),
+ Fear = c("afraid" , "scared" , "nervous" , "jittery" ) ,
+ Hostility = c("angry" ,  "hostile", "irritable", "scornful" ), 
+ Guilt = c("guilty" , "ashamed" ),
+ Sadness = c( "sad"  , "blue" , "lonely",  "alone" ),
+ Joviality =c("happy","delighted", "cheerful", "excited", "enthusiastic", "lively", "energetic"), 
+ Self.Assurance=c( "proud","strong" , "confident" , "-fearful" ),
+ Attentiveness = c("alert" , "determined" , "attentive" )
+ #acquiscence = c("sleepy" ,  "wakeful" ,  "relaxed","tense")
+    ))
>        
> msq.scores <- scoreItems(keys,msq[1:75])
> 
> #show a circumplex structure for the non-overlapping items
> fcirc <- fa(msq.scores$scores[,5:12],2)  
> fa.plot(fcirc,labels=colnames(msq.scores$scores)[5:12])
> 
> #now, find the correlations corrected for item overlap
> msq.overlap <- scoreOverlap(keys,msq[1:75])
Warning in smc(r) :
  Missing values (NAs) in the correlation matrix do not allow for SMC's to be found for all variables.  
I will try to estimate SMCs for those variables by their non-NA  correlations.

SMC(s) for variables  alone kindly scornful were replaced (if possible) with smcs based upon their  (its) non-NA correlations
> f2 <- fa(msq.overlap$cor,2)
> fa.plot(f2,labels=colnames(msq.overlap$cor),title="2 dimensions of affect, corrected for overlap")
> if(FALSE) {
+ #extend this solution to EA/TA  NA/PA space
+ fe  <- fa.extension(cor(msq.scores$scores[,5:12],msq.scores$scores[,1:4]),fcirc)
+ fa.diagram(fcirc,fe=fe,main="Extending the circumplex structure to  EA/TA and PA/NA ")
+ 
+ #show the 2 dimensional structure
+ f2 <- fa(msq[1:72],2)
+ fa.plot(f2,labels=colnames(msq)[1:72],title="2 dimensions of affect at the item level")
+ 
+ #sort them by polar coordinates
+ round(polar(f2),2)
+ }
>             
> 
> 
> 
> 
> cleanEx()
> nameEx("mssd")
> ### * mssd
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mssd
> ### Title: Find von Neuman's Mean Square of Successive Differences
> ### Aliases: mssd rmssd autoR
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> t <- seq(-pi, pi, .1)
> trial <- 1: length(t)
> gr <- trial %% 8 
> c <- cos(t)
> ts <- sample(t,length(t))
> cs <- cos(ts)
> x.df <- data.frame(trial,gr,t,c,ts,cs)
> rmssd(x.df)
     trial         gr          t          c         ts         cs 
1.00816340 2.55432773 0.10081634 0.07173326 2.50085231 1.09000552 
> rmssd(x.df,gr)
     trial gr         t         c       ts        cs
0 8.763561  0 0.8763561 0.6692496 4.181866 1.0792300
1 8.640988  0 0.8640988 0.6231704 2.804164 1.4319802
2 8.640988  0 0.8640988 0.6270824 2.891078 1.2655632
3 8.640988  0 0.8640988 0.6297245 3.418820 1.2444714
4 8.640988  0 0.8640988 0.6310078 3.937639 0.6482595
5 8.640988  0 0.8640988 0.6308895 1.954056 0.9734633
6 8.640988  0 0.8640988 0.6293734 3.885657 0.8297314
7 8.640988  0 0.8640988 0.6265103 1.616581 0.8996703
> autoR(x.df,gr)

Autocorrelations 
Call: autoR(x = x.df, group = gr)
  trial  gr    t    c    ts    cs
0  0.86 NaN 0.86 0.55 -0.29 -0.22
1  0.89 NaN 0.89 0.69 -0.24 -0.24
2  0.89 NaN 0.89 0.68 -0.11 -0.47
3  0.89 NaN 0.89 0.67  0.05 -0.25
4  0.89 NaN 0.89 0.67 -0.44  0.33
5  0.89 NaN 0.89 0.67  0.15  0.25
6  0.89 NaN 0.89 0.67 -0.46  0.45
7  0.89 NaN 0.89 0.68  0.47  0.23
> describe(x.df)
      vars  n  mean    sd median trimmed   mad   min   max range  skew kurtosis
trial    1 63 32.00 18.33  32.00   32.00 23.72  1.00 63.00  62.0  0.00    -1.26
gr       2 63  3.56  2.28   4.00    3.57  2.97  0.00  7.00   7.0 -0.01    -1.28
t        3 63 -0.04  1.83  -0.04   -0.04  2.37 -3.14  3.06   6.2  0.00    -1.26
c        4 63  0.00  0.71   0.01    0.00  1.06 -1.00  1.00   2.0  0.00    -1.55
ts       5 63 -0.04  1.83  -0.04   -0.04  2.37 -3.14  3.06   6.2  0.00    -1.26
cs       6 63  0.00  0.71   0.01    0.00  1.06 -1.00  1.00   2.0  0.00    -1.55
        se
trial 2.31
gr    0.29
t     0.23
c     0.09
ts    0.23
cs    0.09
> #pairs.panels(x.df)
> #mlPlot(x.df,grp="gr",Time="t",items=c(4:6))
> 
> 
> 
> cleanEx()
> nameEx("multi.hist")
> ### * multi.hist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multi.hist
> ### Title: Multiple histograms with density and normal fits on one page
> ### Aliases: multi.hist histo.density histBy
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> multi.hist(sat.act) 
> multi.hist(sat.act,bcol="red")
> multi.hist(sat.act,dcol="blue")  #make both lines blue
> multi.hist(sat.act,dcol= c("blue","red"),dlty=c("dotted", "solid")) 
> multi.hist(sat.act,freq=TRUE)   #show the frequency plot
> multi.hist(sat.act,nrow=2)
> histBy(sat.act,"SATQ","gender")
> 
> 
> 
> cleanEx()
> nameEx("multilevel.reliability")
> ### * multilevel.reliability
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: multilevel.reliability
> ### Title: Find and plot various reliability/gneralizability coefficients
> ###   for multilevel data
> ### Aliases: mlr multilevel.reliability mlArrange mlPlot
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #data from Shrout and Lane, 2012.
> 
> shrout <- structure(list(Person = c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 
+ 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L), Time = c(1L, 1L, 
+ 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
+ 4L, 4L), Item1 = c(2L, 3L, 6L, 3L, 7L, 3L, 5L, 6L, 3L, 8L, 4L, 
+ 4L, 7L, 5L, 6L, 1L, 5L, 8L, 8L, 6L), Item2 = c(3L, 4L, 6L, 4L, 
+ 8L, 3L, 7L, 7L, 5L, 8L, 2L, 6L, 8L, 6L, 7L, 3L, 9L, 9L, 7L, 8L
+ ), Item3 = c(6L, 4L, 5L, 3L, 7L, 4L, 7L, 8L, 9L, 9L, 5L, 7L, 
+ 9L, 7L, 8L, 4L, 7L, 9L, 9L, 6L)), .Names = c("Person", "Time", 
+ "Item1", "Item2", "Item3"), class = "data.frame", row.names = c(NA, 
+ -20L))
> 
> #make shrout super wide
> #Xwide <- reshape(shrout,v.names=c("Item1","Item2","Item3"),timevar="Time", 
> #direction="wide",idvar="Person")
> #add more helpful Names
> #colnames(Xwide ) <- c("Person",c(paste0("Item",1:3,".T",1),paste0("Item",1:3,".T",2), 
> #paste0("Item",1:3,".T",3),paste0("Item",1:3,".T",4)))
> #make superwide into normal form  (i.e., just return it to the original shrout data
> #Xlong <-Xlong <- reshape(Xwide,idvar="Person",2:13)
> 
> #Now use these data for a multilevel repliability study, use the normal wide form output
> mg <- mlr(shrout,grp="Person",Time="Time",items=3:5) 
> #which is the same as 
> #mg <- multilevel.reliability(shrout,grp="Person",Time="Time",items=
> #         c("Item1","Item2","Item3"),plot=TRUE)
> #to show the lattice plot by subjects, set plot = TRUE
> 
> #Alternatively for long input (returned in this case from the prior run)
> mlr(mg$long,grp="id",Time ="time",items="items", values="values",long=TRUE)

Multilevel Generalizability analysis   
Call: mlr(x = mg$long, grp = "id", Time = "time", items = "items", 
    long = TRUE, values = "values")

The data had  5  observations taken over  4  time intervals for  3 items.

 Alternative estimates of reliabilty based upon Generalizability theory

RkF  =  0.97 Reliability of average of all ratings across all items and  times (Fixed time effects)
R1R  =  0.6 Generalizability of a single time point across all items (Random time effects)
RkR  =  0.85 Generalizability of average time points across all items (Random time effects)
Rc   =  0.74 Generalizability of change (fixed time points, fixed items) 
RkRn =  0.85 Generalizability of between person differences averaged over time (time nested within people)
Rcn  =  0.65 Generalizability of within person variations averaged over items  (time nested within people)

 These reliabilities are derived from the components of variance estimated by ANOVA 
             variance Percent
ID               2.34    0.44
Time             0.38    0.07
Items            0.61    0.11
ID x time        0.92    0.17
ID x items       0.12    0.02
time x items     0.05    0.01
Residual         0.96    0.18
Total            5.38    1.00

 The nested components of variance estimated from lme are:
         variance Percent
id            2.3    0.45
id(time)      1.1    0.21
residual      1.7    0.34
total         5.1    1.00

To see the ANOVA and alpha by subject, use the short = FALSE option.
 To see the summaries of the ICCs by subject and time, use all=TRUE
 To see specific objects select from the following list:
 ANOVA s.lmer s.lme alpha summary.by.person summary.by.time ICC.by.person ICC.by.time lmer long Call> 
> #example of mlArrange
> #First, add two new columns to shrout and 
> #then convert to long output using mlArrange
> total <- rowSums(shrout[3:5])
> caseid <- rep(paste0("ID",1:5),4)
> new.shrout <- cbind(shrout,total=total,case=caseid)
> #now convert to long
> new.long <- mlArrange(new.shrout,grp="Person",Time="Time",items =3:5,extra=6:7)
> headTail(new.long,6,6)
     id time values items total case
1     1    1      2 Item1    11  ID1
2     1    2      3 Item1    10  ID1
3     1    3      4 Item1    11  ID1
4     1    4      1 Item1     8  ID1
5     1    1      3 Item2    11  ID1
6     1    2      3 Item2    10  ID1
... ...  ...    ...  <NA>   ... <NA>
55    5    3      7 Item2    21  ID5
56    5    4      8 Item2    20  ID5
57    5    1      7 Item3    22  ID5
58    5    2      9 Item3    25  ID5
59    5    3      8 Item3    21  ID5
60    5    4      6 Item3    20  ID5
> 
> 
> 
> cleanEx()
> nameEx("neo")
> ### * neo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: neo
> ### Title: NEO correlation matrix from the NEO_PI_R manual
> ### Aliases: neo
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(neo)
> n5 <- fa(neo,5)
> neo.keys <- make.keys(30,list(N=c(1:6),E=c(7:12),O=c(13:18),A=c(19:24),C=c(25:30)))
> n5p <- target.rot(n5,neo.keys) #show a targeted rotation for simple structure
> n5p

Call: NULL
Standardized loadings (pattern matrix) based upon correlation matrix
     MR1   MR4   MR3   MR2   MR5   h2   u2
N1  0.79  0.00  0.03  0.08  0.07 0.62 0.38
N2  0.61  0.02  0.02 -0.40  0.07 0.54 0.46
N3  0.77 -0.12  0.09  0.05 -0.06 0.63 0.37
N4  0.67 -0.18 -0.01  0.09  0.01 0.50 0.50
N5  0.43  0.33 -0.01 -0.08 -0.27 0.37 0.63
N6  0.62 -0.15 -0.01  0.10 -0.22 0.49 0.51
E1 -0.03  0.57  0.08  0.46 -0.02 0.52 0.48
E2 -0.11  0.56 -0.03  0.16 -0.14 0.35 0.65
E3 -0.22  0.43  0.11 -0.26  0.19 0.39 0.61
E4  0.16  0.51  0.05 -0.16  0.34 0.43 0.57
E5  0.04  0.55  0.01 -0.24 -0.13 0.39 0.61
E6  0.04  0.67  0.06  0.21 -0.04 0.49 0.51
O1  0.10  0.10  0.51 -0.09 -0.29 0.38 0.62
O2  0.13 -0.10  0.69  0.15  0.14 0.52 0.48
O3  0.39  0.30  0.43  0.08  0.11 0.42 0.58
O4 -0.17  0.14  0.43  0.04 -0.09 0.26 0.74
O5 -0.15 -0.12  0.70 -0.12  0.12 0.55 0.45
O6 -0.13  0.06  0.34 -0.06 -0.15 0.17 0.83
A1 -0.30  0.13  0.11  0.50 -0.07 0.37 0.63
A2  0.02 -0.21 -0.05  0.58  0.24 0.46 0.54
A3  0.06  0.45 -0.12  0.61  0.16 0.57 0.43
A4 -0.15 -0.18  0.05  0.69 -0.02 0.55 0.45
A5  0.15 -0.17 -0.09  0.51 -0.04 0.34 0.66
A6  0.06  0.14  0.11  0.58 -0.04 0.37 0.63
C1 -0.26  0.13  0.07  0.00  0.55 0.42 0.58
C2  0.09  0.07 -0.18  0.00  0.64 0.46 0.54
C3 -0.07 -0.08  0.02  0.22  0.63 0.47 0.53
C4  0.09  0.19  0.10 -0.12  0.71 0.57 0.43
C5 -0.14  0.16 -0.13  0.02  0.69 0.56 0.44
C6 -0.14 -0.27 -0.01  0.12  0.53 0.40 0.60

                       MR1  MR4  MR3  MR2  MR5
SS loadings           3.22 2.68 1.88 2.79 2.97
Proportion Var        0.11 0.09 0.06 0.09 0.10
Cumulative Var        0.11 0.20 0.26 0.35 0.45
Proportion Explained  0.24 0.20 0.14 0.21 0.22
Cumulative Proportion 0.24 0.44 0.57 0.78 1.00
      MR1   MR4   MR3   MR2   MR5
MR1  1.00 -0.05 -0.04 -0.02 -0.08
MR4 -0.05  1.00  0.09 -0.07  0.03
MR3 -0.04  0.09  1.00  0.00 -0.03
MR2 -0.02 -0.07  0.00  1.00  0.01
MR5 -0.08  0.03 -0.03  0.01  1.00
> 
> 
> 
> 
> cleanEx()
> nameEx("omega")
> ### * omega
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: omega
> ### Title: Calculate McDonald's omega estimates of general and total factor
> ###   saturation
> ### Aliases: omega omegaSem omegaFromSem omegah
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> ## Not run: 
> ##D  test.data <- Harman74.cor$cov
> ##D # if(!require(GPArotation)) {message("Omega requires GPA rotation" )} else {
> ##D       my.omega <- omega(test.data)       
> ##D       print(my.omega,digits=2)
> ##D #}
> ##D  
> ##D #create 9 variables with a hierarchical structure
> ##D v9 <- sim.hierarchical()
> ##D #with correlations of
> ##D round(v9,2)
> ##D #find omega 
> ##D v9.omega <- omega(v9,digits=2)
> ##D v9.omega
> ##D 
> ##D #create 8 items with a two factor solution, showing the use of the flip option
> ##D sim2 <- item.sim(8)
> ##D omega(sim2)   #an example of misidentification-- remember to look at the loadings matrices.
> ##D omega(sim2,2)  #this shows that in fact there is no general factor
> ##D omega(sim2,2,option="first") #but, if we define one of the two group factors 
> ##D      #as a general factor, we get a falsely high omega 
> ##D #apply omega to analyze 6 mental ability tests 
> ##D data(ability.cov)   #has a covariance matrix
> ##D omega(ability.cov$cov)
> ##D 
> ##D #om <- omega(Thurstone)
> ##D #round(om$omega.group,2)
> ##D #round(om$omega.group[2]/om$omega.group[1],2)  #fraction of reliable that is general variance
> ##D # round(om$omega.group[3]/om$omega.group[1],2)  #fraction of reliable that is group variance
> ##D 
> ##D #To find factor score estimates for the hierarchical model it is necessary to 
> ##D #do two extra steps.
> ##D 
> ##D #Consider the case of the raw data in an object data.  (An example from simulation)
> ##D # set.seed(42)
> ##D # gload <- matrix(c(.9,.8,.7),nrow=3)
> ##D # fload <- matrix(c(.8,.7,.6,rep(0,9),.7,.6,.5,rep(0,9),.7,.6,.4),   ncol=3)
> ##D # data <- sim.hierarchical(gload=gload,fload=fload, n=100000, raw=TRUE)
> ##D # 
> ##D # f3 <- fa(data$observed,3,scores="tenBerge", oblique.scores=TRUE)
> ##D # f1 <- fa(f3$scores)
> ##D 
> ##D # om <- omega(data$observed,sl=FALSE) #draw the hierarchical figure
> ##D # The scores from om are based upon the Schmid-Leiman factors and although the g factor 
> ##D # is identical, the group factors are not.
> ##D # This is seen in the following correlation matrix
> ##D # hier.scores <- cbind(om$scores,f1$scores,f3$scores)
> ##D # lowerCor(hier.scores)
> ##D #
> ##D #jensen <- sim.hierarchical()   #create a hierarchical structure
> ##D #om.jen <- omegaSem(jensen,lavaan=TRUE)  #do the exploratory omega with confirmatory as well
> ##D #lav.mod <- om.jen$omegaSem$model$lavaan #get the lavaan code or create it yourself
> ##D # lav.mod <- 'g =~ +V1+V2+V3+V4+V5+V6+V7+V8+V9
> ##D #              F1=~  + V1 + V2 + V3             
> ##D #              F2=~  + V4 + V5 + V6 
> ##D #              F3=~  + V7 + V8 + V9 '  
> ##D #lav.jen <- cfa(lav.mod,sample.cov=jensen,sample.nobs=500,orthogonal=TRUE,std.lv=TRUE)
> ##D # omegaFromSem(lav.jen,jensen)
> ##D 
> ##D #try a one factor solution -- this is not recommended, but sometimes done
> ##D #it will just give omega_total
> ##D # lav.mod.1 <- 'g =~ +V1+V2+V3+V4+V5+V6+V7+V8+V9 '  
> ##D #lav.jen.1<- cfa(lav.mod.1,sample.cov=jensen,sample.nobs=500,orthogonal=TRUE,std.lv=TRUE)
> ##D # omegaFromSem(lav.jen.1,jensen)
> ##D 
> ##D 
> ##D 
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("omega.graph")
> ### * omega.graph
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: omega.graph
> ### Title: Graph hierarchical factor structures
> ### Aliases: omega.diagram omega.graph
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> #24 mental tests from Holzinger-Swineford-Harman
> if(require(GPArotation) ) {om24 <- omega(Harman74.cor$cov,4) } #run omega
Loading required package: GPArotation
> 
> #
> #example hierarchical structure from Jensen and Weng
> if(require(GPArotation) ) {jen.omega <- omega(make.hierarchical())}
> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:GPArotation’

> nameEx("outlier")
> ### * outlier
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: outlier
> ### Title: Find and graph Mahalanobis squared distances to detect outliers
> ### Aliases: outlier
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #first, just find and graph the outliers
> d2 <- outlier(sat.act)
> #combine with the data frame and plot it with the outliers highlighted in blue
> sat.d2 <- data.frame(sat.act,d2)
> pairs.panels(sat.d2,bg=c("yellow","blue")[(d2 > 25)+1],pch=21)
> 
> 
> 
> cleanEx()
> nameEx("p.rep")
> ### * p.rep
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: p.rep
> ### Title: Find the probability of replication for an F, t, or r and
> ###   estimate effect size
> ### Aliases: p.rep p.rep.f p.rep.t p.rep.r
> ### Keywords: models univar
> 
> ### ** Examples
> 
> 
> p.rep(.05)  #probability of replicating a result if the original study had a  p = .05
[1] 0.8776029
> p.rep.f(9.0,98)  #probability of replicating  a result with F = 9.0 with 98 df
$p.rep
[1] 0.9720729

$dprime
[1] 0.6060915

$prob
[1] 0.003423264

$r.equiv
[1] 0.2900209

> p.rep.r(.4,50)    #probability of replicating a result if r =.4 with n = 50
$p.rep
[1] 0.9818287

$dprime
[1] 0.8728716

$prob
[1] 0.003075876

> p.rep.t(3,98)   #probability of replicating a result if t = 3 with df =98
$p.rep
[1] 0.9720729

$dprime
[1] 0.6060915

$prob
[1] 0.003423264

$r.equiv
[1] 0.2900209

> p.rep.t(2.14,84,14) #effect of equal sample sizes (see Rosnow et al.)
$p.rep
[1] 0.9002023

$dprime
[1] 0.6054045

$prob
[1] 0.0348374

$r.equiv
[1] 0.2112921

> 
> 
> 
> 
> cleanEx()
> nameEx("paired.r")
> ### * paired.r
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: paired.r
> ### Title: Test the difference between (un)paired correlations
> ### Aliases: paired.r
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> paired.r(.5,.3, .4, 100) #dependent correlations
Call: paired.r(xy = 0.5, xz = 0.3, yz = 0.4, n = 100)
[1] "test of difference between two correlated  correlations"
t = 2.06  With probability =  0.04> paired.r(.5,.3,NULL,100) #independent correlations same sample size
Call: paired.r(xy = 0.5, xz = 0.3, yz = NULL, n = 100)
[1] "test of difference between two independent correlations"
z = 1.67  With probability =  0.09> paired.r(.5,.3,NULL, 100, 64) # independent correlations, different sample sizes
Call: paired.r(xy = 0.5, xz = 0.3, yz = NULL, n = 100, n2 = 64)
[1] "test of difference between two independent correlations"
z = 1.47  With probability =  0.14> 
> 
> 
> cleanEx()
> nameEx("pairs.panels")
> ### * pairs.panels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: pairs.panels
> ### Title: SPLOM, histograms and correlations for a data matrix
> ### Aliases: pairs.panels panel.cor panel.cor.scale panel.hist panel.lm
> ###   panel.lm.ellipse panel.hist.density panel.ellipse panel.smoother
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> 
> pairs.panels(attitude)   #see the graphics window
> data(iris)
> pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
+         pch=21,main="Fisher Iris data by Species") #to show color grouping
> 
> pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
+   pch=21+as.numeric(iris$Species),main="Fisher Iris data by Species",hist.col="red") 
>    #to show changing the diagonal
>    
> #to show 'significance'
>    pairs.panels(iris[1:4],bg=c("red","yellow","blue")[iris$Species],
+   pch=21+as.numeric(iris$Species),main="Fisher Iris data by Species",hist.col="red",stars=TRUE) 
>   
> 
> 
> #demonstrate not showing the data points
> data(sat.act)
> pairs.panels(sat.act,show.points=FALSE)
> #better yet is to show the points as a period
> pairs.panels(sat.act,pch=".")
> #show many variables with 0 gap between scatterplots
> # data(bfi)
> # pairs.panels(bfi,show.points=FALSE,gap=0)
> 
> #plot raw data points and then the weighted correlations.
> #output from statsBy
> sb <- statsBy(sat.act,"education")
> pairs.panels(sb$mean,wt=sb$n)  #report the weighted correlations
> #compare with 
> pairs.panels(sb$mean) #unweighed correlations
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("parcels")
> ### * parcels
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: parcels
> ### Title: Find miniscales (parcels) of size 2 or 3 from a set of items
> ### Aliases: parcels keysort
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> parcels(Thurstone)
                  P1 P2 P3
Sentences          1  0  0
Vocabulary         1  0  0
Sent.Completion    1  0  0
First.Letters      0  1  0
Four.Letter.Words  0  1  0
Suffixes           0  1  0
Letter.Series      0  0  1
Pedigrees          0  0  1
Letter.Group       0  0  1
> keys <- parcels(bfi)
> keys <- keysort(keys)
> score.items(keys,bfi)
score.items has been replaced by scoreItems, please change your call
Call: scoreItems(keys = keys, items = items, totals = totals, ilabels = ilabels, 
    missing = missing, impute = impute, delete = delete, min = min, 
    max = max, digits = digits, select = select)

(Unstandardized) Alpha:
        P3   P4  P6   P2   P1  P9   P8   P5  P7
alpha 0.72 0.65 0.5 0.72 0.82 0.3 0.12 0.63 0.5

Standard errors of unstandardized Alpha:
         P3    P4    P6    P2    P1    P9    P8    P5    P7
ASE   0.019 0.021 0.024 0.019 0.016 0.028 0.016 0.021 0.024

Average item correlation:
            P3   P4   P6   P2  P1   P9    P8   P5   P7
average.r 0.46 0.38 0.25 0.46 0.6 0.12 0.044 0.36 0.25

 Guttman 6* reliability: 
           P3   P4   P6   P2   P1   P9   P8   P5   P7
Lambda.6 0.69 0.64 0.53 0.69 0.79 0.39 0.22 0.62 0.55

Signal/Noise based upon av.r : 
              P3  P4 P6  P2  P1   P9   P8  P5   P7
Signal/Noise 2.5 1.8  1 2.6 4.5 0.43 0.14 1.7 0.98

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
       P3     P4     P6     P2     P1      P9     P8     P5     P7
P3  0.717 -0.328  0.519  0.630 -0.192  0.4029  0.551  0.589 -0.146
P4 -0.223  0.647 -0.971 -0.294  0.319  0.0302 -0.412 -0.315  0.436
P6  0.312 -0.554  0.502  0.466 -0.091 -0.0402  0.500  0.620 -0.256
P2  0.453 -0.200  0.280  0.720 -0.211  0.1775  0.201  0.598 -0.535
P1 -0.147  0.232 -0.059 -0.162  0.818  0.1087 -0.426 -0.090  0.853
P9  0.187  0.013 -0.016  0.082  0.054  0.2999 -0.025 -0.418  0.059
P8  0.162 -0.115  0.123  0.059 -0.134 -0.0047  0.121  0.152 -0.252
P5  0.397 -0.202  0.350  0.404 -0.065 -0.1823  0.042  0.633 -0.083
P7 -0.087  0.247 -0.128 -0.320  0.543  0.0226 -0.062 -0.047  0.496

 In order to see the item by scale loadings and frequency counts of the data
 print with the short option = FALSE> 
> 
> 
> cleanEx()
> nameEx("partial.r")
> ### * partial.r
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: partial.r
> ### Title: Find the partial correlations for a set (x) of variables with
> ###   set (y) removed.
> ### Aliases: partial.r
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> jen <- make.hierarchical()    #make up a correlation matrix 
> round(jen[1:5,1:5],2)
     V1   V2   V3   V4   V5
V1 1.00 0.56 0.48 0.40 0.35
V2 0.56 1.00 0.42 0.35 0.30
V3 0.48 0.42 1.00 0.30 0.26
V4 0.40 0.35 0.30 1.00 0.42
V5 0.35 0.30 0.26 0.42 1.00
> par.r <- partial.r(jen,c(1,3,5),c(2,4))
> cp <- corr.p(par.r,n=98)  #assumes the jen data based upon n =100.
> print(cp,short=FALSE)  #show the confidence intervals as well
Call:corr.p(r = par.r, n = 98)
Correlation matrix 
partial correlations 
     V1   V3   V5
V1 1.00 0.29 0.14
V3 0.29 1.00 0.10
V5 0.14 0.10 1.00
Sample Size 
[1] 98
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
partial correlations 
     V1   V3   V5
V1 0.00 0.01 0.31
V3 0.00 0.00 0.34
V5 0.16 0.34 0.00

 To see confidence intervals of the correlations, print with the short=FALSE option

 Confidence intervals based upon normal theory.  To get bootstrapped values, try cor.ci
      lower    r upper    p
V1-V3  0.10 0.29  0.46 0.00
V1-V5 -0.06 0.14  0.33 0.16
V3-V5 -0.10 0.10  0.29 0.34
> 
> 
> 
> cleanEx()
> nameEx("peas")
> ### * peas
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: peas
> ### Title: Galton's Peas
> ### Aliases: peas
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(peas)
> pairs.panels(peas,lm=TRUE,xlim=c(14,22),ylim=c(14,22),main="Galton's Peas")
Warning in rug(x) : some values will be clipped
> describe(peas)
       vars   n  mean   sd median trimmed  mad   min   max range skew kurtosis
parent    1 700 18.00 2.00  18.00   18.00 2.97 15.00 21.00   6.0 0.00    -1.25
child     2 700 16.29 1.98  16.07   16.14 2.55 13.77 22.67   8.9 0.49    -0.64
         se
parent 0.08
child  0.07
> pairs.panels(peas,main="Galton's Peas")
> 
> 
> 
> cleanEx()
> nameEx("phi")
> ### * phi
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: phi
> ### Title: Find the phi coefficient of correlation between two dichotomous
> ###   variables
> ### Aliases: phi
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> phi(c(30,20,20,30))
[1] 0.2
> phi(c(40,10,10,40))
[1] 0.6
> x <- matrix(c(40,5,20,20),ncol=2)
> phi(x)
[1] 0.43
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("phi.demo")
> ### * phi.demo
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: phi.demo
> ### Title: A simple demonstration of the Pearson, phi, and polychoric
> ###   corelation
> ### Aliases: phi.demo
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #demo <- phi.demo() #compare the phi (lower off diagonal and polychoric correlations
> # (upper off diagonal)
> #show the result from tetrachoric  which corrects for zero entries by default
> #round(demo$tetrachoric$rho,2)
> #show the result from phi2poly
> #tetrachorics above the diagonal, phi below the diagonal 
> #round(demo$phis,2) 
> 
> 
> 
> cleanEx()
> nameEx("phi2poly")
> ### * phi2poly
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: phi2tetra
> ### Title: Convert a phi coefficient to a tetrachoric correlation
> ### Aliases: phi2tetra phi2poly
> ### Keywords: models models
> 
> ### ** Examples
> 
>  phi2tetra(.3,c(.5,.5))
[1] 0.4540107
> #phi2poly(.3,.3,.7)
> 
> 
> 
> cleanEx()
> nameEx("plot.psych")
> ### * plot.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: plot.psych
> ### Title: Plotting functions for the psych package of class "psych"
> ### Aliases: plot.psych plot.poly plot.irt plot.residuals
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> test.data <- Harman74.cor$cov
> f4 <- fa(test.data,4)
> plot(f4)
> plot(resid(f4))
> plot(resid(f4),main="Residuals from a 4 factor solution",qq=FALSE)
Warning in text.default(xy$x[worst[1:bad]], xy$y[worst[1:bad]], paste(rname[worstItems[,  :
  "qq" is not a graphical parameter
> #not run
> #data(bfi)
> #e.irt <- irt.fa(bfi[11:15])  #just the extraversion items
> #plot(e.irt)   #the information curves
> #
> ic <- iclust(test.data,3)   #shows hierarchical structure 
> plot(ic)                    #plots loadings
Use ICLUST.diagram to see the  hierarchical structure
> #
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("polar")
> ### * polar
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: polar
> ### Title: Convert Cartesian factor loadings into polar coordinates
> ### Aliases: polar
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> circ.data <- circ.sim(24,500)
> circ.fa <- fa(circ.data,2)
> circ.polar <- round(polar(circ.fa),2)
> circ.polar
    Var theta21 vecl21
v2    2    2.63   0.35
v3    3   17.62   0.46
v4    4   32.42   0.39
v5    5   44.85   0.35
v6    6   67.00   0.41
v7    7   83.75   0.38
v8    8   94.65   0.40
v9    9  110.86   0.36
v10  10  120.06   0.33
v11  11  135.48   0.45
v12  12  155.32   0.43
v13  13  162.20   0.36
v14  14  186.11   0.39
v15  15  194.06   0.35
v16  16  210.68   0.38
v17  17  229.94   0.40
v18  18  243.88   0.39
v19  19  260.13   0.38
v20  20  274.87   0.34
v21  21  291.23   0.37
v22  22  302.14   0.35
v23  23  323.05   0.38
v24  24  338.85   0.36
v1    1  345.26   0.38
> #compare to the graphic
> cluster.plot(circ.fa)
> 
> 
> 
> cleanEx()
> nameEx("polychor.matrix")
> ### * polychor.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: polychor.matrix
> ### Title: Phi or Yule coefficient matrix to polychoric coefficient matrix
> ### Aliases: polychor.matrix Yule2poly.matrix phi2poly.matrix
> ###   Yule2phi.matrix
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> #demo <- phi.demo() 
> #compare the phi (lower off diagonal and polychoric correlations (upper off diagonal)
> #show the result from poly.mat
> #round(demo$tetrachoric$rho,2)
> #show the result from phi2poly
> #tetrachorics above the diagonal, phi below the diagonal 
> #round(demo$phis,2) 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.psych")
> ### * predict.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.psych
> ### Title: Prediction function for factor analysis or principal components
> ### Aliases: predict.psych
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> set.seed(42)
> x <- sim.item(12,500)
> f2 <- fa(x[1:250,],2,scores="regression")  # a two factor solution
> p2 <- principal(x[1:250,],2,scores=TRUE)  # a two component solution
> round(cor(f2$scores,p2$scores),2)  #correlate the components and factors from the A set
    RC1  RC2
MR1   1 0.01
MR2   0 1.00
> #find the predicted scores (The B set)
> pf2 <- predict(f2,x[251:500,],x[1:250,]) 
>   #use the original data for standardization values 
> pp2 <- predict(p2,x[251:500,],x[1:250,]) 
>  #standardized based upon the first set 
> round(cor(pf2,pp2),2)   #find the correlations in the B set
      RC1   RC2
MR1  1.00 -0.06
MR2 -0.07  1.00
> #test how well these predicted scores match the factor scores from the second set
> fp2 <- fa(x[251:500,],2,scores=TRUE)
> round(cor(fp2$scores,pf2),2)
      MR1   MR2
MR1  0.07 -0.99
MR2 -0.99  0.08
> 
> pf2.n <- predict(f2,x[251:500,])  #Standardized based upon the new data set
> round(cor(fp2$scores,pf2.n))   
    MR1 MR2
MR1   0  -1
MR2  -1   0
>    #predict factors of set two from factors of set 1, factor order is arbitrary
> 
> 
> #note that the signs of the factors in the second set are arbitrary
> 
> 
> 
> cleanEx()
> nameEx("principal")
> ### * principal
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: principal
> ### Title: Principal components analysis (PCA)
> ### Aliases: principal pca
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #Four principal components of the Harman 24 variable problem
> #compare to a four factor principal axes solution using factor.congruence
> pc <- principal(Harman74.cor$cov,4,rotate="varimax")
> mr <- fa(Harman74.cor$cov,4,rotate="varimax")  #minres factor analysis
> pa <- fa(Harman74.cor$cov,4,rotate="varimax",fm="pa")  # principal axis factor analysis
> round(factor.congruence(list(pc,mr,pa)),2)
     RC1  RC3  RC2  RC4  MR1  MR3  MR2  MR4  PA1  PA3  PA2  PA4
RC1 1.00 0.53 0.43 0.46 1.00 0.61 0.46 0.54 1.00 0.61 0.46 0.54
RC3 0.53 1.00 0.43 0.47 0.54 0.99 0.44 0.54 0.54 0.99 0.44 0.54
RC2 0.43 0.43 1.00 0.47 0.44 0.50 1.00 0.55 0.44 0.50 1.00 0.55
RC4 0.46 0.47 0.47 1.00 0.47 0.53 0.49 0.99 0.47 0.53 0.49 0.99
MR1 1.00 0.54 0.44 0.47 1.00 0.61 0.46 0.55 1.00 0.61 0.46 0.55
MR3 0.61 0.99 0.50 0.53 0.61 1.00 0.50 0.61 0.61 1.00 0.50 0.61
MR2 0.46 0.44 1.00 0.49 0.46 0.50 1.00 0.57 0.46 0.50 1.00 0.57
MR4 0.54 0.54 0.55 0.99 0.55 0.61 0.57 1.00 0.55 0.61 0.57 1.00
PA1 1.00 0.54 0.44 0.47 1.00 0.61 0.46 0.55 1.00 0.61 0.46 0.55
PA3 0.61 0.99 0.50 0.53 0.61 1.00 0.50 0.61 0.61 1.00 0.50 0.61
PA2 0.46 0.44 1.00 0.49 0.46 0.50 1.00 0.57 0.46 0.50 1.00 0.57
PA4 0.54 0.54 0.55 0.99 0.55 0.61 0.57 1.00 0.55 0.61 0.57 1.00
> 
> pc2 <- principal(Harman.5,2,rotate="varimax")
> pc2
Principal Components Analysis
Call: principal(r = Harman.5, nfactors = 2, rotate = "varimax")
Standardized loadings (pattern matrix) based upon correlation matrix
              RC1   RC2   h2    u2 com
population   0.02  0.99 0.99 0.012 1.0
schooling    0.94 -0.01 0.89 0.115 1.0
employment   0.14  0.98 0.98 0.021 1.0
professional 0.83  0.45 0.88 0.120 1.5
housevalue   0.97 -0.01 0.94 0.062 1.0

                       RC1  RC2
SS loadings           2.52 2.15
Proportion Var        0.50 0.43
Cumulative Var        0.50 0.93
Proportion Explained  0.54 0.46
Cumulative Proportion 0.54 1.00

Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.03 
 with the empirical chi square  0.29  with prob <  0.59 

Fit based upon off diagonal values = 1> round(cor(Harman.5,pc2$scores),2)  #compare these correlations to the loadings 
              RC1   RC2
population   0.02  0.99
schooling    0.94 -0.01
employment   0.14  0.98
professional 0.83  0.45
housevalue   0.97 -0.01
> #now do it for unstandardized scores, and transform obliquely
> pc2o <- principal(Harman.5,2,rotate="promax",covar=TRUE)
> pc2o
Principal Components Analysis
Call: principal(r = Harman.5, nfactors = 2, rotate = "promax", covar = TRUE)
Unstandardized loadings (pattern matrix) based upon covariance matrix
                RC1     RC2      h2      u2   H2      U2
population    -40.1 3440.30 1.2e+07 6.7e+03 1.00 5.7e-04
schooling       1.5   -0.01 2.4e+00 8.1e-01 0.75 2.5e-01
employment    110.3 1210.10 1.5e+06 5.4e+04 0.96 3.5e-02
professional   87.7   48.30 1.0e+04 2.9e+03 0.78 2.2e-01
housevalue   6368.4  -23.16 4.1e+07 2.2e+01 1.00 5.4e-07

                              RC1         RC2
SS loadings           40571924.73 13297286.79
Proportion Var               0.75        0.25
Cumulative Var               0.75        1.00
Proportion Explained         0.75        0.25
Cumulative Proportion        0.75        1.00

 Standardized loadings (pattern matrix)
             item   RC1   RC2   h2      u2
population      1 -0.01  1.00 1.00 5.7e-04
schooling       2  0.86 -0.01 0.75 2.5e-01
employment      3  0.09  0.97 0.96 3.5e-02
professional    4  0.76  0.42 0.78 2.2e-01
housevalue      5  1.00  0.00 1.00 5.4e-07

                 RC1  RC2
SS loadings     3.76 1.23
Proportion Var  0.75 0.25
Cumulative Var  0.75 1.00
Cum. factor Var 0.75 1.00

 With component correlations of 
     RC1  RC2
RC1 1.00 0.04
RC2 0.04 1.00

Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  6040.31 
 with the empirical chi square  8756488842  with prob <  0 

Fit based upon off diagonal values = 1> round(cov(Harman.5,pc2o$scores),2) 
                 RC1     RC2
population     89.53 3438.79
schooling       1.54    0.05
employment    155.90 1214.25
professional   89.56   51.60
housevalue   6367.49  216.72
> pc2o$Structure    #this matches the covariances with the scores
                     RC1          RC2
population     89.532324 3.438787e+03
schooling       1.542246 4.714838e-02
employment    155.900367 1.214255e+03
professional   89.559504 5.160112e+01
housevalue   6367.487506 2.167238e+02
> biplot(pc2,main="Biplot of the Harman.5 socio-economic variables",labels=paste0(1:12))
> 
> #For comparison with SPSS  (contributed by Gottfried Helms)
> pc2v <- principal(iris[1:4],2,rotate="varimax",normalize=FALSE,eps=1e-14)
> print(pc2v,digits=7)
Principal Components Analysis
Call: principal(r = iris[1:4], nfactors = 2, rotate = "varimax", normalize = FALSE, 
    eps = 1e-14)
Standardized loadings (pattern matrix) based upon correlation matrix
                    RC1        RC2        h2          u2      com
Sepal.Length  0.9593182  0.0480331 0.9225986 0.077401362 1.005014
Sepal.Width  -0.1442732  0.9849389 0.9909193 0.009080678 1.042893
Petal.Length  0.9441083 -0.3039564 0.9837300 0.016270047 1.205101
Petal.Width   0.9323563 -0.2568894 0.9352804 0.064719625 1.150960

                            RC1       RC2
SS loadings           2.7017349 1.1307934
Proportion Var        0.6754337 0.2826983
Cumulative Var        0.6754337 0.9581321
Proportion Explained  0.7049485 0.2950515
Cumulative Proportion 0.7049485 1.0000000

Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.030876 
 with the empirical chi square  1.715987  with prob <  NA 

Fit based upon off diagonal values = 0.9978717> pc2V <- principal(iris[1:4],2,rotate="Varimax",eps=1e-7)
> p <- print(pc2V,digits=7)
Principal Components Analysis
Call: principal(r = iris[1:4], nfactors = 2, rotate = "Varimax", eps = 1e-07)
Standardized loadings (pattern matrix) based upon correlation matrix
                    RC1        RC2        h2          u2      com
Sepal.Length  0.9593182  0.0480331 0.9225986 0.077401362 1.005014
Sepal.Width  -0.1442732  0.9849389 0.9909193 0.009080678 1.042893
Petal.Length  0.9441083 -0.3039563 0.9837300 0.016270047 1.205101
Petal.Width   0.9323563 -0.2568893 0.9352804 0.064719625 1.150960

                            RC1       RC2
SS loadings           2.7017350 1.1307933
Proportion Var        0.6754338 0.2826983
Cumulative Var        0.6754338 0.9581321
Proportion Explained  0.7049485 0.2950515
Cumulative Proportion 0.7049485 1.0000000

Mean item complexity =  1.1
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.030876 
 with the empirical chi square  1.715987  with prob <  NA 

Fit based upon off diagonal values = 0.9978717> round(p$Vaccounted,2)   # the amount of variance accounted for is returned as an object of print
                       RC1  RC2
SS loadings           2.70 1.13
Proportion Var        0.68 0.28
Cumulative Var        0.68 0.96
Proportion Explained  0.70 0.30
Cumulative Proportion 0.70 1.00
> 
> 
> 
> cleanEx()
> nameEx("print.psych")
> ### * print.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: print.psych
> ### Title: Print and summary functions for the psych class
> ### Aliases: print.psych summary.psych anova.psych
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> data(bfi)
>  keys.list <- list(agree=c(-1,2:5),conscientious=c(6:8,-9,-10),
+  extraversion=c(-11,-12,13:15),neuroticism=c(16:20),openness = c(21,-22,23,24,-25))
>  keys <- make.keys(25,keys.list,item.labels=colnames(bfi[1:25]))
>  scores <- score.items(keys,bfi[1:25])
score.items has been replaced by scoreItems, please change your call
>  scores
Call: scoreItems(keys = keys, items = items, totals = totals, ilabels = ilabels, 
    missing = missing, impute = impute, delete = delete, min = min, 
    max = max, digits = digits, select = select)

(Unstandardized) Alpha:
      agree conscientious extraversion neuroticism openness
alpha   0.7          0.72         0.76        0.81      0.6

Standard errors of unstandardized Alpha:
      agree conscientious extraversion neuroticism openness
ASE   0.014         0.014        0.013       0.011    0.017

Average item correlation:
          agree conscientious extraversion neuroticism openness
average.r  0.32          0.34         0.39        0.46     0.23

 Guttman 6* reliability: 
         agree conscientious extraversion neuroticism openness
Lambda.6   0.7          0.72         0.76        0.81      0.6

Signal/Noise based upon av.r : 
             agree conscientious extraversion neuroticism openness
Signal/Noise   2.3           2.6          3.2         4.3      1.5

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.26          0.72         0.35      -0.305     0.30
extraversion   0.46          0.26         0.76      -0.284     0.32
neuroticism   -0.18         -0.23        -0.22       0.812    -0.12
openness       0.15          0.19         0.22      -0.086     0.60

 In order to see the item by scale loadings and frequency counts of the data
 print with the short option = FALSE>  summary(scores)
Call: scoreItems(keys = keys, items = items, totals = totals, ilabels = ilabels, 
    missing = missing, impute = impute, delete = delete, min = min, 
    max = max, digits = digits, select = select)

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, (unstandardized) alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.26          0.72         0.35      -0.305     0.30
extraversion   0.46          0.26         0.76      -0.284     0.32
neuroticism   -0.18         -0.23        -0.22       0.812    -0.12
openness       0.15          0.19         0.22      -0.086     0.60
> 
> 
> 
> cleanEx()
> nameEx("r.test")
> ### * r.test
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: r.test
> ### Title: Tests of significance for correlations
> ### Aliases: r.test
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> n <- 30 
> r <- seq(0,.9,.1) 
> rc <- matrix(r.con(r,n),ncol=2) 
> test <- r.test(n,r)
> r.rc <- data.frame(r=r,z=fisherz(r),lower=rc[,1],upper=rc[,2],t=test$t,p=test$p) 
> round(r.rc,2) 
     r    z lower upper     t    p
1  0.0 0.00 -0.36  0.36  0.00 1.00
2  0.1 0.10 -0.27  0.44  0.53 0.60
3  0.2 0.20 -0.17  0.52  1.08 0.29
4  0.3 0.31 -0.07  0.60  1.66 0.11
5  0.4 0.42  0.05  0.66  2.31 0.03
6  0.5 0.55  0.17  0.73  3.06 0.00
7  0.6 0.69  0.31  0.79  3.97 0.00
8  0.7 0.87  0.45  0.85  5.19 0.00
9  0.8 1.10  0.62  0.90  7.06 0.00
10 0.9 1.47  0.80  0.95 10.93 0.00
> 
> r.test(50,r)
Correlation tests 
Call:r.test(n = 50, r12 = r)
Test of significance of a  correlation 
 t value 0 0.7 1.41 2.18 3.02 4 5.2 6.79 9.24 14.3    with probability < 1 0.49 0.16 0.034 0.004 0.00022 4.1e-06 1.5e-08 3.2e-12 0
 and confidence interval  -0.28 -0.18 -0.08 0.02 0.14 0.26 0.39 0.52 0.67 0.83 0.28 0.37 0.45 0.53 0.61 0.68 0.75 0.82 0.88 0.94> r.test(30,.4,.6)       #test the difference between two independent correlations
Correlation tests 
Call:r.test(n = 30, r12 = 0.4, r34 = 0.6)
Test of difference between two independent correlations 
 z value 0.99    with probability  0.32> r.test(103,.4,.5,.1)   #Steiger case A of dependent correlations 
Correlation tests 
Call:[1] "r.test(n =  103 ,  r12 =  0.4 ,  r23 =  0.1 ,  r13 =  0.5 )"
Test of difference between two correlated  correlations 
 t value -0.89    with probability < 0.37> r.test(n=103, r12=.4, r13=.5,r23=.1)  
Correlation tests 
Call:[1] "r.test(n =  103 ,  r12 =  0.4 ,  r23 =  0.1 ,  r13 =  0.5 )"
Test of difference between two correlated  correlations 
 t value -0.89    with probability < 0.37> #for complicated tests, it is probably better to specify correlations by name
> r.test(n=103,r12=.5,r34=.6,r13=.7,r23=.5,r14=.5,r24=.8)   #steiger Case B 
Correlation tests 
Call:r.test(n = 103, r12 = 0.5, r34 = 0.6, r23 = 0.5, r13 = 0.7, r14 = 0.5, 
    r24 = 0.8)
Test of difference between two dependent correlations 
 z value -1.4    with probability  0.16> 
> 
> 
> 
> cleanEx()
> nameEx("range.correction")
> ### * range.correction
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rangeCorrection
> ### Title: Correct correlations for restriction of range. (Thorndike Case
> ###   2)
> ### Aliases: rangeCorrection
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> rangeCorrection(.33,100.32,48.19) #example from Revelle (in prep) Chapter 4.
[1] 0.5884233
> 
> 
> 
> cleanEx()
> nameEx("read.clipboard")
> ### * read.clipboard
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: read.file
> ### Title: Shortcuts for reading from the clipboard or a file
> ### Aliases: read.clipboard read.clipboard.csv read.clipboard.tab
> ###   read.clipboard.lower read.clipboard.upper read.clipboard.fwf
> ###   read.file read.file.csv write.file write.file.csv read.https
> ### Keywords: multivariate IO
> 
> ### ** Examples
> 
> #Because these are dynamic functions, they need to be run interactively and 
> # can not be shown as examples.
> 
> #my.data <- read.file()   #search the directory for a file and then read it.
> #the example data set from read.delim in the readr package to read a remote csv file
> #my.data  <-read.file("https://github.com/tidyverse/readr/raw/master/inst/extdata/mtcars.csv")
> #my.data <- read.clipboad()
> #my.data <- read.clipboard.csv()
> #my.data <- read.clipboad(header=FALSE)
> #my.matrix <- read.clipboard.lower()
> 
> 
> 
> cleanEx()
> nameEx("rescale")
> ### * rescale
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rescale
> ### Title: Function to convert scores to "conventional " metrics
> ### Aliases: rescale
> ### Keywords: multivariate models univar
> 
> ### ** Examples
> 
> T <- rescale(attitude,50,10) #all put on same scale
> describe(T)
           vars  n mean sd median trimmed   mad   min   max range  skew
rating        1 30   50 10  50.71   50.47  8.53 29.76 66.73 36.97 -0.36
complaints    2 30   50 10  48.80   50.36 11.14 27.77 67.57 39.81 -0.22
privileges    3 30   50 10  48.67   49.69  8.48 31.09 74.41 43.32  0.38
learning      4 30   50 10  50.11   50.18 12.63 30.94 65.88 34.93 -0.05
raises        5 30   50 10  48.91   49.87 10.69 29.19 72.47 43.28  0.20
critical      6 30   50 10  52.76   51.08  7.49 23.96 67.42 43.46 -0.87
advance       7 30   50 10  48.12   48.93  8.65 32.57 78.25 45.68  0.85
           kurtosis   se
rating        -0.77 1.83
complaints    -0.68 1.83
privileges    -0.41 1.83
learning      -1.22 1.83
raises        -0.60 1.83
critical       0.17 1.83
advance        0.47 1.83
> T1 <- rescale(attitude,seq(0,300,50),seq(10,70,10)) #different means and sigmas
> describe(T1)
           vars  n mean sd median trimmed   mad    min    max  range  skew
rating        1 30    0 10   0.71    0.47  8.53 -20.24  16.73  36.97 -0.36
complaints    2 30   50 20  47.60   50.73 22.27   5.54  85.15  79.61 -0.22
privileges    3 30  100 30  96.00   99.06 25.45  43.28 173.23 129.95  0.38
learning      4 30  150 40 150.45  150.74 50.53  73.77 213.50 139.73 -0.05
raises        5 30  200 50 194.55  199.36 53.47  95.97 312.37 216.40  0.20
critical      6 30  250 60 266.57  256.47 44.95  93.76 354.50 260.74 -0.87
advance       7 30  300 70 286.85  292.52 60.52 177.99 497.76 319.77  0.85
           kurtosis    se
rating        -0.77  1.83
complaints    -0.68  3.65
privileges    -0.41  5.48
learning      -1.22  7.30
raises        -0.60  9.13
critical       0.17 10.95
advance        0.47 12.78
> 
> 
> 
> cleanEx()
> nameEx("residuals.psych")
> ### * residuals.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: residuals.psych
> ### Title: Extract residuals from various psych objects
> ### Aliases: residuals.psych resid.psych
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> f3 <- fa(Thurstone,3)
> residuals(f3)
                  Sntnc Vcblr Snt.C Frs.L F.L.W Sffxs Ltt.S Pdgrs Ltt.G
Sentences          0.18                                                
Vocabulary         0.01  0.16                                          
Sent.Completion    0.00 -0.01  0.26                                    
First.Letters     -0.01  0.00  0.01  0.27                              
Four.Letter.Words  0.01  0.00  0.00  0.00  0.37                        
Suffixes           0.00  0.00  0.00  0.00  0.00  0.50                  
Letter.Series      0.00  0.01 -0.01  0.01 -0.01  0.00  0.27            
Pedigrees         -0.01  0.00  0.02  0.00  0.00  0.01  0.00  0.49      
Letter.Group       0.01 -0.01  0.00  0.00  0.01  0.00  0.00  0.00  0.48
> 
> 
> 
> cleanEx()
> nameEx("reverse.code")
> ### * reverse.code
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: reverse.code
> ### Title: Reverse the coding of selected items prior to scale analysis
> ### Aliases: reverse.code
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> original <- matrix(sample(6,50,replace=TRUE),10,5)
> keys <- c(1,1,-1,-1,1)  #reverse the 3rd and 4th items
> new <- reverse.code(keys,original,mini=rep(1,5),maxi=rep(6,5))
> original[1:3,]
     [,1] [,2] [,3] [,4] [,5]
[1,]    2    2    6    3    5
[2,]    3    2    2    4    4
[3,]    4    5    4    3    5
> new[1:3,]
     <NA> <NA> - - <NA>
[1,]    2    2 1 4    5
[2,]    3    2 5 3    4
[3,]    4    5 3 4    5
> 
> 
> 
> cleanEx()
> nameEx("sat.act")
> ### * sat.act
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sat.act
> ### Title: 3 Measures of ability: SATV, SATQ, ACT
> ### Aliases: sat.act
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(sat.act)
> describe(sat.act)
          vars   n   mean     sd median trimmed    mad min max range  skew
gender       1 700   1.65   0.48      2    1.68   0.00   1   2     1 -0.61
education    2 700   3.16   1.43      3    3.31   1.48   0   5     5 -0.68
age          3 700  25.59   9.50     22   23.86   5.93  13  65    52  1.64
ACT          4 700  28.55   4.82     29   28.84   4.45   3  36    33 -0.66
SATV         5 700 612.23 112.90    620  619.45 118.61 200 800   600 -0.64
SATQ         6 687 610.22 115.64    620  617.25 118.61 200 800   600 -0.59
          kurtosis   se
gender       -1.62 0.02
education    -0.07 0.05
age           2.42 0.36
ACT           0.53 0.18
SATV          0.33 4.27
SATQ         -0.02 4.41
> pairs.panels(sat.act)
> 
> 
> 
> cleanEx()
> nameEx("scatter.hist")
> ### * scatter.hist
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scatterHist
> ### Title: Draw a scatter plot with associated X and Y histograms,
> ###   densities and correlation
> ### Aliases: scatter.hist scatterHist
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> data(sat.act)
> with(sat.act,scatter.hist(SATV,SATQ))
> #or for something a bit more splashy
> scatter.hist(sat.act[5:6],pch=(19+sat.act$gender),col=c("blue","red")[sat.act$gender],grid=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("schmid")
> ### * schmid
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: schmid
> ### Title: Apply the Schmid Leiman transformation to a correlation matrix
> ### Aliases: schmid
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> jen <- sim.hierarchical()  #create a hierarchical demo
> if(!require(GPArotation)) {
+    message("I am sorry, you must have GPArotation installed to use schmid.")} else {
+    p.jen <- schmid(jen,digits=2)   #use the oblimin rotation
+ p.jen <- schmid(jen,rotate="promax") #use the promax rotation
+ }
Loading required package: GPArotation
> 
> 
> 
> cleanEx()

detaching ‘package:GPArotation’

> nameEx("score.alpha")
> ### * score.alpha
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: score.alpha
> ### Title: Score scales and find Cronbach's alpha as well as associated
> ###   statistics
> ### Aliases: score.alpha
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> y <- attitude     #from the datasets package
> keys <- matrix(c(rep(1,7),rep(1,4),rep(0,7),rep(-1,3)),ncol=3)
> labels <- c("first","second","third")
> x <- score.alpha(keys,y,labels) #deprecated
Warning: score.alpha is deprecated.  Please use the scoreItems function
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("score.irt")
> ### * score.irt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scoreIrt
> ### Title: Find Item Response Theory (IRT) based scores for dichotomous or
> ###   polytomous items
> ### Aliases: scoreIrt scoreIrt.1pl scoreIrt.2pl score.irt score.irt.poly
> ###   score.irt.2 irt.stats.like irt.tau irt.se
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> if(FALSE) {  #not run in the interest of time, but worth doing
+ d9 <- sim.irt(9,1000,-2.5,2.5,mod="normal") #dichotomous items
+ test <- irt.fa(d9$items)
+ scores <- scoreIrt(test,d9$items)
+ scores.df <- data.frame(scores,true=d9$theta) #combine the estimates with the true thetas.
+ pairs.panels(scores.df,pch=".",
+ main="Comparing IRT and classical with complete data") 
+ #now show how to do this with a quasi-Rasch model
+ tau <- irt.tau(d9$items)
+ scores.rasch <- scoreIrt(tau,d9$items,key=rep(1,9))
+ scores.dfr<- data.frame(scores.df,scores.rasch) #almost identical to 2PL model!
+ pairs.panels(scores.dfr)
+ #with all the data, why bother ?
+ 
+ #now delete some of the data
+ d9$items[1:333,1:3] <- NA
+ d9$items[334:666,4:6] <- NA
+ d9$items[667:1000,7:9] <- NA
+ scores <- scoreIrt(test,d9$items)
+ scores.df <- data.frame(scores,true=d9$theta) #combine the estimates with the true thetas.
+ pairs.panels(scores.df, pch=".",
+ main="Comparing IRT and classical with random missing data")
+  #with missing data, the theta estimates are noticably better.
+ #now show how to do this with a quasi-Rasch model
+ tau <- irt.tau(d9$items)
+ scores.rasch <- scoreIrt(tau,d9$items,key=rep(1,9))
+ scores.dfr <- data.frame(scores.df,rasch = scores.rasch)
+ pairs.panels(scores.dfr)  #rasch is actually better!
+ 
+ 
+ 
+ v9 <- sim.irt(9,1000,-2.,2.,mod="normal") #dichotomous items
+ items <- v9$items
+ test <- irt.fa(items)
+ total <- rowSums(items)
+ ord <- order(total)
+ items <- items[ord,]
+ 
+ 
+ #now delete some of the data - note that they are ordered by score
+ items[1:333,5:9] <- NA
+ items[334:666,3:7] <- NA
+ items[667:1000,1:4] <- NA
+ items[990:995,1:9] <- NA   #the case of terrible data
+ items[996:998,] <- 0   #all wrong
+ items[999:1000] <- 1   #all right
+ scores <- scoreIrt(test,items)
+ unitweighted <- scoreIrt(items=items,keys=rep(1,9)) #each item has a discrimination of 1
+ #combine the estimates with the true thetas.
+ scores.df <- data.frame(v9$theta[ord],scores,unitweighted) 
+    
+ colnames(scores.df) <- c("True theta","irt theta","total","fit","rasch","total","fit")
+ pairs.panels(scores.df,pch=".",main="Comparing IRT and classical with missing data") 
+  #with missing data, the theta estimates are noticably better estimates 
+  #of the generating theta than using the empirically derived factor loading weights
+ 
+ #now show the ability to score multiple scales using keys
+ ab.tau <- irt.tau(ability)  #first find the tau values
+ ab.keys <- make.keys(ability,list(g=1:16,reason=1:4,letter=5:8,matrix=9:12,rotate=13:16))
+ ab.scores <- scoreIrt(stats=ab.tau, items = ability, keys = ab.keys)
+ 
+ #and now do it for polytomous items 
+ bfi.tau <- irt.tau(bfi[1:25])
+ bfi.keys <- make.keys(bfi[1:25],list(agree=c(-1,2:5),conscientious=c(6:8,-9,-10),
+ extra=c(11:12,-13,-14,-15),neur = 16:20,open =c(21,-22,23,24,-25)))
+ bfi.scores <- scoreIrt(bfi.tau,bfi[1:25],bfi.keys)
+ }
> 
> 
> 
> cleanEx()
> nameEx("score.items")
> ### * score.items
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scoreItems
> ### Title: Score item composite scales and find Cronbach's alpha, Guttman
> ###   lambda 6 and item whole correlations
> ### Aliases: scoreItems scoreFast score.items response.frequencies
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> 
> #see  the example including the bfi data set
> data(bfi)
> keys.list <- list(agree=c("-A1","A2","A3","A4","A5"),
+   conscientious=c("C1","C2","C3","-C4","-C5"),extraversion=c("-E1","-E2","E3","E4","E5"),
+   neuroticism=c("N1","N2","N3","N4","N5"), openness = c("O1","-O2","O3","O4","-O5")) 
>   keys <- make.keys(bfi,keys.list)  #no longer necessary
>  scores <- scoreItems(keys,bfi,min=1,max=6)  #using a keys matrix 
>  scores <- scoreItems(keys.list,bfi,min=1,max=6)  # or just use the keys.list
>  summary(scores)
Call: scoreItems(keys = keys.list, items = bfi, min = 1, max = 6)

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, (unstandardized) alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.26          0.72         0.35      -0.305     0.30
extraversion   0.46          0.26         0.76      -0.284     0.32
neuroticism   -0.18         -0.23        -0.22       0.812    -0.12
openness       0.15          0.19         0.22      -0.086     0.60
>  #to get the response frequencies, we need to not use the age variable
>  scores <- scoreItems(keys[1:25,],bfi[1:25]) #we do not need to specify min or max if
>            #there are no values (such as age) outside the normal item range.
>  scores
Call: scoreItems(keys = keys[1:25, ], items = bfi[1:25])

(Unstandardized) Alpha:
      agree conscientious extraversion neuroticism openness
alpha   0.7          0.72         0.76        0.81      0.6

Standard errors of unstandardized Alpha:
      agree conscientious extraversion neuroticism openness
ASE   0.014         0.014        0.013       0.011    0.017

Average item correlation:
          agree conscientious extraversion neuroticism openness
average.r  0.32          0.34         0.39        0.46     0.23

 Guttman 6* reliability: 
         agree conscientious extraversion neuroticism openness
Lambda.6   0.7          0.72         0.76        0.81      0.6

Signal/Noise based upon av.r : 
             agree conscientious extraversion neuroticism openness
Signal/Noise   2.3           2.6          3.2         4.3      1.5

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:
              agree conscientious extraversion neuroticism openness
agree          0.70          0.36         0.63      -0.245     0.23
conscientious  0.26          0.72         0.35      -0.305     0.30
extraversion   0.46          0.26         0.76      -0.284     0.32
neuroticism   -0.18         -0.23        -0.22       0.812    -0.12
openness       0.15          0.19         0.22      -0.086     0.60

 In order to see the item by scale loadings and frequency counts of the data
 print with the short option = FALSE>  #The scores themselves are available in the scores$scores object.  I.e.,
>  describe(scores$scores)
              vars    n mean   sd median trimmed  mad min max range  skew
agree            1 2800 4.65 0.89    4.8    4.73 0.89 1.0   6   5.0 -0.77
conscientious    2 2800 4.27 0.95    4.4    4.31 0.89 1.0   6   5.0 -0.41
extraversion     3 2800 4.15 1.05    4.2    4.20 1.19 1.0   6   5.0 -0.48
neuroticism      4 2800 3.16 1.19    3.0    3.13 1.19 1.0   6   5.0  0.22
openness         5 2800 4.59 0.80    4.6    4.62 0.89 1.2   6   4.8 -0.34
              kurtosis   se
agree             0.44 0.02
conscientious    -0.16 0.02
extraversion     -0.19 0.02
neuroticism      -0.65 0.02
openness         -0.28 0.02
>  
>  
>  #compare this output to that for the impute="none" option for SAPA type data
>  #first make many of the items missing in a missing pattern way
>  missing.bfi <- bfi
>  missing.bfi[1:1000,3:8] <- NA
>  missing.bfi[1001:2000,c(1:2,9:10)] <- NA
>  scores <- scoreItems(keys.list,missing.bfi,impute="none",min=1,max=6)
>  scores
Call: scoreItems(keys = keys.list, items = missing.bfi, impute = "none", 
    min = 1, max = 6)

(Standardized) Alpha:
      agree conscientious extraversion neuroticism openness
alpha  0.72          0.72         0.76        0.81      0.6

Standard errors of unstandardized Alpha:
      agree conscientious extraversion neuroticism openness
ASE   0.014         0.014        0.013       0.011    0.017

Standardized Alpha of observed scales:
     agree conscientious extraversion neuroticism openness
[1,]  0.62          0.62         0.76        0.81      0.6

Average item correlation:
          agree conscientious extraversion neuroticism openness
average.r  0.34          0.34         0.39        0.47     0.23

 Guttman 6* reliability: 
         agree conscientious extraversion neuroticism openness
Lambda.6  0.73          0.72         0.77        0.81     0.61

Signal/Noise based upon av.r : 
             agree conscientious extraversion neuroticism openness
Signal/Noise   2.5           2.6          3.2         4.4      1.5

Scale intercorrelations corrected for attenuation 
 raw correlations below the diagonal, alpha on the diagonal 
 corrected correlations above the diagonal:

Note that these are the correlations of the complete scales based on the correlation matrix,
 not the observed scales based on the raw items.
              agree conscientious extraversion neuroticism openness
agree          0.72          0.35         0.64      -0.243     0.25
conscientious  0.25          0.72         0.33      -0.275     0.28
extraversion   0.47          0.24         0.76      -0.284     0.32
neuroticism   -0.19         -0.21        -0.22       0.814    -0.12
openness       0.17          0.19         0.22      -0.086     0.60

 In order to see the item by scale loadings and frequency counts of the data
 print with the short option = FALSE>  describe(scores$scores)  #the actual scores themselves
              vars    n mean   sd median trimmed  mad min max range  skew
agree            1 2800 4.67 1.00   5.00    4.77 0.99 1.0   6   5.0 -0.82
conscientious    2 2800 4.27 1.09   4.33    4.33 1.09 1.0   6   5.0 -0.47
extraversion     3 2800 4.15 1.06   4.20    4.20 1.19 1.0   6   5.0 -0.48
neuroticism      4 2800 3.16 1.20   3.00    3.13 1.48 1.0   6   5.0  0.21
openness         5 2800 4.59 0.81   4.60    4.62 0.89 1.2   6   4.8 -0.34
              kurtosis   se
agree             0.42 0.02
conscientious    -0.24 0.02
extraversion     -0.21 0.02
neuroticism      -0.67 0.02
openness         -0.29 0.02
>  
>  #If we want to delete scales scores for people who did not answer some items for one 
>  #(or more) scales, we can do the following:
>  
>   scores <- scoreItems(keys.list,missing.bfi,totals=TRUE,min=1,max=6) #find total scores
>   describe(scores$scores) #note that missing data were replaced with median for the item
              vars    n  mean   sd median trimmed  mad min max range  skew
agree            1 2800 23.89 3.34     25   24.24 2.97   8  30    22 -1.14
conscientious    2 2800 22.37 3.48     23   22.60 2.97   6  30    24 -0.78
extraversion     3 2800 20.73 5.27     21   21.02 5.93   5  30    25 -0.48
neuroticism      4 2800 15.81 5.93     15   15.64 5.93   5  30    25  0.22
openness         5 2800 22.95 4.02     23   23.10 4.45   6  30    24 -0.34
              kurtosis   se
agree             1.93 0.06
conscientious     1.22 0.07
extraversion     -0.19 0.10
neuroticism      -0.65 0.11
openness         -0.28 0.08
>   scores$scores[scores$missing > 0] <- NA  #get rid of cases with missing data
>   describe(scores$scores)
              vars    n  mean   sd median trimmed  mad min max range  skew
agree            1  776 23.17 4.56     24   23.55 4.45   8  30    22 -0.75
conscientious    2  769 21.52 4.72     22   21.76 4.45   6  30    24 -0.49
extraversion     3 2713 20.72 5.30     21   21.01 5.93   5  30    25 -0.47
neuroticism      4 2694 15.82 5.97     15   15.65 7.41   5  30    25  0.22
openness         5 2726 22.97 4.04     23   23.12 4.45   6  30    24 -0.35
              kurtosis   se
agree             0.31 0.16
conscientious     0.01 0.17
extraversion     -0.20 0.10
neuroticism      -0.66 0.12
openness         -0.28 0.08
> 
> 
> 
> 
> cleanEx()
> nameEx("score.multiple.choice")
> ### * score.multiple.choice
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: score.multiple.choice
> ### Title: Score multiple choice items and provide basic test statistics
> ### Aliases: score.multiple.choice
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> data(iqitems)
> iq.keys <- c(4,4,4, 6,6,3,4,4,  5,2,2,4,  3,2,6,7)
> score.multiple.choice(iq.keys,iqitems)
Call: score.multiple.choice(key = iq.keys, data = iqitems)

(Unstandardized) Alpha:
[1] 0.84

Average item correlation:
[1] 0.25

item statistics 
          key    0    1    2    3    4    5    6    7    8 miss    r    n mean
reason.4    4 0.05 0.05 0.11 0.10 0.64 0.03 0.02 0.00 0.00    0 0.59 1523 0.64
reason.16   4 0.04 0.06 0.08 0.10 0.70 0.01 0.00 0.00 0.00    0 0.53 1524 0.70
reason.17   4 0.05 0.03 0.05 0.03 0.70 0.03 0.11 0.00 0.00    0 0.59 1523 0.70
reason.19   6 0.04 0.02 0.13 0.03 0.06 0.10 0.62 0.00 0.00    0 0.56 1523 0.62
letter.7    6 0.05 0.01 0.05 0.03 0.11 0.14 0.60 0.00 0.00    0 0.58 1524 0.60
letter.33   3 0.06 0.10 0.13 0.57 0.04 0.09 0.02 0.00 0.00    0 0.56 1523 0.57
letter.34   4 0.04 0.09 0.07 0.11 0.61 0.05 0.02 0.00 0.00    0 0.59 1523 0.61
letter.58   4 0.06 0.14 0.09 0.09 0.44 0.16 0.01 0.00 0.00    0 0.58 1525 0.44
matrix.45   5 0.04 0.01 0.06 0.14 0.18 0.53 0.04 0.00 0.00    0 0.51 1523 0.53
matrix.46   2 0.04 0.12 0.55 0.07 0.11 0.06 0.05 0.00 0.00    0 0.52 1524 0.55
matrix.47   2 0.04 0.05 0.61 0.07 0.11 0.06 0.06 0.00 0.00    0 0.55 1523 0.61
matrix.55   4 0.04 0.02 0.18 0.14 0.37 0.07 0.18 0.00 0.00    0 0.45 1524 0.37
rotate.3    3 0.04 0.03 0.04 0.19 0.22 0.15 0.05 0.12 0.15    0 0.51 1523 0.19
rotate.4    2 0.04 0.03 0.21 0.05 0.18 0.04 0.04 0.25 0.15    0 0.56 1523 0.21
rotate.6    6 0.04 0.22 0.02 0.05 0.14 0.05 0.30 0.04 0.14    0 0.55 1523 0.30
rotate.8    7 0.04 0.03 0.21 0.07 0.16 0.05 0.13 0.19 0.13    0 0.48 1524 0.19
            sd
reason.4  0.48
reason.16 0.46
reason.17 0.46
reason.19 0.49
letter.7  0.49
letter.33 0.50
letter.34 0.49
letter.58 0.50
matrix.45 0.50
matrix.46 0.50
matrix.47 0.49
matrix.55 0.48
rotate.3  0.40
rotate.4  0.41
rotate.6  0.46
rotate.8  0.39
> #just convert the items to true or false 
> iq.tf <- score.multiple.choice(iq.keys,iqitems,score=FALSE)
> describe(iq.tf)  #compare to previous results
          vars    n mean   sd median trimmed mad min max range  skew kurtosis
reason.4     1 1523 0.64 0.48      1    0.68   0   0   1     1 -0.58    -1.66
reason.16    2 1524 0.70 0.46      1    0.75   0   0   1     1 -0.86    -1.26
reason.17    3 1523 0.70 0.46      1    0.75   0   0   1     1 -0.86    -1.26
reason.19    4 1523 0.62 0.49      1    0.64   0   0   1     1 -0.47    -1.78
letter.7     5 1524 0.60 0.49      1    0.62   0   0   1     1 -0.41    -1.84
letter.33    6 1523 0.57 0.50      1    0.59   0   0   1     1 -0.29    -1.92
letter.34    7 1523 0.61 0.49      1    0.64   0   0   1     1 -0.46    -1.79
letter.58    8 1525 0.44 0.50      0    0.43   0   0   1     1  0.23    -1.95
matrix.45    9 1523 0.53 0.50      1    0.53   0   0   1     1 -0.10    -1.99
matrix.46   10 1524 0.55 0.50      1    0.56   0   0   1     1 -0.20    -1.96
matrix.47   11 1523 0.61 0.49      1    0.64   0   0   1     1 -0.47    -1.78
matrix.55   12 1524 0.37 0.48      0    0.34   0   0   1     1  0.52    -1.73
rotate.3    13 1523 0.19 0.40      0    0.12   0   0   1     1  1.55     0.40
rotate.4    14 1523 0.21 0.41      0    0.14   0   0   1     1  1.40    -0.03
rotate.6    15 1523 0.30 0.46      0    0.25   0   0   1     1  0.88    -1.24
rotate.8    16 1524 0.19 0.39      0    0.11   0   0   1     1  1.62     0.63
            se
reason.4  0.01
reason.16 0.01
reason.17 0.01
reason.19 0.01
letter.7  0.01
letter.33 0.01
letter.34 0.01
letter.58 0.01
matrix.45 0.01
matrix.46 0.01
matrix.47 0.01
matrix.55 0.01
rotate.3  0.01
rotate.4  0.01
rotate.6  0.01
rotate.8  0.01
> 
> 
> 
> 
> cleanEx()
> nameEx("scrub")
> ### * scrub
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: scrub
> ### Title: A utility for basic data cleaning and recoding.  Changes values
> ###   outside of minimum and maximum limits to NA.
> ### Aliases: scrub
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> data(attitude)
> x <- scrub(attitude,isvalue=55) #make all occurrences of 55 NA
> x1 <- scrub(attitude, where=c(4,5,6), isvalue =c(30,40,50), 
+      newvalue = c(930,940,950)) #will do this for the 4th, 5th, and 6th variables
> x2 <- scrub(attitude, where=c(4,4,4), isvalue =c(30,40,50), 
+             newvalue = c(930,940,950)) #will just do it for the 4th column
> #get rid of a complicated set of cases and replace with missing values
> y <- scrub(attitude,where=2:4,min=c(20,30,40),max= c(120,110,100),isvalue= c(32,43,54))
> y1 <- scrub(attitude,where="learning",isvalue=55,newvalue=999) #change a column by name
> y2 <- scrub(attitude,where="learning",min=45,newvalue=999) #change a column by name
> 
> y3 <- scrub(attitude,where="learning",isvalue=c(45,48),
+     newvalue=999) #change a column by name look for multiple values in that column
> y4 <- scrub(attitude,where="learning",isvalue=c(45,48),
+       newvalue= c(999,-999)) #change values in one column to one of two different things
> 
> 
> 
> cleanEx()
> nameEx("set.cor")
> ### * set.cor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: setCor
> ### Title: Set Correlation and Multiple Regression from matrix or raw input
> ### Aliases: setCor setCor.diagram set.cor mat.regress matReg
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> 
> #the Kelly data from Hoteling
> kelly <- structure(list(speed = c(1, 0.4248, 0.042, 0.0215, 0.0573), power = c(0.4248, 
+ 1, 0.1487, 0.2489, 0.2843), words = c(0.042, 0.1487, 1, 0.6693, 
+ 0.4662), symbols = c(0.0215, 0.2489, 0.6693, 1, 0.6915), meaningless = c(0.0573, 
+ 0.2843, 0.4662, 0.6915, 1)), .Names = c("speed", "power", "words", 
+ "symbols", "meaningless"), class = "data.frame", row.names = c("speed", 
+ "power", "words", "symbols", "meaningless"))
> 
> 
> kelly
             speed  power  words symbols meaningless
speed       1.0000 0.4248 0.0420  0.0215      0.0573
power       0.4248 1.0000 0.1487  0.2489      0.2843
words       0.0420 0.1487 1.0000  0.6693      0.4662
symbols     0.0215 0.2489 0.6693  1.0000      0.6915
meaningless 0.0573 0.2843 0.4662  0.6915      1.0000
> 
> setCor(1:2,3:5,kelly)
Call: setCor(y = 1:2, x = 3:5, data = kelly)

Multiple Regression from matrix input 

Beta weights 
            speed power
words        0.05 -0.03
symbols     -0.07  0.12
meaningless  0.08  0.22

Multiple R 
speed power 
 0.07  0.29 
multiple R2 
 speed  power 
0.0053 0.0867 

Multiple Inflation Factor (VIF) = 1/(1-SMC) = 
      words     symbols meaningless 
       1.81        2.72        1.92 

 Unweighted multiple R 
speed power 
 0.05  0.26 
 Unweighted multiple R2 
speed power 
 0.00  0.07 

Various estimates of between set correlations
Squared Canonical Correlations 
[1] 0.0946 0.0035

 Average squared canonical correlation =  0.05
 Cohen's Set Correlation R2 =  0.1
Unweighted correlation between the two sets =  0.18> #Hotelling reports canonical correlations of .3073 and .0583  or squared correlations of
> # 0.09443329 and 0.00339889 vs. our values of  0.0946 0.0035,
> 
> 
> setCor(y=c(7:9),x=c(1:6),data=Thurstone,n.obs=213)
Call: setCor(y = c(7:9), x = c(1:6), data = Thurstone, n.obs = 213)

Multiple Regression from matrix input 

Beta weights 
                  Letter.Series Pedigrees Letter.Group
Sentences                  0.23      0.20         0.18
Vocabulary                 0.08      0.15        -0.05
Sent.Completion            0.04      0.21         0.07
First.Letters              0.12      0.02         0.17
Four.Letter.Words          0.19      0.11         0.24
Suffixes                  -0.05     -0.01         0.00

Multiple R 
Letter.Series     Pedigrees  Letter.Group 
         0.51          0.59          0.51 
multiple R2 
Letter.Series     Pedigrees  Letter.Group 
         0.26          0.35          0.26 

Multiple Inflation Factor (VIF) = 1/(1-SMC) = 
        Sentences        Vocabulary   Sent.Completion     First.Letters 
             3.71              3.94              3.00              2.20 
Four.Letter.Words          Suffixes 
             2.00              1.74 

 Unweighted multiple R 
Letter.Series     Pedigrees  Letter.Group 
         0.49          0.56          0.48 
 Unweighted multiple R2 
Letter.Series     Pedigrees  Letter.Group 
         0.25          0.31          0.23 

 SE of Beta weights 
                  Letter.Series Pedigrees Letter.Group
Sentences                  0.12      0.11         0.12
Vocabulary                 0.12      0.11         0.12
Sent.Completion            0.10      0.10         0.10
First.Letters              0.09      0.08         0.09
Four.Letter.Words          0.08      0.08         0.08
Suffixes                   0.08      0.07         0.08

 t of Beta Weights 
                  Letter.Series Pedigrees Letter.Group
Sentences                  2.02      1.87         1.58
Vocabulary                 0.71      1.34        -0.38
Sent.Completion            0.39      2.11         0.68
First.Letters              1.33      0.26         1.89
Four.Letter.Words          2.27      1.41         2.87
Suffixes                  -0.62     -0.12         0.05

Probability of t < 
                  Letter.Series Pedigrees Letter.Group
Sentences                 0.044     0.063       0.1200
Vocabulary                0.480     0.180       0.7000
Sent.Completion           0.700     0.036       0.5000
First.Letters             0.180     0.790       0.0610
Four.Letter.Words         0.024     0.160       0.0045
Suffixes                  0.540     0.900       0.9600

 Shrunken R2 
Letter.Series     Pedigrees  Letter.Group 
         0.24          0.33          0.24 

Standard Error of R2  
Letter.Series     Pedigrees  Letter.Group 
        0.050         0.051         0.050 

F 
Letter.Series     Pedigrees  Letter.Group 
        12.37         18.19         12.01 

Probability of F < 
Letter.Series     Pedigrees  Letter.Group 
     7.01e-12      0.00e+00      1.47e-11 

 degrees of freedom of regression 
[1]   6 206

Various estimates of between set correlations
Squared Canonical Correlations 
[1] 0.4115 0.0689 0.0069
Chisq of canonical correlations 
[1] 109.7  14.8   1.4

 Average squared canonical correlation =  0.16
 Cohen's Set Correlation R2 =  0.46
 Shrunken Set Correlation R2 =  0.4
 F and df of Cohen's Set Correlation  7.47 18 560.51
Unweighted correlation between the two sets =  0.62> #now try partialling out some variables
> set.cor(y=c(7:9),x=c(1:3),z=c(4:6),data=Thurstone) #compare with the previous
Call: setCor(y = y, x = x, data = data, z = z, n.obs = n.obs, use = use, 
    std = std, square = square, main = main, plot = plot, show = show)

Multiple Regression from matrix input 

Beta weights 
                Letter.Series Pedigrees Letter.Group
Sentences                0.23      0.20         0.18
Vocabulary               0.08      0.15        -0.05
Sent.Completion          0.04      0.21         0.07

Multiple R 
Letter.Series     Pedigrees  Letter.Group 
         0.31          0.47          0.20 
multiple R2 
Letter.Series     Pedigrees  Letter.Group 
        0.098         0.218         0.040 

Multiple Inflation Factor (VIF) = 1/(1-SMC) = 
      Sentences      Vocabulary Sent.Completion 
           2.73            2.69            2.20 

 Unweighted multiple R 
Letter.Series     Pedigrees  Letter.Group 
         0.28          0.43          0.16 
 Unweighted multiple R2 
Letter.Series     Pedigrees  Letter.Group 
         0.08          0.18          0.03 

Various estimates of between set correlations
Squared Canonical Correlations 
[1] 0.2281 0.0065 0.0041

 Average squared canonical correlation =  0.08
 Cohen's Set Correlation R2 =  0.24
Unweighted correlation between the two sets =  0.41> #compare complete print out with summary printing 
> sc <- setCor(x=c("gender","education"),y=c("SATV","SATQ"),data=sat.act) # regression from raw data
> sc
Call: setCor(y = c("SATV", "SATQ"), x = c("gender", "education"), data = sat.act)

Multiple Regression from raw data 

Beta weights 
           SATV  SATQ
gender    -0.02 -0.17
education  0.05  0.05

Multiple R 
SATV SATQ 
0.05 0.17 
multiple R2 
  SATV   SATQ 
0.0027 0.0299 

Multiple Inflation Factor (VIF) = 1/(1-SMC) = 
   gender education 
     1.01      1.01 

 Unweighted multiple R 
SATV SATQ 
0.04 0.14 
 Unweighted multiple R2 
SATV SATQ 
0.00 0.02 

 SE of Beta weights 
          SATV SATQ
gender    0.04 0.04
education 0.04 0.04

 t of Beta Weights 
           SATV  SATQ
gender    -0.61 -4.54
education  1.28  1.32

Probability of t < 
          SATV    SATQ
gender    0.54 6.6e-06
education 0.20 1.9e-01

 Shrunken R2 
    SATV     SATQ 
-0.00017  0.02710 

Standard Error of R2  
  SATV   SATQ 
0.0039 0.0126 

F 
 SATV  SATQ 
 0.94 10.73 

Probability of F < 
    SATV     SATQ 
3.91e-01 2.56e-05 

 degrees of freedom of regression 
[1]   2 697

Various estimates of between set correlations
Squared Canonical Correlations 
[1] 0.0417 0.0021
Chisq of canonical correlations 
[1] 29.7  1.4

 Average squared canonical correlation =  0.02
 Cohen's Set Correlation R2 =  0.04
 Shrunken Set Correlation R2 =  0.04
 F and df of Cohen's Set Correlation  7.91 4 1388
Unweighted correlation between the two sets =  -0.04> summary(sc)

Multiple Regression from raw data 
setCor(y = c("SATV", "SATQ"), x = c("gender", "education"), data = sat.act)

Multiple Regression from matrix input 

Beta weights 
            SATV   SATQ
gender    -0.023 -0.170
education  0.048  0.049

Multiple R 
 SATV  SATQ 
0.052 0.173 

Multiple R2 
  SATV   SATQ 
0.0027 0.0299 

Cohen's set correlation R2 
[1] 0.044

Squared Canonical Correlations
[1] 0.0417 0.0021
> 
> 
> 
> cleanEx()
> nameEx("sim")
> ### * sim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim
> ### Title: Functions to simulate psychological/psychometric data.
> ### Aliases: sim sim.simplex sim.minor sim.omega sim.general sim.parallel
> ###   sim.rasch sim.irt sim.npl sim.npn sim.poly sim.poly.npl sim.poly.npn
> ###   sim.poly.ideal sim.poly.ideal.npl sim.poly.ideal.npn sim.poly.mat
> ### Keywords: multivariate datagen
> 
> ### ** Examples
> 
> simplex <- sim.simplex() #create the default simplex structure
> lowerMat(simplex) #the correlation matrix
    V1   V2   V3   V4   V5   V6   V7   V8   V9   V10  V11 
V1  1.00                                                  
V2  0.80 1.00                                             
V3  0.64 0.80 1.00                                        
V4  0.51 0.64 0.80 1.00                                   
V5  0.41 0.51 0.64 0.80 1.00                              
V6  0.33 0.41 0.51 0.64 0.80 1.00                         
V7  0.26 0.33 0.41 0.51 0.64 0.80 1.00                    
V8  0.21 0.26 0.33 0.41 0.51 0.64 0.80 1.00               
V9  0.17 0.21 0.26 0.33 0.41 0.51 0.64 0.80 1.00          
V10 0.13 0.17 0.21 0.26 0.33 0.41 0.51 0.64 0.80 1.00     
V11 0.11 0.13 0.17 0.21 0.26 0.33 0.41 0.51 0.64 0.80 1.00
V12 0.09 0.11 0.13 0.17 0.21 0.26 0.33 0.41 0.51 0.64 0.80
[1] 1.00
> #create a congeneric matrix
> congeneric <- sim.congeneric()
> lowerMat(congeneric)
   V1   V2   V3   V4  
V1 1.00               
V2 0.56 1.00          
V3 0.48 0.42 1.00     
V4 0.40 0.35 0.30 1.00
> R <- sim.hierarchical()
> lowerMat(R)
   V1   V2   V3   V4   V5   V6   V7   V8   V9  
V1 1.00                                        
V2 0.56 1.00                                   
V3 0.48 0.42 1.00                              
V4 0.40 0.35 0.30 1.00                         
V5 0.35 0.30 0.26 0.42 1.00                    
V6 0.29 0.25 0.22 0.35 0.30 1.00               
V7 0.30 0.26 0.23 0.24 0.20 0.17 1.00          
V8 0.25 0.22 0.19 0.20 0.17 0.14 0.30 1.00     
V9 0.20 0.18 0.15 0.16 0.13 0.11 0.24 0.20 1.00
> #now simulate categorical items with the hierarchical factor structure.  
> #Let the items be dichotomous with varying item difficulties.
> marginals = matrix(c(seq(.1,.9,.1),seq(.9,.1,-.1)),byrow=TRUE,nrow=2)
> X <- sim.poly.mat(R=R,m=marginals,n=1000)
> lowerCor(X) #show the raw correlations
   C1   C2   C3   C4   C5   C6   C7   C8   C9  
R1 1.00                                        
R2 0.40 1.00                                   
R3 0.37 0.25 1.00                              
R4 0.29 0.27 0.19 1.00                         
R5 0.27 0.18 0.23 0.23 1.00                    
R6 0.15 0.18 0.07 0.23 0.13 1.00               
R7 0.23 0.14 0.21 0.15 0.21 0.02 1.00          
R8 0.12 0.18 0.12 0.14 0.09 0.12 0.24 1.00     
R9 0.17 0.08 0.09 0.06 0.13 0.02 0.16 0.13 1.00
> #lowerMat(tetrachoric(X)$rho) # show the tetrachoric correlations (not run)
> #generate a structure 
> fx <- matrix(c(.9,.8,.7,rep(0,6),c(.8,.7,.6)),ncol=2)
> fy <- c(.6,.5,.4)
> Phi <- matrix(c(1,0,.5,0,1,.4,0,0,0),ncol=3)
> R <- sim.structure(fx,Phi,fy) 
> cor.plot(R$model) #show it graphically
> 
> simp <- sim.simplex()
> #show the simplex structure using cor.plot
> cor.plot(simp,colors=TRUE,main="A simplex structure")
> #Show a STARS model 
> simp <- sim.simplex(alpha=.8,lambda=.4)
> #show the simplex structure using cor.plot
> cor.plot(simp,colors=TRUE,main="State Trait Auto Regressive Simplex" )
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.VSS")
> ### * sim.VSS
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.VSS
> ### Title: create VSS like data
> ### Aliases: sim.VSS VSS.simulate VSS.sim
> ### Keywords: multivariate models datagen
> 
> ### ** Examples
> 
> ## Not run: 
> ##D simulated <- sim.VSS(1000,20,4,.6)
> ##D vss <- VSS(simulated,rotate="varimax")
> ##D VSS.plot(vss)
> ## End(Not run)
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.anova")
> ### * sim.anova
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.anova
> ### Title: Simulate a 3 way balanced ANOVA or linear model, with or without
> ###   repeated measures.
> ### Aliases: sim.anova
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> set.seed(42)
> data.df <- sim.anova(es1=1,es2=.5,es13=1)  # one main effect and one interaction
> describe(data.df)
     vars  n mean   sd median trimmed  mad   min  max range  skew kurtosis   se
IV1*    1 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
IV2*    2 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
IV3*    3 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
DV      4 16 0.49 1.85    1.0    0.62 1.63 -3.78 3.03   6.8 -0.67    -0.45 0.46
> pairs.panels(data.df)   #show how the design variables are orthogonal
> #
> summary(lm(DV~IV1*IV2*IV3,data=data.df))

Call:
lm(formula = DV ~ IV1 * IV2 * IV3, data = data.df)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.8966 -0.3917  0.0000  0.3917  0.8966 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)      1.1798     0.5803   2.033 0.076503 .  
IV11            -1.9469     0.8207  -2.372 0.045093 *  
IV21             0.1076     0.8207   0.131 0.898974    
IV31            -4.0620     0.8207  -4.949 0.001122 ** 
IV11:IV21        2.6342     1.1607   2.269 0.052933 .  
IV11:IV31        6.0582     1.1607   5.220 0.000803 ***
IV21:IV31        2.0421     1.1607   1.759 0.116556    
IV11:IV21:IV31  -3.3524     1.6415  -2.042 0.075397 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.8207 on 8 degrees of freedom
Multiple R-squared:  0.8952,	Adjusted R-squared:  0.8035 
F-statistic: 9.764 on 7 and 8 DF,  p-value: 0.002274

> summary(aov(DV~IV1*IV2*IV3,data=data.df))
            Df Sum Sq Mean Sq F value   Pr(>F)    
IV1          1  9.749   9.749  14.473 0.005204 ** 
IV2          1 10.337  10.337  15.346 0.004435 ** 
IV3          1  2.890   2.890   4.290 0.072097 .  
IV1:IV2      1  0.918   0.918   1.362 0.276751    
IV1:IV3      1 19.202  19.202  28.507 0.000695 ***
IV2:IV3      1  0.134   0.134   0.199 0.667572    
IV1:IV2:IV3  1  2.810   2.810   4.171 0.075397 .  
Residuals    8  5.389   0.674                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> set.seed(42)
>  #demonstrate the effect of not centering the data on the regression
> data.df <- sim.anova(es1=1,es2=.5,es13=1,center=FALSE)  #
> describe(data.df)
     vars  n mean   sd median trimmed  mad   min  max range  skew kurtosis   se
IV1*    1 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
IV2*    2 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
IV3*    3 16 1.50 0.52    1.5    1.50 0.74  1.00 2.00   1.0  0.00    -2.12 0.13
DV      4 16 0.49 1.85    1.0    0.62 1.63 -3.78 3.03   6.8 -0.67    -0.45 0.46
> #
> #this one is incorrect, because the IVs are not centered
> summary(lm(DV~IV1*IV2*IV3,data=data.df)) 

Call:
lm(formula = DV ~ IV1 * IV2 * IV3, data = data.df)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.8966 -0.3917  0.0000  0.3917  0.8966 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)      1.1798     0.5803   2.033 0.076503 .  
IV11            -1.9469     0.8207  -2.372 0.045093 *  
IV21             0.1076     0.8207   0.131 0.898974    
IV31            -4.0620     0.8207  -4.949 0.001122 ** 
IV11:IV21        2.6342     1.1607   2.269 0.052933 .  
IV11:IV31        6.0582     1.1607   5.220 0.000803 ***
IV21:IV31        2.0421     1.1607   1.759 0.116556    
IV11:IV21:IV31  -3.3524     1.6415  -2.042 0.075397 .  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.8207 on 8 degrees of freedom
Multiple R-squared:  0.8952,	Adjusted R-squared:  0.8035 
F-statistic: 9.764 on 7 and 8 DF,  p-value: 0.002274

> 
> summary(aov(DV~IV1*IV2*IV3,data=data.df)) #compare with the lm model
            Df Sum Sq Mean Sq F value   Pr(>F)    
IV1          1  9.749   9.749  14.473 0.005204 ** 
IV2          1 10.337  10.337  15.346 0.004435 ** 
IV3          1  2.890   2.890   4.290 0.072097 .  
IV1:IV2      1  0.918   0.918   1.362 0.276751    
IV1:IV3      1 19.202  19.202  28.507 0.000695 ***
IV2:IV3      1  0.134   0.134   0.199 0.667572    
IV1:IV2:IV3  1  2.810   2.810   4.171 0.075397 .  
Residuals    8  5.389   0.674                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> #now examine multiple levels and quadratic terms
> set.seed(42)
> data.df <- sim.anova(es1=1,es13=1,n2=3,n3=4,es22=1)
> summary(lm(DV~IV1*IV2*IV3,data=data.df))

Call:
lm(formula = DV ~ IV1 * IV2 * IV3, data = data.df)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0018 -0.3397  0.0000  0.3397  2.0018 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)       3.4260     0.7844   4.367 0.000207 ***
IV11             -2.7790     1.1093  -2.505 0.019434 *  
IV20             -3.0489     1.1093  -2.748 0.011190 *  
IV21             -1.2009     1.1093  -1.083 0.289778    
IV3-1            -1.5254     1.1093  -1.375 0.181825    
IV31             -4.4713     1.1093  -4.031 0.000488 ***
IV33             -5.1016     1.1093  -4.599 0.000115 ***
IV11:IV20         1.5126     1.5689   0.964 0.344596    
IV11:IV21         1.3254     1.5689   0.845 0.406549    
IV11:IV3-1        3.2038     1.5689   2.042 0.052272 .  
IV11:IV31         6.1556     1.5689   3.924 0.000639 ***
IV11:IV33         8.5233     1.5689   5.433  1.4e-05 ***
IV20:IV3-1        2.1234     1.5689   1.353 0.188512    
IV21:IV3-1        1.1223     1.5689   0.715 0.481280    
IV20:IV31         1.3930     1.5689   0.888 0.383389    
IV21:IV31         2.2484     1.5689   1.433 0.164711    
IV20:IV33         1.5838     1.5689   1.010 0.322782    
IV21:IV33         1.5504     1.5689   0.988 0.332899    
IV11:IV20:IV3-1  -2.6968     2.2187  -1.215 0.236008    
IV11:IV21:IV3-1  -1.2671     2.2187  -0.571 0.573237    
IV11:IV20:IV31   -0.4246     2.2187  -0.191 0.849846    
IV11:IV21:IV31   -3.3169     2.2187  -1.495 0.147959    
IV11:IV20:IV33   -2.4872     2.2187  -1.121 0.273368    
IV11:IV21:IV33   -0.6422     2.2187  -0.289 0.774713    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.109 on 24 degrees of freedom
Multiple R-squared:  0.8651,	Adjusted R-squared:  0.7359 
F-statistic: 6.693 on 23 and 24 DF,  p-value: 8.495e-06

> summary(aov(DV~IV1*IV2*IV3,data=data.df))
            Df Sum Sq Mean Sq F value   Pr(>F)    
IV1          1  36.11   36.11  29.346 1.45e-05 ***
IV2          2  32.23   16.12  13.096 0.000143 ***
IV3          3  10.76    3.59   2.914 0.054955 .  
IV1:IV2      2   0.03    0.01   0.011 0.988728    
IV1:IV3      3  98.02   32.67  26.549 8.49e-08 ***
IV2:IV3      6   3.71    0.62   0.502 0.800543    
IV1:IV2:IV3  6   8.58    1.43   1.162 0.358682    
Residuals   24  29.54    1.23                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
> pairs.panels(data.df)
> #
> data.df <- sim.anova(es1=1,es2=-.5,within=c(-1,0,1),n=10)
> pairs.panels(data.df)
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.congeneric")
> ### * sim.congeneric
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.congeneric
> ### Title: Simulate a congeneric data set
> ### Aliases: congeneric.sim sim.congeneric make.congeneric
> ### Keywords: multivariate datagen
> 
> ### ** Examples
> 
> test <- sim.congeneric(c(.9,.8,.7,.6))   #just the population matrix
> test <- sim.congeneric(c(.9,.8,.7,.6),N=100)   # a sample correlation matrix
> test <- sim.congeneric(short=FALSE, N=100)
> round(cor(test$observed),2) # show  a congeneric correlation matrix
     V1   V2   V3   V4
V1 1.00 0.50 0.49 0.28
V2 0.50 1.00 0.37 0.29
V3 0.49 0.37 1.00 0.42
V4 0.28 0.29 0.42 1.00
> f1=fa(test$observed,scores=TRUE)
> round(cor(f1$scores,test$latent),2)  
    theta   e1   e2   e3   e4
MR1  0.84 0.29 0.14 0.47 0.18
>      #factor score estimates are correlated with but not equal to the factor scores
> set.seed(42)
> #500 responses to 4 discrete items
> items <- sim.congeneric(N=500,short=FALSE,low=-2,high=2,categorical=TRUE) 
> d4 <- irt.fa(items$observed)  #item response analysis of congeneric measures
6 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.hierarchical")
> ### * sim.hierarchical
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.hierarchical
> ### Title: Create a population or sample correlation matrix, perhaps with
> ###   hierarchical structure.
> ### Aliases: sim.hierarchical make.hierarchical
> ### Keywords: multivariate models datagen
> 
> ### ** Examples
> 
> 
> gload <-  gload<-matrix(c(.9,.8,.7),nrow=3)    # a higher order factor matrix
> fload <-matrix(c(                    #a lower order (oblique) factor matrix
+            .8,0,0,
+            .7,0,.0,
+            .6,0,.0,
+             0,.7,.0,
+             0,.6,.0,
+             0,.5,0,
+             0,0,.6,
+             0,0,.5,
+             0,0,.4),   ncol=3,byrow=TRUE)
>             
> jensen <- sim.hierarchical(gload,fload)    #the test set used by omega
> round(jensen,2)     
     V1   V2   V3   V4   V5   V6   V7   V8   V9
V1 1.00 0.56 0.48 0.40 0.35 0.29 0.30 0.25 0.20
V2 0.56 1.00 0.42 0.35 0.30 0.25 0.26 0.22 0.18
V3 0.48 0.42 1.00 0.30 0.26 0.22 0.23 0.19 0.15
V4 0.40 0.35 0.30 1.00 0.42 0.35 0.24 0.20 0.16
V5 0.35 0.30 0.26 0.42 1.00 0.30 0.20 0.17 0.13
V6 0.29 0.25 0.22 0.35 0.30 1.00 0.17 0.14 0.11
V7 0.30 0.26 0.23 0.24 0.20 0.17 1.00 0.30 0.24
V8 0.25 0.22 0.19 0.20 0.17 0.14 0.30 1.00 0.20
V9 0.20 0.18 0.15 0.16 0.13 0.11 0.24 0.20 1.00
> 
> #simulate a non-hierarchical structure
> fload <- matrix(c(c(c(.9,.8,.7,.6),rep(0,20)),c(c(.9,.8,.7,.6),rep(0,20)),
+     c(c(.9,.8,.7,.6),rep(0,20)),c(c(c(.9,.8,.7,.6),rep(0,20)),c(.9,.8,.7,.6))),ncol=5)
> gload <- matrix(rep(0,5))
> five.factor <- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
> #do it again with a hierachical structure
> gload <- matrix(rep(.7,5)  )
> five.factor.g <- sim.hierarchical(gload,fload,500,TRUE) #create sample data set
> #compare these two with omega
> #not run
> #om.5 <- omega(five.factor$observed,5)
> #om.5g <- omega(five.factor.g$observed,5)
> 
> 
> 
> cleanEx()
> nameEx("sim.item")
> ### * sim.item
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.item
> ### Title: Generate simulated data structures for circumplex, spherical, or
> ###   simple structure
> ### Aliases: sim.spherical item.sim sim.item sim.dichot item.dichot
> ###   sim.circ circ.sim con2cat
> ### Keywords: multivariate datagen
> 
> ### ** Examples
> 
> 
> round(cor(circ.sim(nvar=8,nsub=200)),2)
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,]  1.00  0.26 -0.04 -0.23 -0.21 -0.22  0.11  0.28
[2,]  0.26  1.00  0.21  0.00 -0.14 -0.35 -0.20  0.08
[3,] -0.04  0.21  1.00  0.35  0.08 -0.26 -0.42 -0.27
[4,] -0.23  0.00  0.35  1.00  0.23 -0.01 -0.30 -0.48
[5,] -0.21 -0.14  0.08  0.23  1.00  0.18  0.00 -0.26
[6,] -0.22 -0.35 -0.26 -0.01  0.18  1.00  0.20 -0.03
[7,]  0.11 -0.20 -0.42 -0.30  0.00  0.20  1.00  0.26
[8,]  0.28  0.08 -0.27 -0.48 -0.26 -0.03  0.26  1.00
> plot(fa(circ.sim(16,500),2)$loadings,main="Circumplex Structure") #circumplex structure
> #
> #
> plot(fa(item.sim(16,500),2)$loadings,main="Simple Structure") #simple structure
> #
> cluster.plot(fa(item.dichot(16,low=0,high=1),2))
> 
>  set.seed(42)
>  
>  data <- mnormt::rmnorm(1000, c(0, 0), matrix(c(1, .5, .5, 1), 2, 2)) #continuous data
>  new <- con2cat(data,c(-1.5,-.5,.5,1.5))  #discreet data
>  polychoric(new)
1 cells were adjusted for 0 values using the correction for continuity. Examine your data carefully.
Call: polychoric(x = new)
Polychoric correlations 
   C1   C2  
R1 1.00     
R2 0.48 1.00

 with tau of 
        1     2    3   4
[1,] -1.5 -0.49 0.54 1.6
[2,] -1.5 -0.52 0.52 1.6
> #not run
> #x12 <- sim.item(12,gloading=.6)
> #f3 <- fa(x12,3,rotate="none")
> #f3  #observe the general factor
> #oblimin(f3$loadings[,2:3])  #show the 2nd and 3 factors.
> #f3 <- fa(x12,3)   #now do it with oblimin rotation
> #f3  # not what one naively expect.
> 
> 
> 
> 
> cleanEx()
> nameEx("sim.multilevel")
> ### * sim.multilevel
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.multilevel
> ### Title: Simulate multilevel data with specified within group and between
> ###   group correlations
> ### Aliases: sim.multilevel sim.multi
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #First, show a few results from sim.multi
> 
> x.df <- sim.multi()  #the default is 4 subjects for two variables 
> #                     over 16 days measured 6 times/day 
> 
> #sb <- statsBy(x.df,group ="id",cors=TRUE)
> #round(sb$within,2)  #show the within subject correlations
> 
> #get some parameters to simulate
> data(withinBetween)
> wb.stats <- statsBy(withinBetween,"Group")
> rwg <- wb.stats$rwg
> rbg <- wb.stats$rbg
> eta <- rep(.5,9)
> 
> #simulate them.  Try this again to see how it changes
> XY <- sim.multilevel(ncases=100,ngroups=10,rwg=rwg,rbg=rbg,eta=eta)
> lowerCor(XY$wg)  #based upon 89 df
   C1    C2    C3    C4    C5    C6    C7    C8    C9   
R1  1.00                                                
R2 -0.03  1.00                                          
R3 -1.00  0.03  1.00                                    
R4  1.00 -0.03 -1.00  1.00                              
R5 -0.03  1.00  0.03 -0.03  1.00                        
R6 -1.00  0.03  1.00 -1.00  0.03  1.00                  
R7  1.00 -0.03 -1.00  1.00 -0.03 -1.00  1.00            
R8 -0.03  1.00  0.03 -0.03  1.00  0.03 -0.03  1.00      
R9 -1.00  0.03  1.00 -1.00  0.03  1.00 -1.00  0.03  1.00
> lowerCor(XY$bg)  #based upon 9 df   -- 
   C1    C2    C3    C4    C5    C6    C7    C8    C9   
R1  1.00                                                
R2  1.00  1.00                                          
R3  1.00  1.00  1.00                                    
R4 -0.48 -0.48 -0.48  1.00                              
R5 -0.48 -0.48 -0.48  1.00  1.00                        
R6 -0.48 -0.48 -0.48  1.00  1.00  1.00                  
R7 -1.00 -1.00 -1.00  0.48  0.48  0.48  1.00            
R8 -1.00 -1.00 -1.00  0.48  0.48  0.48  1.00  1.00      
R9 -1.00 -1.00 -1.00  0.48  0.48  0.48  1.00  1.00  1.00
> 
> 
> 
> cleanEx()
> nameEx("sim.structural")
> ### * sim.structural
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sim.structure
> ### Title: Create correlation matrices or data matrices with a particular
> ###   measurement and structural model
> ### Aliases: sim.structure sim.structural sim.correlation
> ### Keywords: multivariate datagen
> 
> ### ** Examples
> 
> fx <-matrix(c( .9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)  
> fy <- matrix(c(.6,.5,.4),ncol=1)
> rownames(fx) <- c("V","Q","A","nach","Anx")
> rownames(fy)<- c("gpa","Pre","MA")
> Phi <-matrix( c(1,0,.7,.0,1,.7,.7,.7,1),ncol=3)
> gre.gpa <- sim.structural(fx,Phi,fy)
> print(gre.gpa,2)  
Call: sim.structural(fx = fx, Phi = Phi, fy = fy)

 $model (Population correlation matrix) 
        V    Q     A  nach   Anx   gpa   Pre    MA
V    1.00 0.72  0.54  0.00  0.00  0.38  0.32  0.25
Q    0.72 1.00  0.48  0.00  0.00  0.34  0.28  0.22
A    0.54 0.48  1.00  0.48 -0.42  0.50  0.42  0.34
nach 0.00 0.00  0.48  1.00 -0.56  0.34  0.28  0.22
Anx  0.00 0.00 -0.42 -0.56  1.00 -0.29 -0.24 -0.20
gpa  0.38 0.34  0.50  0.34 -0.29  1.00  0.30  0.24
Pre  0.32 0.28  0.42  0.28 -0.24  0.30  1.00  0.20
MA   0.25 0.22  0.34  0.22 -0.20  0.24  0.20  1.00

$reliability (population reliability) 
   V    Q    A nach  Anx  gpa  Pre   MA 
0.81 0.64 0.72 0.64 0.49 0.36 0.25 0.16 
> #correct for attenuation to see structure
> 
> round(correct.cor(gre.gpa$model,gre.gpa$reliability),2)  
        V    Q     A  nach   Anx   gpa   Pre    MA
V    0.81 1.00  0.71  0.00  0.00  0.70  0.70  0.70
Q    0.72 0.64  0.71  0.00  0.00  0.70  0.70  0.70
A    0.54 0.48  0.72  0.71 -0.71  0.99  0.99  0.99
nach 0.00 0.00  0.48  0.64 -1.00  0.70  0.70  0.70
Anx  0.00 0.00 -0.42 -0.56  0.49 -0.70 -0.70 -0.70
gpa  0.38 0.34  0.50  0.34 -0.29  0.36  1.00  1.00
Pre  0.32 0.28  0.42  0.28 -0.24  0.30  0.25  1.00
MA   0.25 0.22  0.34  0.22 -0.20  0.24  0.20  0.16
> congeneric <- sim.structure(f=c(.9,.8,.7,.6)) # a congeneric model 
> congeneric 
Call: sim.structure(f = c(0.9, 0.8, 0.7, 0.6))

 $model (Population correlation matrix) 
     V1   V2   V3   V4
V1 1.00 0.72 0.63 0.54
V2 0.72 1.00 0.56 0.48
V3 0.63 0.56 1.00 0.42
V4 0.54 0.48 0.42 1.00

$reliability (population reliability) 
[1] 0.81 0.64 0.49 0.36
> 
> 
> 
> 
> cleanEx()
> nameEx("simulation.circ")
> ### * simulation.circ
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: simulation.circ
> ### Title: Simulations of circumplex and simple structure
> ### Aliases: simulation.circ circ.simulation simulation.circ circ.sim.plot
> ### Keywords: multivariate datagen
> 
> ### ** Examples
> 
> #not run
> demo <- simulation.circ()
> boxplot(demo[3:14])
> title("4 tests of Circumplex Structure",sub="Circumplex, Ellipsoid, Simple Structure")
> circ.sim.plot(demo[3:14])  #compare these results to real data
> 
> 
> 
> cleanEx()
> nameEx("skew")
> ### * skew
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: mardia
> ### Title: Calculate univariate or multivariate (Mardia's test) skew and
> ###   kurtosis for a vector, matrix, or data.frame
> ### Aliases: mardia skew kurtosi
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> round(skew(attitude),2)   #type 3 (default)
[1] -0.36 -0.22  0.38 -0.05  0.20 -0.87  0.85
> round(kurtosi(attitude),2)  #type 3 (default)
    rating complaints privileges   learning     raises   critical    advance 
     -0.77      -0.68      -0.41      -1.22      -0.60       0.17       0.47 
> #for the differences between the three types of skew and kurtosis:
> round(skew(attitude,type=1),2)  #type 1
[1] -0.38 -0.23  0.40 -0.06  0.21 -0.91  0.89
> round(skew(attitude,type=2),2)  #type 2 
[1] -0.40 -0.24  0.42 -0.06  0.22 -0.96  0.94
> mardia(attitude)
Call: mardia(x = attitude)

Mardia tests of multivariate skew and kurtosis
Use describe(x) the to get univariate tests
n.obs = 30   num.vars =  7 
b1p =  20.09   skew =  100.45  with probability =  0.11
 small sample skew =  113.23  with probability =  0.018
b2p =  61.91   kurtosis =  -0.27  with probability =  0.79> x <- matrix(rnorm(1000),ncol=10)
> describe(x)
    vars   n  mean   sd median trimmed  mad   min  max range  skew kurtosis
X1     1 100  0.11 0.90   0.11    0.12 0.87 -2.21 2.40  4.62 -0.07    -0.05
X2     2 100 -0.04 0.96  -0.18   -0.08 0.87 -1.91 2.31  4.22  0.44    -0.31
X3     3 100  0.03 1.03   0.00    0.06 0.94 -2.89 2.65  5.54 -0.24     0.26
X4     4 100  0.05 0.99   0.02    0.09 0.97 -2.59 1.97  4.56 -0.32    -0.29
X5     5 100 -0.04 1.17  -0.14   -0.07 1.03 -3.01 3.81  6.82  0.35     0.36
X6     6 100 -0.04 0.97  -0.06   -0.02 1.01 -2.53 2.02  4.55 -0.22    -0.31
X7     7 100 -0.20 1.08  -0.25   -0.22 1.17 -2.94 2.68  5.62  0.11    -0.16
X8     8 100  0.00 1.10  -0.04    0.01 1.38 -2.60 2.02  4.62 -0.11    -0.85
X9     9 100  0.01 1.09   0.03    0.01 1.00 -2.42 3.06  5.48  0.08    -0.06
X10   10 100  0.00 1.05   0.11    0.02 0.93 -3.00 2.32  5.32 -0.22     0.24
      se
X1  0.09
X2  0.10
X3  0.10
X4  0.10
X5  0.12
X6  0.10
X7  0.11
X8  0.11
X9  0.11
X10 0.11
> mardia(x)
Call: mardia(x = x)

Mardia tests of multivariate skew and kurtosis
Use describe(x) the to get univariate tests
n.obs = 100   num.vars =  10 
b1p =  10.93   skew =  182.16  with probability =  0.97
 small sample skew =  188.64  with probability =  0.94
b2p =  115.25   kurtosis =  -1.53  with probability =  0.13> 
> 
> 
> cleanEx()
> nameEx("smc")
> ### * smc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: smc
> ### Title: Find the Squared Multiple Correlation (SMC) of each variable
> ###   with the remaining variables in a matrix
> ### Aliases: smc
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> R <- make.hierarchical()
> round(smc(R),2)
  V1   V2   V3   V4   V5   V6   V7   V8   V9 
0.44 0.37 0.28 0.30 0.24 0.17 0.18 0.14 0.09 
>  
> 
> 
> cleanEx()
> nameEx("spider")
> ### * spider
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: spider
> ### Title: Make "radar" or "spider" plots.
> ### Aliases: spider radar
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> op <- par(mfrow=c(3,2))
> spider(y=1,x=2:9,data=Thurstone,connect=FALSE) #a radar plot
> spider(y=1,x=2:9,data=Thurstone) #same plot as a spider plot
>  spider(y=1:3,x=4:9,data=Thurstone,overlay=TRUE)
>  #make a somewhat oversized plot
> spider(y=26:28,x=1:25,data=cor(bfi,use="pairwise"),fill=TRUE,scale=2) 
> par(op)
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx()
> nameEx("statsBy")
> ### * statsBy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: statsBy
> ### Title: Find statistics (including correlations) within and between
> ###   groups for basic multilevel analyses
> ### Aliases: statsBy statsBy.boot statsBy.boot.summary faBy
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #Taken from Pedhazur, 1997
> pedhazur <- structure(list(Group = c(1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
+ 2L), X = c(5L, 2L, 4L, 6L, 3L, 8L, 5L, 7L, 9L, 6L), Y = 1:10), .Names = c("Group", 
+ "X", "Y"), class = "data.frame", row.names = c(NA, -10L))
> pedhazur
   Group X  Y
1      1 5  1
2      1 2  2
3      1 4  3
4      1 6  4
5      1 3  5
6      2 8  6
7      2 5  7
8      2 7  8
9      2 9  9
10     2 6 10
> ped.stats <- statsBy(pedhazur,"Group")
> ped.stats
Statistics within and between groups  
Call: statsBy(data = pedhazur, group = "Group")
Intraclass Correlation 1 (Percentage of variance due to groups) 
Group     X     Y 
 1.00  0.62  0.83 
Intraclass Correlation 2 (Reliability of group differences) 
Group     X     Y 
 1.00  0.89  0.96 
eta^2 between groups  
X.bg Y.bg 
0.53 0.76 

To see the correlations between and within groups, use the short=FALSE option in your print statement.
Many results are not shown directly. To see specific objects select from the following list:
 mean sd n F ICC1 ICC2 ci1 ci2 raw rbg pbg rwg nw pwg etabg etawg nwg nG Call> 
> 
> #Now do this for the sat.act data set
> sat.stats <- statsBy(sat.act,c("education","gender"),cor=TRUE)   #group by two grouping variables
> print(sat.stats,short=FALSE)
Statistics within and between groups  
Call: statsBy(data = sat.act, group = c("education", "gender"), cor = TRUE)
Intraclass Correlation 1 (Percentage of variance due to groups) 
   gender education       age       ACT      SATV      SATQ 
     1.00      1.00      0.45      0.03      0.00      0.03 
Intraclass Correlation 2 (Reliability of group differences) 
   gender education       age       ACT      SATV      SATQ 
     1.00      1.00      0.98      0.67     -0.11      0.64 
eta^2 between groups  
 age.bg  ACT.bg SATV.bg SATQ.bg 
   0.44    0.05    0.01    0.04 
Correlation between groups 
        ag.bg ACT.b SATV. SATQ.
age.bg  1.00                   
ACT.bg  0.64  1.00             
SATV.bg 0.26  0.72  1.00       
SATQ.bg 0.22  0.52  0.64  1.00 
Correlation within groups 
        ag.wg ACT.w SATV. SATQ.
age.wg   1.00                  
ACT.wg   0.03  1.00            
SATV.wg -0.08  0.56  1.00      
SATQ.wg -0.09  0.59  0.65  1.00

Many results are not shown directly. To see specific objects select from the following list:
 mean sd n F ICC1 ICC2 ci1 ci2 raw rbg pbg rwg nw pwg etabg etawg nwg nG Call> lowerMat(sat.stats$pbg)  #get the probability values
        ag.bg ACT.b SATV. SATQ.
age.bg  0.00                   
ACT.bg  0.02  0.00             
SATV.bg 0.41  0.01  0.00       
SATQ.bg 0.49  0.08  0.03  0.00 
> 
> #show means by groups
> round(sat.stats$mean)
      gender education age ACT SATV SATQ
 [1,]      1         0  17  29  640  643
 [2,]      1         1  20  27  603  626
 [3,]      1         2  25  27  560  569
 [4,]      1         3  21  29  617  643
 [5,]      1         4  32  29  620  636
 [6,]      1         5  36  31  623  658
 [7,]      2         0  17  26  595  600
 [8,]      2         1  19  28  597  593
 [9,]      2         2  30  27  594  586
[10,]      2         3  21  28  610  591
[11,]      2         4  29  29  615  598
[12,]      2         5  34  29  620  607
> 
> #Do separate factor analyses for each group
> #faBy(sat.stats,1)
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("structure.diagram")
> ### * structure.diagram
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: structure.diagram
> ### Title: Draw a structural equation model specified by two measurement
> ###   models and a structural model
> ### Aliases: structure.diagram structure.graph structure.sem lavaan.diagram
> ### Keywords: multivariate hplot
> 
> ### ** Examples
> 
> fx <- matrix(c(.9,.8,.6,rep(0,4),.6,.8,-.7),ncol=2)
> fy <- matrix(c(.6,.5,.4),ncol=1)
> Phi <- matrix(c(1,0,0,0,1,0,.7,.7,1),ncol=3,byrow=TRUE)
> f1 <- structure.diagram(fx,Phi,fy,main="A structural path diagram")
> 
> #symbolic input
> X2 <- matrix(c("a",0,0,"b","e1",0,0,"e2"),ncol=4)
> colnames(X2) <- c("X1","X2","E1","E2")
> phi2 <- diag(1,4,4)
> phi2[2,1] <- phi2[1,2] <- "r"
> f2 <- structure.diagram(X2,Phi=phi2,errors=FALSE,main="A symbolic model") 
> 
> #symbolic input with error 
> X2 <- matrix(c("a",0,0,"b"),ncol=2)
> colnames(X2) <- c("X1","X2")
> phi2 <- diag(1,2,2)
> phi2[2,1] <- phi2[1,2] <- "r"
> f3 <- structure.diagram(X2,Phi=phi2,main="an alternative representation")
> 
> #and yet another one
> X6 <- matrix(c("a","b","c",rep(0,6),"d","e","f"),nrow=6)
> colnames(X6) <- c("L1","L2")
> rownames(X6) <- c("x1","x2","x3","x4","x5","x6")
> Y3 <- matrix(c("u","w","z"),ncol=1)
> colnames(Y3) <- "Y"
> rownames(Y3) <- c("y1","y2","y3")
> phi21 <- matrix(c(1,0,"r1",0,1,"r2",0,0,1),ncol=3)
> colnames(phi21) <- rownames(phi21) <-  c("L1","L2","Y")
> f4 <- structure.diagram(X6,phi21,Y3)
> 
> 
> 
> # and finally, a regression model
> X7 <- matrix(c("a","b","c","d","e","f"),nrow=6)
> f5 <- structure.diagram(X7,regression=TRUE)
> 
> #and a really messy regession model
> x8 <- c("b1","b2","b3")
> r8 <- matrix(c(1,"r12","r13","r12",1,"r23","r13","r23",1),ncol=3)
> f6<- structure.diagram(x8,Phi=r8,regression=TRUE)
> 
> 
> 
> cleanEx()
> nameEx("structure.list")
> ### * structure.list
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: structure.list
> ### Title: Create factor model matrices from an input list
> ### Aliases: structure.list phi.list
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> fx <- structure.list(9,list(F1=c(1,2,3),F2=c(4,5,6),F3=c(7,8,9)))
> fy <- structure.list(3,list(Y=c(1,2,3)),"Y")
> phi <- phi.list(4,list(F1=c(4),F2=c(1,4),F3=c(2),F4=c(1,2,3)))
> fx
      F1   F2   F3  
 [1,] "a1" "0"  "0" 
 [2,] "a2" "0"  "0" 
 [3,] "a3" "0"  "0" 
 [4,] "0"  "b4" "0" 
 [5,] "0"  "b5" "0" 
 [6,] "0"  "b6" "0" 
 [7,] "0"  "0"  "c7"
 [8,] "0"  "0"  "c8"
 [9,] "0"  "0"  "c9"
> phi
   F1    F2    F3    F4   
F1 "1"   "rba" "0"   "rda"
F2 "0"   "1"   "rcb" "rdb"
F3 "0"   "0"   "1"   "rdc"
F4 "rad" "rbd" "0"   "1"  
> fy
     Y    
[1,] "Ya1"
[2,] "Ya2"
[3,] "Ya3"
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("super.matrix")
> ### * super.matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: superMatrix
> ### Title: Form a super matrix from two sub matrices.
> ### Aliases: superMatrix super.matrix
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> mx <- matrix(c(.9,.8,.7,rep(0,4),.8,.7,.6),ncol=2)
> my <- matrix(c(.6,.5,.4))
> 
> colnames(mx) <- paste("X",1:dim(mx)[2],sep="")
> rownames(mx) <- paste("Xv",1:dim(mx)[1],sep="")
> colnames(my) <- "Y"
> rownames(my) <- paste("Yv",1:3,sep="")
> mxy <- superMatrix(mx,my)
> #show the use of a list to do this as well
> key1 <- make.keys(6,list(first=c(1,-2,3),second=4:6,all=1:6))  #make a scoring key
> key2 <- make.keys(4,list(EA=c(1,2),TA=c(3,4)))
> superMatrix(list(key1,key2))
    first second all EA TA
Vx1     1      0   1  0  0
Vx2    -1      0   1  0  0
Vx3     1      0   1  0  0
Vx4     0      1   1  0  0
Vx5     0      1   1  0  0
Vx6     0      1   1  0  0
Vy1     0      0   0  1  0
Vy2     0      0   0  1  0
Vy3     0      0   0  0  1
Vy4     0      0   0  0  1
> 
> 
> 
> cleanEx()
> nameEx("table2matrix")
> ### * table2matrix
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: table2matrix
> ### Title: Convert a table with counts to a matrix or data.frame
> ###   representing those counts.
> ### Aliases: table2matrix table2df
> ### Keywords: models
> 
> ### ** Examples
> 
> data(cubits)
> cubit <- table2matrix(cubits,labs=c("height","cubit"))
> describe(cubit)
       vars   n  mean   sd median trimmed  mad  min   max range  skew kurtosis
height    1 348 67.07 2.36  67.00   67.11 2.97 63.0 71.00  8.00 -0.09    -0.92
cubit     2 348 18.10 0.78  18.25   18.11 0.74 16.5 19.75  3.25 -0.09    -0.60
         se
height 0.13
cubit  0.04
> ellipses(cubit,n=1)
> data(bock)
> responses <- table2df(bock.table[,2:6],count=bock.table[,7],labs= paste("lsat6.",1:5,sep=""))
> describe(responses)
        vars    n mean   sd median trimmed mad min max range  skew kurtosis
lsat6.1    1 1000 0.92 0.27      1    1.00   0   0   1     1 -3.20     8.22
lsat6.2    2 1000 0.71 0.45      1    0.76   0   0   1     1 -0.92    -1.16
lsat6.3    3 1000 0.55 0.50      1    0.57   0   0   1     1 -0.22    -1.95
lsat6.4    4 1000 0.76 0.43      1    0.83   0   0   1     1 -1.24    -0.48
lsat6.5    5 1000 0.87 0.34      1    0.96   0   0   1     1 -2.17     2.72
          se
lsat6.1 0.01
lsat6.2 0.01
lsat6.3 0.02
lsat6.4 0.01
lsat6.5 0.01
> 
> 
> 
> cleanEx()
> nameEx("test.irt")
> ### * test.irt
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: test.irt
> ### Title: A simple demonstration (and test) of various IRT scoring
> ###   algorthims.
> ### Aliases: test.irt
> ### Keywords: multivariate models
> 
> ### ** Examples
> 
> #not run
> #test.irt(9,1000)
> 
> 
> 
> cleanEx()
> nameEx("test.psych")
> ### * test.psych
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: test.psych
> ### Title: Testing of functions in the psych package
> ### Aliases: test.psych
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> #test <- test.psych()
> #not run
> #test.psych(all=TRUE)
> #    f3 <- fa(bfi[1:15],3,n.iter=5)
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="Varimax")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="varimax")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="bifactor")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="varimin")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="bentlerT")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="geominT")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="equamax")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="Promax")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="cluster")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="biquartimin")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="equamax")
> #    f3 <- fa(bfi[1:15],3,n.iter=5,rotate="Promax")
> #    
> #     fpoly <- fa(bfi[1:10],2,n.iter=5,cor="poly")
> #     f1 <- fa(ability,n.iter=4)
> #     f1p <- fa(ability,n.iter=4,cor="tet")
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("tetrachor")
> ### * tetrachor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tetrachoric
> ### Title: Tetrachoric, polychoric, biserial and polyserial correlations
> ###   from various types of input
> ### Aliases: tetrachoric tetrachor polychoric biserial polydi polyserial
> ###   poly.mat
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> #if(require(mnormt)) {
> data(bock)
> tetrachoric(lsat6)

Call: tetrachoric(x = lsat6)
tetrachoric correlation 
   Q1   Q2   Q3   Q4   Q5  
Q1 1.00                    
Q2 0.17 1.00               
Q3 0.23 0.19 1.00          
Q4 0.11 0.11 0.19 1.00     
Q5 0.07 0.17 0.11 0.20 1.00

 with tau of 
   Q1    Q2    Q3    Q4    Q5 
-1.43 -0.55 -0.13 -0.72 -1.13 
> polychoric(lsat6)  #values should be the same
Call: polychoric(x = lsat6)
Polychoric correlations 
   Q1   Q2   Q3   Q4   Q5  
Q1 1.00                    
Q2 0.17 1.00               
Q3 0.23 0.19 1.00          
Q4 0.11 0.11 0.19 1.00     
Q5 0.07 0.17 0.11 0.20 1.00

 with tau of 
       1
Q1 -1.43
Q2 -0.55
Q3 -0.13
Q4 -0.72
Q5 -1.13
> tetrachoric(matrix(c(44268,193,14,0),2,2))  #MPLUS reports.24
For i = 1 j = 1  A cell entry of 0 was replaced with correct =  0.5.  Check your data!
Call: tetrachoric(x = matrix(c(44268, 193, 14, 0), 2, 2))
tetrachoric correlation 
[1] 0.23

 with tau of 
[1] 2.6 3.4
> 
> #Do not apply continuity correction -- compare with previous analysis!
> tetrachoric(matrix(c(44268,193,14,0),2,2),correct=0)  
Call: tetrachoric(x = matrix(c(44268, 193, 14, 0), 2, 2), correct = 0)
tetrachoric correlation 
[1] -0.71

 with tau of 
[1] 2.6 3.4
> 
> #the default is to add correct=.5 to 0 cells 
> tetrachoric(matrix(c(61661,1610,85,20),2,2)) #Mplus reports .35
Call: tetrachoric(x = matrix(c(61661, 1610, 85, 20), 2, 2))
tetrachoric correlation 
[1] 0.35

 with tau of 
[1] 1.9 2.9
> tetrachoric(matrix(c(62503,105,768,0),2,2)) #Mplus reports -.10
For i = 1 j = 1  A cell entry of 0 was replaced with correct =  0.5.  Check your data!
Call: tetrachoric(x = matrix(c(62503, 105, 768, 0), 2, 2))
tetrachoric correlation 
[1] -0.1

 with tau of 
[1] 2.9 2.3
> tetrachoric(matrix(c(24875,265,47,0),2,2)) #Mplus reports  0
For i = 1 j = 1  A cell entry of 0 was replaced with correct =  0.5.  Check your data!
Call: tetrachoric(x = matrix(c(24875, 265, 47, 0), 2, 2))
tetrachoric correlation 
[1] -0.00016

 with tau of 
[1] 2.3 2.9
> 
> polychoric(matrix(c(61661,1610,85,20),2,2)) #Mplus reports .35
[1] "You seem to have a table, I will return just one correlation."
$rho
[1] 0.3480731

$objective
[1] 0.131411

$tau.row
[1] 1.947799

$tau.col
[1] 2.937045

> polychoric(matrix(c(62503,105,768,0),2,2)) #Mplus reports -.10
[1] "You seem to have a table, I will return just one correlation."
$rho
[1] -0.1019161

$objective
[1] 0.0778664

$tau.row
[1] 2.937045

$tau.col
[1] 2.253362

> polychoric(matrix(c(24875,265,47,0),2,2)) #Mplus reports  0
[1] "You seem to have a table, I will return just one correlation."
$rho
[1] -0.000161322

$objective
[1] 0.07218902

$tau.row
[1] 2.307219

$tau.col
[1] 2.899962

> 
> #Do not apply continuity correction- compare with previous analysis
> tetrachoric(matrix(c(24875,265,47,0),2,2), correct=0) 
Call: tetrachoric(x = matrix(c(24875, 265, 47, 0), 2, 2), correct = 0)
tetrachoric correlation 
[1] -0.83

 with tau of 
[1] 2.3 2.9
> polychoric(matrix(c(24875,265,47,0),2,2), correct=0)  #the same result
[1] "You seem to have a table, I will return just one correlation."
$rho
[1] -0.8263137

$objective
[1] 0.07195411

$tau.row
[1] 2.307219

$tau.col
[1] 2.899962

> 
> 
> #examples from Kirk 1973  
> #note that Kirk's tables have joint probability followed by marginals, but 
> #tetrachoric needs marginals followed by joint probability
> 
> tetrachoric(c(.5,.5,.333333))   #should be .5
Call: tetrachoric(x = c(0.5, 0.5, 0.333333))
tetrachoric correlation 
[1] 0.5

 with tau of 
[1] 0 0
> tetrachoric(c(.5,.5,.1150267))  #should be -.75
Call: tetrachoric(x = c(0.5, 0.5, 0.1150267))
tetrachoric correlation 
[1] -0.75

 with tau of 
[1] 0 0
> tetrachoric(c(.5,.5,.397584))   #should e .8
Call: tetrachoric(x = c(0.5, 0.5, 0.397584))
tetrachoric correlation 
[1] 0.8

 with tau of 
[1] 0 0
> tetrachoric(c(.158655254,.158655254,.145003)) #should be .99
Call: tetrachoric(x = c(0.158655254, 0.158655254, 0.145003))
tetrachoric correlation 
[1] 0.99

 with tau of 
[1] -1 -1
> 
> 
> #the example from Olsson, 1979
>  x <- as.table(matrix(c(13,69,41,6,113,132,0,22,104),3,3))
>  polychoric(x,correct=FALSE)
[1] "You seem to have a table, I will return just one correlation."
$rho
[1] 0.4913712

$objective
[1] 1.781665

$tau.row
         A          B 
-1.7743819 -0.1357739 

$tau.col
         A          B 
-0.6871313  0.6682093 

> #Olsson reports rho = .49, tau row = -1.77, -.14 and tau col =  -.69, .67
> 
> #give a vector of two marginals and the comorbidity
> tetrachoric(c(.2, .15, .1))
Call: tetrachoric(x = c(0.2, 0.15, 0.1))
tetrachoric correlation 
[1] 0.75

 with tau of 
[1] -1.04 -0.84
> tetrachoric(c(.2, .1001, .1))
Call: tetrachoric(x = c(0.2, 0.1001, 0.1))
tetrachoric correlation 
[1] 0.98

 with tau of 
[1] -1.28 -0.84
>  #} else {
>  #       message("Sorry, you must have mnormt installed")}
> 
> # 4 plots comparing biserial to point biserial and latent Pearson correlation
> set.seed(42)
> x.4 <- sim.congeneric(loads =c(.9,.6,.3,0),N=1000,short=FALSE)
> y  <- x.4$latent[,1]
> for(i in 1:4) {
+ x <- x.4$observed[,i]
+ r <- round(cor(x,y),1)
+ ylow <- y[x<= 0]
+ yhigh <- y[x > 0]
+ yc <- c(ylow,yhigh)
+ rpb <- round(cor((x>=0),y),2)
+ rbis <- round(biserial(y,(x>=0)),2)
+ ellipses(x,y,ylim=c(-3,3),xlim=c(-4,3),pch=21 - (x>0),
+        main =paste("r = ",r,"rpb = ",rpb,"rbis =",rbis))
+ 
+ dlow <- density(ylow)
+ dhigh <- density(yhigh)
+ points(dlow$y*5-4,dlow$x,typ="l",lty="dashed")
+ lines(dhigh$y*5-4,dhigh$x,typ="l")
+ }
  Biserial |                                                                                                    |   0%
  Biserial |                                                                                                    |   0%
  Biserial |                                                                                                    |   0%
  Biserial |                                                                                                    |   0%
> 
> 
> 
> 
> 
> 
> 
> cleanEx()
> nameEx("thurstone")
> ### * thurstone
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: thurstone
> ### Title: Thurstone Case V scaling
> ### Aliases: thurstone
> ### Keywords: models
> 
> ### ** Examples
> 
> data(vegetables)
> thurstone(veg)
Thurstonian scale (case 5) scale values 
Call: thurstone(x = veg)
   Turn     Cab    Beet     Asp     Car    Spin S.Beans    Peas    Corn 
   0.00    0.52    0.65    0.98    1.12    1.14    1.40    1.44    1.63 

 Goodness of fit of model   0.99> 
> 
> 
> cleanEx()
> nameEx("tr")
> ### * tr
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: tr
> ### Title: Find the trace of a square matrix
> ### Aliases: tr
> ### Keywords: multivariate
> 
> ### ** Examples
> 
>  m <- matrix(1:16,ncol=4)
>  m
     [,1] [,2] [,3] [,4]
[1,]    1    5    9   13
[2,]    2    6   10   14
[3,]    3    7   11   15
[4,]    4    8   12   16
>  tr(m)
[1] 34
>  
> 
> 
> 
> cleanEx()
> nameEx("unidim")
> ### * unidim
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: unidim
> ### Title: Several indices of the unidimensionality of a set of variables.
> ### Aliases: unidim
> ### Keywords: models multivariate
> 
> ### ** Examples
> 
> #test the unidimensionality of the five factors of the bfi data set.
> 
> keys.list <-
+   list(agree=c("-A1","A2","A3","A4","A5"),conscientious=c("C1","C2","C3","-C4","-C5"),
+ extraversion=c("-E1","-E2","E3","E4","E5"),neuroticism=c("N1","N2","N3","N4","N5"),
+ openness = c("O1","-O2","O3","O4","-O5")) 
> unidim(bfi,keys.list) 
> 
> #Try a known 3 factor structure
> x <- sim.minor(nfact=3,bipolar=FALSE)
> unidim(x$model) 
> keys.list <- list(first =c(1:4),second = 5:8,third=9:12,all=1:12)
> unidim(x$model,keys.list)
> 
> x <- sim.minor(nfact=3)
> unidim(x$model,keys.list,flip=TRUE)
Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate,  :
   A loading greater than abs(1) was detected.  Examine the loadings carefully.
The estimated weights for the factor scores are probably incorrect.  Try a different factor extraction method.
Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate,  :
  An ultra-Heywood case was detected.  Examine the results carefully
> 
> 
> 
> 
> cleanEx()
> nameEx("vegetables")
> ### * vegetables
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: vegetables
> ### Title: Paired comparison of preferences for 9 vegetables
> ### Aliases: vegetables veg
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(vegetables)
> thurstone(veg)
Thurstonian scale (case 5) scale values 
Call: thurstone(x = veg)
   Turn     Cab    Beet     Asp     Car    Spin S.Beans    Peas    Corn 
   0.00    0.52    0.65    0.98    1.12    1.14    1.40    1.44    1.63 

 Goodness of fit of model   0.99> 
> 
> 
> cleanEx()
> nameEx("winsor")
> ### * winsor
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: winsor
> ### Title: Find the Winsorized scores, means, sds or variances for a
> ###   vector, matrix, or data.frame
> ### Aliases: winsor winsor.mean winsor.means winsor.sd winsor.var
> ### Keywords: univar
> 
> ### ** Examples
> 
> data(sat.act)
> winsor.means(sat.act) #compare with the means of the winsorized scores
    gender  education        age        ACT       SATV       SATQ 
  1.647143   3.391429  23.954286  28.957143 615.570000 614.521106 
> y <- winsor(sat.act)
> describe(y)
          vars   n   mean    sd median trimmed    mad   min max range  skew
gender       1 700   1.65  0.48      2    1.68   0.00   1.0   2   1.0 -0.61
education    2 700   3.39  1.03      3    3.36   1.48   2.0   5   3.0  0.27
age          3 700  23.95  5.11     22   23.57   4.45  19.0  32  13.0  0.56
ACT          4 700  28.96  3.18     29   28.97   4.45  24.8  33   8.2 -0.06
SATV         5 700 615.57 72.79    620  618.21 118.61 510.0 700 190.0 -0.24
SATQ         6 687 614.52 80.88    620  616.87 118.61 500.0 710 210.0 -0.24
          kurtosis   se
gender       -1.62 0.02
education    -1.07 0.04
age          -1.30 0.19
ACT          -1.56 0.12
SATV         -1.43 2.75
SATQ         -1.47 3.09
> xy <- data.frame(sat.act,y)
> #pairs.panels(xy) #to see the effect of winsorizing 
> x <- matrix(1:100,ncol=5)
> winsor(x)
      [,1] [,2] [,3] [,4] [,5]
 [1,]  4.8 24.8 44.8 64.8 84.8
 [2,]  4.8 24.8 44.8 64.8 84.8
 [3,]  4.8 24.8 44.8 64.8 84.8
 [4,]  4.8 24.8 44.8 64.8 84.8
 [5,]  5.0 25.0 45.0 65.0 85.0
 [6,]  6.0 26.0 46.0 66.0 86.0
 [7,]  7.0 27.0 47.0 67.0 87.0
 [8,]  8.0 28.0 48.0 68.0 88.0
 [9,]  9.0 29.0 49.0 69.0 89.0
[10,] 10.0 30.0 50.0 70.0 90.0
[11,] 11.0 31.0 51.0 71.0 91.0
[12,] 12.0 32.0 52.0 72.0 92.0
[13,] 13.0 33.0 53.0 73.0 93.0
[14,] 14.0 34.0 54.0 74.0 94.0
[15,] 15.0 35.0 55.0 75.0 95.0
[16,] 16.0 36.0 56.0 76.0 96.0
[17,] 16.2 36.2 56.2 76.2 96.2
[18,] 16.2 36.2 56.2 76.2 96.2
[19,] 16.2 36.2 56.2 76.2 96.2
[20,] 16.2 36.2 56.2 76.2 96.2
> winsor.means(x)
[1] 10.5 30.5 50.5 70.5 90.5
> y <- 1:11
> winsor(y,trim=.5)
 [1] 6 6 6 6 6 6 6 6 6 6 6
> 
> 
> 
> cleanEx()
> nameEx("withinBetween")
> ### * withinBetween
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: withinBetween
> ### Title: An example of the distinction between within group and between
> ###   group correlations
> ### Aliases: withinBetween
> ### Keywords: datasets
> 
> ### ** Examples
> 
> data(withinBetween)
> pairs.panels(withinBetween,bg=c("red","blue","white","black")[withinBetween[,1]],
+      pch=21,ellipses=FALSE)
> stats <- statsBy(withinBetween,'Group')
> print(stats,short=FALSE)
Statistics within and between groups  
Call: statsBy(data = withinBetween, group = "Group")
Intraclass Correlation 1 (Percentage of variance due to groups) 
Group    V1    V2    V3    V4    V5    V6    V7    V8    V9 
 1.00  0.43  0.43  0.43  0.43  0.43  0.43  0.43  0.43  0.43 
Intraclass Correlation 2 (Reliability of group differences) 
Group    V1    V2    V3    V4    V5    V6    V7    V8    V9 
 1.00  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75 
eta^2 between groups  
V1.bg V2.bg V3.bg V4.bg V5.bg V6.bg V7.bg V8.bg V9.bg 
  0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5   0.5 
Correlation between groups 
      V1.bg V2.bg V3.bg V4.bg V5.bg V6.bg V7.bg V8.bg V9.bg
V1.bg  1                                                   
V2.bg  1     1                                             
V3.bg  1     1     1                                       
V4.bg  0     0     0     1                                 
V5.bg  0     0     0     1     1                           
V6.bg  0     0     0     1     1     1                     
V7.bg -1    -1    -1     0     0     0     1               
V8.bg -1    -1    -1     0     0     0     1     1         
V9.bg -1    -1    -1     0     0     0     1     1     1   
Correlation within groups 
      V1.wg V2.wg V3.wg V4.wg V5.wg V6.wg V7.wg V8.wg V9.wg
V1.wg  1                                                   
V2.wg  0     1                                             
V3.wg -1     0     1                                       
V4.wg  1     0    -1     1                                 
V5.wg  0     1     0     0     1                           
V6.wg -1     0     1    -1     0     1                     
V7.wg  1     0    -1     1     0    -1     1               
V8.wg  0     1     0     0     1     0     0     1         
V9.wg -1     0     1    -1     0     1    -1     0     1   

Many results are not shown directly. To see specific objects select from the following list:
 mean sd n F ICC1 ICC2 ci1 ci2 raw rbg pbg rwg nw pwg etabg etawg nwg nG Call> 
> 
> 
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  46.176 1.1 56.152 2.572 0.796 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
